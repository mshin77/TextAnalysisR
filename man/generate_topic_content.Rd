% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/topic_modeling.R
\name{generate_topic_content}
\alias{generate_topic_content}
\title{Generate Content from Topic Terms}
\usage{
generate_topic_content(
  topic_terms_df,
  content_type = c("survey_item", "research_question", "theme_description",
    "policy_recommendation", "interview_question", "custom"),
  topic_var = "topic",
  term_var = "term",
  weight_var = "beta",
  provider = c("openai", "ollama"),
  model = "gpt-3.5-turbo",
  temperature = 0,
  system_prompt = NULL,
  user_prompt_template = NULL,
  max_tokens = 150,
  api_key = NULL,
  output_var = NULL,
  verbose = TRUE
)
}
\arguments{
\item{topic_terms_df}{A data frame with topic terms, containing columns for
topic identifier, term, and optionally term weight (beta).}

\item{content_type}{Type of content to generate. One of:
\describe{
\item{"survey_item"}{Likert-scale survey items for scale development}
\item{"research_question"}{Research questions for literature review}
\item{"theme_description"}{Theme descriptions for qualitative analysis}
\item{"policy_recommendation"}{Policy recommendations for policy analysis}
\item{"interview_question"}{Interview questions for qualitative research}
\item{"custom"}{Custom content using user-provided prompts}
}}

\item{topic_var}{Name of the column containing topic identifiers (default: "topic").}

\item{term_var}{Name of the column containing terms (default: "term").}

\item{weight_var}{Name of the column containing term weights (default: "beta").}

\item{provider}{LLM provider: "openai" or "ollama" (default: "openai").}

\item{model}{Model name. For OpenAI: "gpt-3.5-turbo", "gpt-4", etc.
For Ollama: "llama3", "mistral", etc.}

\item{temperature}{Sampling temperature (0-2). Lower = more deterministic (default: 0).}

\item{system_prompt}{Custom system prompt. If NULL, uses default for content_type.}

\item{user_prompt_template}{Custom user prompt template with \{terms\} placeholder.
If NULL, uses default for content_type.}

\item{max_tokens}{Maximum tokens for response (default: 150).}

\item{api_key}{OpenAI API key. If NULL, reads from OPENAI_API_KEY environment variable.}

\item{output_var}{Name of the output column (default: based on content_type).}

\item{verbose}{Logical, if TRUE, prints progress messages.}
}
\value{
A data frame with generated content joined to original topic terms.
}
\description{
Uses Large Language Models (LLMs) to generate various types of content
based on topic model terms. Supports multiple content types with optimized
default prompts, or fully custom prompts.
}
\details{
The function generates one piece of content per unique topic. Each content type
has optimized default prompts, but these can be overridden with custom prompts.

For OpenAI, requires an API key set via the \code{api_key} parameter or
OPENAI_API_KEY environment variable (can be loaded from .env file).

For Ollama, requires a local Ollama installation with the specified model.
}
\examples{
\dontrun{
# Generate survey items
survey_items <- generate_topic_content(
  topic_terms_df = top_terms,
  content_type = "survey_item",
  provider = "openai",
  model = "gpt-3.5-turbo"
)

# Generate research questions
research_qs <- generate_topic_content(
  topic_terms_df = top_terms,
  content_type = "research_question",
  provider = "ollama",
  model = "llama3"
)

# Generate with custom prompt
custom_content <- generate_topic_content(
  topic_terms_df = top_terms,
  content_type = "custom",
  system_prompt = "You are an expert in educational policy...",
  user_prompt_template = "Based on {terms}, generate a learning objective:"
)
}
}
\seealso{
Other ai: 
\code{\link{call_gemini_chat}()},
\code{\link{call_llm_api}()},
\code{\link{call_ollama}()},
\code{\link{call_openai_chat}()},
\code{\link{check_ollama}()},
\code{\link{get_api_embeddings}()},
\code{\link{get_best_embeddings}()},
\code{\link{get_content_type_prompt}()},
\code{\link{get_content_type_user_template}()},
\code{\link{get_recommended_ollama_model}()},
\code{\link{list_ollama_models}()},
\code{\link{run_rag_search}()}
}
\concept{ai}
