% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/semantic_analysis.R
\name{run_rag_search}
\alias{run_rag_search}
\title{RAG-Enhanced Semantic Search}
\usage{
run_rag_search(
  query,
  documents,
  provider = c("ollama", "openai", "gemini"),
  api_key = NULL,
  embedding_model = NULL,
  chat_model = NULL,
  top_k = 5
)
}
\arguments{
\item{query}{Character string, user question}

\item{documents}{Character vector, corpus to search}

\item{provider}{Character string, provider: "ollama" (local), "openai", or "gemini"}

\item{api_key}{Character string, API key for cloud providers (or from
OPENAI_API_KEY/GEMINI_API_KEY env). Not required for Ollama.}

\item{embedding_model}{Character string, embedding model. Defaults:
"nomic-embed-text" (ollama), "text-embedding-3-small" (openai),
"gemini-embedding-001" (gemini)}

\item{chat_model}{Character string, chat model. Defaults: "llama3.2" (ollama),
"gpt-4.1-mini" (openai), "gemini-2.5-flash" (gemini)}

\item{top_k}{Integer, number of documents to retrieve (default: 5)}
}
\value{
List with:
\itemize{
\item success: Logical
\item answer: Generated answer
\item confidence: Confidence score (0-1)
\item sources: Vector of source document indices
\item retrieved_docs: Retrieved document chunks
\item scores: Similarity scores
}
}
\description{
Simple in-memory RAG (Retrieval Augmented Generation) for question-answering
over document corpus with source attribution. Uses local (Ollama) or cloud
(OpenAI/Gemini) embeddings for semantic search and LLM for answer generation.
}
\details{
Simple RAG workflow:
\enumerate{
\item Generate embeddings for documents and query
\item Find top-k similar documents via cosine similarity
\item Generate answer using LLM with retrieved context
}
}
\examples{
\dontrun{
documents <- c(
  "Assistive technology helps students with disabilities access curriculum.",
  "Universal Design for Learning provides multiple means of engagement.",
  "Response to Intervention uses tiered support systems."
)

# Using local Ollama (free, private)
result <- run_rag_search(
  query = "How does assistive technology support learning?",
  documents = documents,
  provider = "ollama"
)

# Using OpenAI (requires API key)
result <- run_rag_search(
  query = "How does assistive technology support learning?",
  documents = documents,
  provider = "openai"
)

if (result$success) {
  cat("Answer:", result$answer, "\n")
  cat("Sources:", paste(result$sources, collapse = ", "), "\n")
}
}
}
\seealso{
Other ai: 
\code{\link{call_gemini_chat}()},
\code{\link{call_llm_api}()},
\code{\link{call_ollama}()},
\code{\link{call_openai_chat}()},
\code{\link{check_ollama}()},
\code{\link{describe_image}()},
\code{\link{generate_topic_content}()},
\code{\link{get_api_embeddings}()},
\code{\link{get_best_embeddings}()},
\code{\link{get_content_type_prompt}()},
\code{\link{get_content_type_user_template}()},
\code{\link{get_recommended_ollama_model}()},
\code{\link{list_ollama_models}()}
}
\concept{ai}
