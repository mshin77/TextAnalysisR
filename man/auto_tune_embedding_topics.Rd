% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/topic_modeling.R
\name{auto_tune_embedding_topics}
\alias{auto_tune_embedding_topics}
\title{Auto-tune BERTopic Hyperparameters}
\usage{
auto_tune_embedding_topics(
  texts,
  embeddings = NULL,
  embedding_model = "all-MiniLM-L6-v2",
  n_trials = 12,
  metric = "silhouette",
  seed = 123,
  verbose = TRUE
)
}
\arguments{
\item{texts}{Character vector of documents to analyze.}

\item{embeddings}{Precomputed embeddings matrix (optional). If NULL, embeddings are generated.}

\item{embedding_model}{Embedding model name (default: "all-MiniLM-L6-v2").}

\item{n_trials}{Maximum number of configurations to try (default: 12).}

\item{metric}{Optimization metric: "silhouette", "coherence", or "combined" (default: "silhouette").}

\item{seed}{Random seed for reproducibility.}

\item{verbose}{Logical, if TRUE, prints progress messages.}
}
\value{
A list containing:
\itemize{
\item best_config: Data frame with the optimal hyperparameter configuration
\item best_model: The topic model fitted with optimal parameters
\item all_results: List of all evaluated configurations with metrics
\item n_trials_completed: Number of configurations successfully evaluated
}
}
\description{
Automatically searches for optimal hyperparameters for embedding-based topic modeling.
Evaluates multiple configurations of UMAP and HDBSCAN parameters and returns the best
model based on the specified metric. Embeddings are generated once and reused across
all configurations for efficiency.
}
\details{
The function searches over these parameters:
\itemize{
\item n_neighbors: UMAP neighborhood size (5, 10, 15, 25)
\item min_cluster_size: HDBSCAN minimum cluster size (3, 5, 10)
\item cluster_selection_method: "eom" (broader) or "leaf" (finer-grained)
}
}
\examples{
\dontrun{
  texts <- c("Machine learning for image recognition",
             "Deep learning neural networks",
             "Natural language processing models",
             "Computer vision applications")

  tuning_result <- auto_tune_embedding_topics(
    texts = texts,
    n_trials = 6,
    metric = "silhouette",
    verbose = TRUE
  )

  # View best configuration
  tuning_result$best_config

  # Use the best model
  best_model <- tuning_result$best_model
}
}
\seealso{
Other topic-modeling: 
\code{\link{analyze_semantic_evolution}()},
\code{\link{assess_embedding_stability}()},
\code{\link{assess_hybrid_stability}()},
\code{\link{calculate_assignment_consistency}()},
\code{\link{calculate_eval_metrics_internal}()},
\code{\link{calculate_keyword_stability}()},
\code{\link{calculate_semantic_drift}()},
\code{\link{calculate_topic_probability}()},
\code{\link{calculate_topic_stability}()},
\code{\link{find_optimal_k}()},
\code{\link{find_topic_matches}()},
\code{\link{fit_embedding_model}()},
\code{\link{fit_hybrid_model}()},
\code{\link{fit_temporal_model}()},
\code{\link{generate_topic_labels}()},
\code{\link{get_topic_prevalence}()},
\code{\link{get_topic_terms}()},
\code{\link{get_topic_texts}()},
\code{\link{identify_topic_trends}()},
\code{\link{plot_model_comparison}()},
\code{\link{plot_quality_metrics}()},
\code{\link{run_contrastive_topics_internal}()},
\code{\link{run_neural_topics_internal}()},
\code{\link{run_temporal_topics_internal}()},
\code{\link{validate_semantic_coherence}()}
}
\concept{topic-modeling}
