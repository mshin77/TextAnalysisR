% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{call_llm_api}
\alias{call_llm_api}
\title{Call LLM API (Unified Wrapper)}
\usage{
call_llm_api(
  provider = c("openai", "gemini", "ollama"),
  system_prompt,
  user_prompt,
  model = NULL,
  temperature = 0,
  max_tokens = 150,
  api_key = NULL
)
}
\arguments{
\item{provider}{Character string: "openai", "gemini", or "ollama"}

\item{system_prompt}{Character string with system instructions}

\item{user_prompt}{Character string with user message}

\item{model}{Character string specifying the model (provider-specific defaults apply)}

\item{temperature}{Numeric temperature for response randomness (default: 0)}

\item{max_tokens}{Maximum number of tokens to generate (default: 150)}

\item{api_key}{Character string with API key (required for openai/gemini)}
}
\value{
Character string with the model's response
}
\description{
Unified wrapper for calling different LLM providers (OpenAI, Gemini, Ollama).
Automatically routes to the appropriate provider-specific function.
}
\examples{
\dontrun{
# Using OpenAI
response <- call_llm_api(
  provider = "openai",
  system_prompt = "You are a helpful assistant.",
  user_prompt = "Generate a topic label",
  api_key = Sys.getenv("OPENAI_API_KEY")
)

# Using Gemini
response <- call_llm_api(
  provider = "gemini",
  system_prompt = "You are a helpful assistant.",
  user_prompt = "Generate a topic label",
  api_key = Sys.getenv("GEMINI_API_KEY")
)
}
}
\seealso{
Other ai: 
\code{\link{call_gemini_chat}()},
\code{\link{call_ollama}()},
\code{\link{call_openai_chat}()},
\code{\link{check_ollama}()},
\code{\link{describe_image}()},
\code{\link{generate_topic_content}()},
\code{\link{get_api_embeddings}()},
\code{\link{get_best_embeddings}()},
\code{\link{get_content_type_prompt}()},
\code{\link{get_content_type_user_template}()},
\code{\link{get_recommended_ollama_model}()},
\code{\link{list_ollama_models}()},
\code{\link{run_rag_search}()}
}
\concept{ai}
