[{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"install-the-textanalysisr-package","dir":"Articles","previous_headings":"","what":"Install the TextAnalysisR Package","title":"TextAnalysisR Vignette","text":"development version can installed GitHub:","code":"install.packages(\"devtools\") devtools::install_github(\"mshin77/TextAnalysisR\")"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"load-the-textanalysisr-library","dir":"Articles","previous_headings":"","what":"Load the TextAnalysisR Library","title":"TextAnalysisR Vignette","text":"","code":"library(TextAnalysisR)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"alternatively-launch-and-browse-the-shiny-app","dir":"Articles","previous_headings":"","what":"Alternatively, Launch and Browse the Shiny App","title":"TextAnalysisR Vignette","text":"","code":"if (interactive()) {   TextAnalysisR.app() }"},{"path":[]},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"upload-an-example-dataset","dir":"Articles","previous_headings":"Process Files > Choose the Dataset","what":"Upload an example dataset","title":"TextAnalysisR Vignette","text":"","code":"mydata <- TextAnalysisR::process_files(dataset_choice = \"Upload an Example Dataset\")  head(mydata) ## # A tibble: 6 × 6 ##   reference_type  author                             year title keyword abstract ##   <chr>           <chr>                             <dbl> <chr> <chr>   <chr>    ## 1 journal_article Block, G. H.                       1980 Dysc… Arithm… Notes t… ## 2 thesis          Bukatman, K. L.                    1981 The … locus … This st… ## 3 journal_article Watkins, M. W., & Webb, C.         1981 Comp… Comput… Results… ## 4 journal_article Chaffin, J. D.                     1982 Arc-… Comput… The Arc… ## 5 journal_article Chaffin, J. D., Maxwell, B., & T…  1982 ARC-… Electr… This ar… ## 6 thesis          Golden, C. K.                      1982 The … NA      The pur…"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"upload-your-file","dir":"Articles","previous_headings":"Process Files > Choose the Dataset","what":"Upload your file","title":"TextAnalysisR Vignette","text":"","code":"file_info <- data.frame(filepath = \"inst/extdata/SpecialEduTech.xlsx\") mydata <- TextAnalysisR::process_files(dataset_choice = \"Upload Your File\",                                        file_info = file_info) head(mydata) ## # A tibble: 6 × 6 ##   reference_type  author                             year title keyword abstract ##   <chr>           <chr>                             <dbl> <chr> <chr>   <chr>    ## 1 journal_article Block, G. H.                       1980 Dysc… Arithm… Notes t… ## 2 thesis          Bukatman, K. L.                    1981 The … locus … This st… ## 3 journal_article Watkins, M. W., & Webb, C.         1981 Comp… Comput… Results… ## 4 journal_article Chaffin, J. D.                     1982 Arc-… Comput… The Arc… ## 5 journal_article Chaffin, J. D., Maxwell, B., & T…  1982 ARC-… Electr… This ar… ## 6 thesis          Golden, C. K.                      1982 The … NA      The pur…"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"copy-and-paste-text","dir":"Articles","previous_headings":"Process Files > Choose the Dataset","what":"Copy and paste text","title":"TextAnalysisR Vignette","text":"","code":"text_input <- paste0(\"The purpose of this study was to conduct a content analysis of \",                      \"research on technology use.\") mydata <- TextAnalysisR::process_files(dataset_choice = \"Copy and Paste Text\",                                        text_input = text_input) mydata ## # A tibble: 1 × 1 ##   text                                                                           ##   <chr>                                                                          ## 1 The purpose of this study was to conduct a content analysis of research on te…"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"unite-text-columns","dir":"Articles","previous_headings":"Preprocess Text Data","what":"Unite Text Columns","title":"TextAnalysisR Vignette","text":"","code":"df <- TextAnalysisR::SpecialEduTech  united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))  united_tbl ## # A tibble: 490 × 7 ##    united_texts               reference_type author  year title keyword abstract ##    <chr>                      <chr>          <chr>  <dbl> <chr> <chr>   <chr>    ##  1 Dyscalculia and the minic… journal_artic… Block…  1980 Dysc… Arithm… Notes t… ##  2 The effects of computer-a… thesis         Bukat…  1981 The … locus … This st… ##  3 Computer Assisted Instruc… journal_artic… Watki…  1981 Comp… Comput… Results… ##  4 Arc-Ed Curriculum: Applic… journal_artic… Chaff…  1982 Arc-… Comput… The Arc… ##  5 ARC-ED curriculum: the ap… journal_artic… Chaff…  1982 ARC-… Electr… This ar… ##  6 The Effect of the Hand-he… thesis         Golde…  1982 The … NA      The pur… ##  7 A review of some traditio… journal_artic… Neal,…  1982 A re… tradit… Discuss… ##  8 A study of the effectiven… thesis         Engle…  1983 A st… microc… The pur… ##  9 The influence of computer… thesis         Foste…  1983 The … comput… The eff… ## 10 Using Computer Software t… journal_artic… Pomme…  1983 Usin… Comput… The art… ## # ℹ 480 more rows"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"segment-a-corpus-into-tokens","dir":"Articles","previous_headings":"Preprocess Text Data","what":"Segment a Corpus Into Tokens","title":"TextAnalysisR Vignette","text":"","code":"tokens <- TextAnalysisR::preprocess_texts(united_tbl,                                           text_field = \"united_texts\",                                           min_char = 2,                                           remove_punct = TRUE,                                           remove_symbols = TRUE,                                           remove_numbers = TRUE,                                           remove_url = TRUE,                                           remove_separators = TRUE,                                           split_hyphens = TRUE,                                           split_tags = TRUE,                                           include_docvars = TRUE,                                           keep_acronyms = FALSE,                                           padding = FALSE,                                           verbose = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"detect-multi-word-expressions","dir":"Articles","previous_headings":"Preprocess Text Data","what":"Detect Multi-Word Expressions","title":"TextAnalysisR Vignette","text":"","code":"collocations <- TextAnalysisR::detect_multi_word_expressions(tokens, size = 2:5,                                                               min_count = 2)  head(collocations) ## [1] \"students with\"         \"learning disabilities\" \"this study\"            ## [4] \"problem solving\"       \"with disabilities\"     \"assisted instruction\""},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"process-tokens-with-compound-words","dir":"Articles","previous_headings":"Preprocess Text Data","what":"Process Tokens With Compound Words","title":"TextAnalysisR Vignette","text":"","code":"library(quanteda) custom_dict <- quanteda::dictionary(list(custom = c(\"learning disabilities\", \"problem solving\", \"assisted instruction\")))  toks_compound <- quanteda::tokens_compound(   tokens,   pattern = custom_dict,   concatenator = \"_\",   valuetype = \"glob\",   window = 0,   case_insensitive = TRUE,   join = TRUE,   keep_unigrams = FALSE,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"plot-word-frequency","dir":"Articles","previous_headings":"Preprocess Text Data","what":"Plot Word Frequency","title":"TextAnalysisR Vignette","text":"","code":"dfm_object_init <- quanteda::dfm(toks_compound)  TextAnalysisR::plot_word_frequency(dfm_object_init, n = 20)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"remove-predefined-stopwords","dir":"Articles","previous_headings":"Preprocess Text Data","what":"Remove Predefined Stopwords","title":"TextAnalysisR Vignette","text":"","code":"stopwords <- stopwords::stopwords(\"en\", source = \"snowball\")  toks_removed <- quanteda::tokens_remove(toks_compound, pattern = stopwords, verbose = FALSE)  dfm_init <- quanteda::dfm(toks_removed)  TextAnalysisR::plot_word_frequency(dfm_init, n = 20)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"remove-common-words-in-the-dataset","dir":"Articles","previous_headings":"Preprocess Text Data","what":"Remove Common Words in the Dataset","title":"TextAnalysisR Vignette","text":"","code":"common_words <- c(\"study\", \"based\", \"learning\", \"students\", \"research\", \"results\")  toks_removed_common <- quanteda::tokens_remove(toks_removed, pattern = common_words, verbose = FALSE)  dfm_init_updated <- quanteda::dfm(toks_removed_common)  TextAnalysisR::plot_word_frequency(dfm_init_updated, n = 20)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"lemmatize-tokens","dir":"Articles","previous_headings":"Preprocess Text Data","what":"Lemmatize Tokens","title":"TextAnalysisR Vignette","text":"","code":"library(spacyr) texts <- sapply(toks_removed_common, paste, collapse = \" \")  parsed <- spacyr::spacy_parse(x = texts, lemma = TRUE, entity = FALSE, pos = FALSE)  toks_lemmatized <- quanteda::as.tokens(parsed, use_lemma = TRUE)  dfm_object <- quanteda::dfm(toks_lemmatized)  quanteda::docvars(dfm_object) <- quanteda::docvars(dfm_init_updated)  TextAnalysisR::plot_word_frequency(dfm_object, n = 20)"},{"path":[]},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"word-co-occurrence-network-plot","dir":"Articles","previous_headings":"Word Analysis > Analyze and Visualize Word Co-occurrence Networks","what":"Word co-occurrence network plot","title":"TextAnalysisR Vignette","text":"","code":"word_co_occurrence_network_results <-      TextAnalysisR::word_co_occurrence_network(         dfm_object,         doc_var = \"reference_type\",         co_occur_n = 100,         top_node_n = 30,         nrows = 1,         height = 1200,         width = 800) ## [1] \"doc_var has 2 levels: journal_article, thesis\" ## [1] \"Processing group level: journal_article\" ## [1] \"Processing group level: thesis\" ## [1] \"Generating table for level: journal_article\" ## [1] \"Generating table for level: thesis\" ## [1] \"Generating summary for level: journal_article\" ## [1] \"Generating summary for level: thesis\" word_co_occurrence_network_results$plot"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"word-co-occurrence-network-centrality-table","dir":"Articles","previous_headings":"Word Analysis > Analyze and Visualize Word Co-occurrence Networks","what":"Word co-occurrence network centrality table","title":"TextAnalysisR Vignette","text":"journal_article","code":"word_co_occurrence_network_results$table"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"word-co-occurrence-network-summary","dir":"Articles","previous_headings":"Word Analysis > Analyze and Visualize Word Co-occurrence Networks","what":"Word co-occurrence network summary","title":"TextAnalysisR Vignette","text":"journal_article","code":"word_co_occurrence_network_results$summary"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"word-correlation-network-plot","dir":"Articles","previous_headings":"Word Analysis > Analyze and Visualize Word Correlation Networks","what":"Word correlation network plot","title":"TextAnalysisR Vignette","text":"","code":"word_correlation_network_results <-    TextAnalysisR::word_correlation_network(     dfm_object,     doc_var = \"reference_type\",     common_term_n = 30,     corr_n = 0.42,     top_node_n = 40,     nrows = 2,     height = 1400,     width = 800) ## [1] \"doc_var has 2 levels: journal_article, thesis\" ## [1] \"Processing group level: journal_article\" ## [1] \"Processing group level: thesis\" ## [1] \"Generating table for level: journal_article\" ## [1] \"Generating table for level: thesis\" ## [1] \"Generating summary for level: journal_article\" ## [1] \"Generating summary for level: thesis\" word_correlation_network_results$plot"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"word-correlation-network-centrality-table","dir":"Articles","previous_headings":"Word Analysis > Analyze and Visualize Word Correlation Networks","what":"Word correlation network centrality table","title":"TextAnalysisR Vignette","text":"journal_article thesis","code":"word_correlation_network_results$table"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"word-correlation-network-summary","dir":"Articles","previous_headings":"Word Analysis > Analyze and Visualize Word Correlation Networks","what":"Word correlation network summary","title":"TextAnalysisR Vignette","text":"journal_article thesis","code":"word_correlation_network_results$summary"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"word-frequency-distribution-plot","dir":"Articles","previous_headings":"Word Analysis > Analyze and Visualize Word Frequency Distributions Across a Continuous Variable","what":"Word frequency distribution plot","title":"TextAnalysisR Vignette","text":"","code":"word_frequency_distribution_results <- TextAnalysisR::word_frequency_distribution(   dfm_object,   continuous_variable = \"year\",   selected_terms = c(\"calculator\", \"computer\"),   height = 500,   width = 800)  word_frequency_distribution_results$plot"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"word-frequency-distribution-table","dir":"Articles","previous_headings":"Word Analysis > Analyze and Visualize Word Frequency Distributions Across a Continuous Variable","what":"Word frequency distribution table","title":"TextAnalysisR Vignette","text":"calculator computer","code":"word_frequency_distribution_results$table"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"evaluate-optimal-number-of-topics","dir":"Articles","previous_headings":"Structural Topic Modeling","what":"Evaluate Optimal Number of Topics","title":"TextAnalysisR Vignette","text":"","code":"TextAnalysisR::evaluate_optimal_topic_number(   dfm_object = dfm_object,   topic_range = 5:30,   max.em.its = 75,   categorical_var = \"reference_type\",   continuous_var = \"year\",   height = 600,   width = 800,   verbose = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"select-top-terms-for-each-topic","dir":"Articles","previous_headings":"Structural Topic Modeling","what":"Select Top Terms for Each Topic","title":"TextAnalysisR Vignette","text":"","code":"out <- quanteda::convert(dfm_object, to = \"stm\")  library(stm) stm_15 <- stm(   data = out$meta,   documents = out$documents,   vocab = out$vocab,   max.em.its = 75,   init.type = \"Spectral\",   K = 15,   prevalence = ~ reference_type + s(year),   verbose = FALSE)  top_topic_terms <- TextAnalysisR::select_top_topic_terms(   stm_model = stm_15,   top_term_n = 10,   verbose = FALSE )  library(dplyr) top_topic_terms %>%   mutate_if(is.numeric, ~ round(., 3)) %>%       DT::datatable(       rownames = FALSE,       extensions = 'Buttons',       options = list(         scrollX = TRUE,         scrollY = \"400px\",         width = \"80%\",         dom = 'Bfrtip',         buttons = c('copy', 'csv', 'excel', 'pdf', 'print')       )     ) %>%     DT::formatStyle(       columns = c(\"topic\", \"term\", \"beta\"),       fontSize = '16px'     )"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"directly-input-the-open-ai-api-key","dir":"Articles","previous_headings":"Structural Topic Modeling > Generate Topic Labels Using OpenAI’s API","what":"Directly input the Open AI API key","title":"TextAnalysisR Vignette","text":"","code":"top_labeled_topic_terms <- TextAnalysisR::generate_topic_labels(   top_topic_terms,   model = \"gpt-3.5-turbo\",   temperature = 0.5,   openai_api_key = \"your_openai_api_key\",   verbose = FALSE)  top_labeled_topic_terms"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"load-the-open-ai-api-key-from-the--env-file-in-the-working-directory","dir":"Articles","previous_headings":"Structural Topic Modeling > Generate Topic Labels Using OpenAI’s API","what":"Load the Open AI API key from the .env file in the working directory","title":"TextAnalysisR Vignette","text":"","code":"top_labeled_topic_terms <- TextAnalysisR::generate_topic_labels(   top_topic_terms,   model = \"gpt-3.5-turbo\",   temperature = 0.5,   verbose = FALSE)  top_labeled_topic_terms %>%   mutate_if(is.numeric, ~ round(., 3)) %>%       DT::datatable(       rownames = FALSE,       extensions = 'Buttons',       options = list(         scrollX = TRUE,         scrollY = \"400px\",         width = \"80%\",         dom = 'Bfrtip',         buttons = c('copy', 'csv', 'excel', 'pdf', 'print')       )     ) %>%     DT::formatStyle(       columns = c(\"topic\", \"topic_label\", \"term\", \"beta\"),       fontSize = '16px'     )"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"add-generated-topic-lables-to-the-top-topic-terms","dir":"Articles","previous_headings":"Structural Topic Modeling > Plot Highest Word Probabilities for Each Topic","what":"Add generated topic lables to the top topic terms","title":"TextAnalysisR Vignette","text":"","code":"word_probability_plot <- TextAnalysisR::word_probability_plot(   top_labeled_topic_terms,   topic_label = \"topic_label\",   ncol = 1,   height = 3500,   width = 800)  word_probability_plot"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"use-non-labeled-topics","dir":"Articles","previous_headings":"Structural Topic Modeling > Plot Highest Word Probabilities for Each Topic","what":"Use non-labeled topics","title":"TextAnalysisR Vignette","text":"","code":"word_probability_plot <- TextAnalysisR::word_probability_plot(   top_topic_terms,   ncol = 2,   height = 2000,   width = 800)  word_probability_plot"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"manually-label-topics","dir":"Articles","previous_headings":"Structural Topic Modeling > Plot Highest Word Probabilities for Each Topic","what":"Manually label topics","title":"TextAnalysisR Vignette","text":"","code":"manual_labels <- c(\"1\" = \"CAI for math problem solving\",                    \"2\" = \"STEM technology\",                    \"3\" = \"Use of manipulatives\")  word_probability_plot <- TextAnalysisR::word_probability_plot(   top_topic_terms,   topic_label = manual_labels,   ncol = 2,   height = 2000,   width = 850)  word_probability_plot"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"plot-mean-topic-prevalence-across-documents","dir":"Articles","previous_headings":"Structural Topic Modeling > Per-Document Per-Topic Probabilities","what":"Plot mean topic prevalence across documents","title":"TextAnalysisR Vignette","text":"","code":"topic_probability_plot <- TextAnalysisR::topic_probability_plot(   stm_model= stm_15,   top_n = 10,   height = 500,   width = 800,   verbose = TRUE)  topic_probability_plot"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/TextAnalysisR_Vignette.html","id":"create-a-table-of-mean-topic-prevalence-across-documents","dir":"Articles","previous_headings":"Structural Topic Modeling > Per-Document Per-Topic Probabilities","what":"create a table of mean topic prevalence across documents","title":"TextAnalysisR Vignette","text":"","code":"topic_probability_table <- TextAnalysisR::topic_probability_table(   stm_model= stm_15,   top_n = 10,   verbose = TRUE)  topic_probability_table"},{"path":"https://mshin77.github.io/TextAnalysisR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Mikyung Shin. Author, maintainer.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Shin M (2025). TextAnalysisR: text mining workflow tool. R package version 0.0.2, https://mshin77.github.io/TextAnalysisR.","code":"@Manual{,   title = {TextAnalysisR: A text mining workflow tool},   author = {Mikyung Shin},   year = {2025},   note = {R package version 0.0.2},   url = {https://mshin77.github.io/TextAnalysisR}, }"},{"path":"https://mshin77.github.io/TextAnalysisR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A text mining workflow tool","text":"development version GitHub :","code":"install.packages(\"devtools\") devtools::install_github(\"mshin77/TextAnalysisR\")"},{"path":"https://mshin77.github.io/TextAnalysisR/index.html","id":"load-the-textanalysisr-package","dir":"","previous_headings":"","what":"Load the TextAnalysisR Package","title":"A text mining workflow tool","text":"","code":"library(TextAnalysisR)"},{"path":"https://mshin77.github.io/TextAnalysisR/index.html","id":"alternatively-launch-and-browse-the-shiny-app","dir":"","previous_headings":"","what":"Alternatively, Launch and Browse the Shiny App","title":"A text mining workflow tool","text":"Access web app https://www.textanalysisr.org. Launch browser TextAnalysisR.app local computer:","code":"TextAnalysisR.app()"},{"path":"https://mshin77.github.io/TextAnalysisR/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"A text mining workflow tool","text":"Shin, M. (2025). TextAnalysisR: text mining workflow tool (R package version 0.0.2) [Computer software]. https://mshin77.github.io/TextAnalysisR Shin, M. (2025). TextAnalysisR: text mining workflow tool [Web application]. https://www.textanalysisr.org","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"A text mining workflow tool","text":"Shin, M., Park, H., & Kang, E. (2024). Development education policies practices students learning disabilities South Korea using Delphi surveys topic modeling. Learning Disability Quarterly. GitHub Shin, M., Ok, M. W., Choo, S., Hossain, G., Bryant, D. P., & Kang, E. (2023). content analysis research technology use teaching mathematics students disabilities: word networks topic modeling. International Journal STEM Education, 10(1), 1-23. GitHub","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Mikyung Shin Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/acronym.html","id":null,"dir":"Reference","previous_headings":"","what":"Acronym list — acronym","title":"Acronym list — acronym","text":"Contains acronyms commonly observed bibliographic data research use technology teaching mathematics students disabilities.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/acronym.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Acronym list — acronym","text":"","code":"acronym"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/acronym.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Acronym list — acronym","text":"object class spec_tbl_df (inherits tbl_df, tbl, data.frame) 145 rows 2 columns.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/acronym.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Acronym list — acronym","text":"","code":"acronym #> # A tibble: 145 × 2 #>    `3D`  `three-dimensional`                               #>    <chr> <chr>                                             #>  1 3-D   three-dimensional                                 #>  2 AAC   Augmentative and Alternative Communication        #>  3 ADHD  attention deficit/hyperactivity disorder          #>  4 ADHD  attention deficit hyperactivity disorder          #>  5 AFB   American Foundation for the Blind                 #>  6 AI    app-based instruction                             #>  7 AITA  anchored Instruction with Technology Applications #>  8 ALTs  assistive learning technologies                   #>  9 ANS   Approximate Number System                         #> 10 ANS   Approximate Number System                         #> # ℹ 135 more rows"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_multi_word_expressions.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect Multi-Word Expressions — detect_multi_word_expressions","title":"Detect Multi-Word Expressions — detect_multi_word_expressions","text":"function detects multi-word expressions (collocations) specified sizes appear least specified number times provided tokens.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_multi_word_expressions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect Multi-Word Expressions — detect_multi_word_expressions","text":"","code":"detect_multi_word_expressions(tokens, size = 2:5, min_count = 2)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_multi_word_expressions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect Multi-Word Expressions — detect_multi_word_expressions","text":"tokens tokens object quanteda package. size numeric vector specifying sizes collocations detect (default: 2:5). min_count minimum number occurrences collocation considered (default: 2).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_multi_word_expressions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect Multi-Word Expressions — detect_multi_word_expressions","text":"character vector detected collocations.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_multi_word_expressions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect Multi-Word Expressions — detect_multi_word_expressions","text":"","code":"if (interactive()) {   df <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))    tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")    collocations <- TextAnalysisR::detect_multi_word_expressions(tokens, size = 2:5, min_count = 2)   print(collocations) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_1.html","id":null,"dir":"Reference","previous_headings":"","what":"Data-driven dictionary list 1 — dictionary_list_1","title":"Data-driven dictionary list 1 — dictionary_list_1","text":"Contains dictionary using wildcard values compound words SpecialEduTech","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data-driven dictionary list 1 — dictionary_list_1","text":"","code":"dictionary_list_1"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_1.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data-driven dictionary list 1 — dictionary_list_1","text":"object class list length 235.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data-driven dictionary list 1 — dictionary_list_1","text":"","code":"dictionary_list_1 #> $augmentative_and_alternative_communication #> [1] \"augmentative and alternative communication\" #> [2] \"aac\"                                        #>  #> $achievement_test #> [1] \"achievement test*\" #>  #> $adaptive_test #> [1] \"adaptive test*\"  \"adaptable test*\" #>  #> $adolescent #> [1] \"adolescents\" #>  #> $anchor #> [1] \"anchors\" #>  #> $approximate_number_system #> [1] \"approximate number system*\" \"ans\"                        #>  #> $arithmetic_fact #> [1] \"arithmetic fact*\" #>  #> $arithmetic_skill #> [1] \"arithmetic skill*\" #>  #> $artificial_intelligence #> [1] \"artificial intellig*\" #>  #> $assistive_technology #> [1] \"assistive technolog*\" #>  #> $asynchronous_learning #> [1] \"asynchronous learning\" #>  #> $augmentative_communication #> [1] \"augmentative communication*\" #>  #> $augmented_reality #> [1] \"augmented real*\" #>  #> $authoring_system #> [1] \"authoring system*\" #>  #> $basic_fact #> [1] \"basic fact\" #>  #> $basic_skill #> [1] \"basic skill*\" #>  #> $behavior #> [1] \"behaviors\"  \"behaviour*\" #>  #> $blended_instruction #> [1] \"blended instruction*\" #>  #> $blended_learning #> [1] \"blended learning\" #>  #> $case_based #> [1] \"case based\" #>  #> $common_core_state_standard #> [1] \"common core state standard*\" #>  #> $computational_thinking #> [1] \"computational thinking\" #>  #> $computer_assisted_instruction #> [1] \"computer assisted instruction*\" \"computer aided instruction*\"    #> [3] \"cai\"                            #>  #> $computer_based #> [1] \"computer based\" #>  #> $conceptual_knowledge #> [1] \"conceptual knowledge\" #>  #> $conceptual_model #> [1] \"conceptual model\" #>  #> $conceptual_understanding #> [1] \"conceptual understand*\" #>  #> $concrete_manipulatives #> [1] \"concrete manipulative*\" #>  #> $concrete_representational_abstract #> [1] \"concrete representational abstract\" \"cra\"                                #>  #> $criterion_referenced #> [1] \"criterion reference*\" #>  #> $curriculum_based_measurement #> [1] \"curriculum based measure*\" \"cbm\"                       #>  #> $data_based #> [1] \"data based\" #>  #> $digital_text #> [1] \"digital text*\" \"etext*\"        #>  #> $division_fact #> [1] \"division fact*\" #>  #> $down_syndrome #> [1] \"down syndrome\"   \"down's syndrome\" #>  #> $drill_practice #> [1] \"drill* and practice*\" \"drill* practice*\"     \"drill*\"               #>  #> $educational_technology #> [1] \"educational technolog*\" #>  #> $eigth_grade #> [1] \"eigth grade*\" \"8th grade*\"   #>  #> $e_learning #> [1] \"e learning\" \"elearning\"  #>  #> $elementary_education #> [1] \"elementary education\" #>  #> $elementary_mathematics #> [1] \"elementary mathematics\" #>  #> $elementary_level #> [1] \"elementary level*\" #>  #> $elementary_school #> [1] \"elementary school*\" #>  #> $elementary_student #> [1] \"elementary student*\" #>  #> $eleventh_grade #> [1] \"eleventh grade*\" \"11th grade\"      #>  #> $enhanced_anchored_instruction #> [1] \"enhanced anchor* instruction*\" \"eai\"                           #>  #> $evidence_based_practice #> [1] \"evidence based practice*\" \"ebp\"                      #>  #> $e_portfolio #> [1] \"e portfolio\" #>  #> $e_workbook #> [1] \"eworkbook*\" \"e workbook\" #>  #> $explicit_instruction #> [1] \"explicit instruction*\" #>  #> $eye_tracking #> [1] \"eye track*\" #>  #> $face_to_face #> [1] \"face to face\" #>  #> $fact_fluency #> [1] \"fact fluency\" #>  #> $fifth_grade #> [1] \"fifth grade*\" \"5th grade*\"   #>  #> $first_grade #> [1] \"first grade*\" \"1st grade*\"   #>  #> $flipped_classroom #> [1] \"flipped classroom*\" #>  #> $formative_assessment #> [1] \"formative assessment*\" #>  #> $fourth_grade #> [1] \"fourth grade*\" \"4th grade*\"    #>  #> $functional_relation #> [1] \"functional relation*\" #>  #> $goal_setting #> [1] \"goal setting\" #>  #> $graphic_organizer #> [1] \"graphic* organizer*\" #>  #> $graphing_calculator #> [1] \"graphing calculator*\" #>  #> $group_design #> [1] \"group design*\" #>  #> $hands_on #> [1] \"hand* on\" #>  #> $high_level #> [1] \"high* level*\" #>  #> $high_school #> [1] \"high school*\" #>  #> $higher_order_thinking #> [1] \"higher order think*\" #>  #> $inclusive_classroom #> [1] \"inclusive classroom*\" #>  #> $information_and_communication_technology #> [1] \"information and communication technology\"   #> [2] \"information and communication technologies\" #> [3] \"information communication technology\"       #> [4] \"information communication technologies\"     #> [5] \"ict\"                                        #>  #> $integrated_learning_system #> [1] \"integrated learning system*\" #>  #> $information_processing #> [1] \"information processing\" #>  #> $instructional_approach #> [1] \"instructional approach*\" #>  #> $instructional_component #> [1] \"instructional component*\" #>  #> $instructional_design #> [1] \"instructional design*\" #>  #> $instructional_method #> [1] \"instructional method*\" #>  #> $instructional_practice #> [1] \"instructional practice*\" #>  #> $instructional_program #> [1] \"instructional program*\" #>  #> $instructional_sequence #> [1] \"instructional sequence*\" #>  #> $instructional_strategy #> [1] \"instructional strateg*\" #>  #> $instructional_technology #> [1] \"instructional technolog*\" #>  #> $intelligent_tutor #> [1] \"intelligent* tutor*\" #>  #> $interactive_whiteboard #> [1] \"interactive whiteboard*\" #>  #> $learning_management_system #> [1] \"learning management system*\" #>  #> $learning_process #> [1] \"learning process*\" #>  #> $linear_equation #> [1] \"linear equation*\" #>  #> $long_term_memory #> [1] \"long term memory\" #>  #> $mathematical_concept #> [1] \"math* concept*\" #>  #> $mathematical_difficulty #> [1] \"math* difficult*\" #>  #> $mathematical_expression #> [1] \"math* expression*\" #>  #> $mathematical_reasoning #> [1] \"math* reasoning\" #>  #> $mathematics_assessment #> [1] \"math* assessment*\" #>  #> $mathematics_content #> [1] \"math* content*\" #>  #> $mathematics_fact #> [1] \"math* fact*\" #>  #> $mathematics_performance #> [1] \"mathematics performance*\" #>  #> $mathematics_skill #> [1] \"math* skill*\" #>  #> $mathematics_test #> [1] \"math* test*\" #>  #> $mental_retardation #> [1] \"mental* retard*\" #>  #> $microcomputer #> [1] \"microcomputers\"  \"micro computer\"  \"micro computers\" #>  #> $middle_school #> [1] \"middle school*\" #>  #> $multiple_baseline #> [1] \"multiple baseline\" #>  #> $multiple_probe #> [1] \"multiple probe\" #>  #> $multiplication_fact #> [1] \"\\tmultiplication fact*\" #>  #> $multiplicative_reasoning #> [1] \"multiplicative reasoning\" #>  #> $ninth_grade #> [1] \"ninth grade*\" \"9th grade*\"   #>  #> $norm_referenced #> [1] \"norm reference*\" #>  #> $number_concept #> [1] \"number concept*\" #>  #> $number_fact #> [1] \"number fact*\" #>  #> $number_line #> [1] \"number line*\" #>  #> $number_sense #> [1] \"number sense\" #>  #> $numerical_comparison #> [1] \"numerical comparison\" #>  #> $one_step #> [1] \"one step*\" #>  #> $open_educational_resource #> [1] \"open education* resource*\" \"oer\"                       #>  #> $paper_pencil #> [1] \"paper pencil*\"     \"paper and pencil*\" #>  #> $peer_tutoring #> [1] \"peer tutoring\" #>  #> $pilot_test #> [1] \"pilot test*\" #>  #> $physical_manipulatives #> [1] \"physical manipulative*\" #>  #> $posttest #> [1] \"post test*\" \"posttest*\"  #>  #> $pretest #> [1] \"pre test*\" \"pretest*\"  #>  #> $primary_education #> [1] \"primary education*\" #>  #> $primary_level #> [1] \"primary level*\" #>  #> $primary_school #> [1] \"primary school*\" #>  #> $problem_based #> [1] \"problem based\" #>  #> $problem_solving #> [1] \"problem solving\" #>  #> $procedural_knowledge #> [1] \"procedural knowledge\" #>  #> $procedural_understanding #> [1] \"procedural understand*\" #>  #> $progress_monitoring #> [1] \"progress monitoring\" #>  #> $project_based #> [1] \"project based\" #>  #> $quasi_experimental #> [1] \"quasi experimental\" #>  #> $read_aloud #> [1] \"read aloud*\" #>  #> $real_world #> [1] \"real world\" #>  #> $response_to_instruction #> [1] \"response to instruction\" \"rti\"                     #>  #> $schema_based_instruction #> [1] \"schema based instruction\" \"schema instruction\"       #> [3] \"sbi\"                      #>  #> $second_grade #> [1] \"second grade*\" \"2nd grade*\"    #>  #> $secondary_education #> [1] \"secondary education\" #>  #> $secondary_level #> [1] \"secondary level*\" #>  #> $secondary_mathematics #> [1] \"secondary mathematics\" #>  #> $secondary_school #> [1] \"secondary school*\" #>  #> $secondary_student #> [1] \"secondary student*\" #>  #> $self_assessment #> [1] \"self assess*\" #>  #> $self_contained #> [1] \"self contained\" #>  #> $self_directed #> [1] \"self direct*\" #>  #> $self_efficacy #> [1] \"self efficac*\" #>  #> $self_esteem #> [1] \"self esteem\" #>  #> $self_graphing #> [1] \"self graph*\" #>  #> $self_instruction #> [1] \"self instruct*\" #>  #> $self_management #> [1] \"self manag*\" #>  #> $self_modeling #> [1] \"self model*\" #>  #> $self_monitoring #> [1] \"self monitor*\" #>  #> $self_paced #> [1] \"self paced\" #>  #> $self_regulated #> [1] \"self regulat*\" #>  #> $self_recorded #> [1] \"self record*\" #>  #> $self_reported #> [1] \"self report*\" #>  #> $self_selected #> [1] \"self selected\" #>  #> $self_voicing #> [1] \"self voic*\" #>  #> $serious_game #> [1] \"serious game*\" #>  #> $seventh_grade #> [1] \"seventh grade*\" \"7th grade*\"     #>  #> $sixth_grade #> [1] \"sixth grade*\" \"6th grade*\"   #>  #> $short_term_memory #> [1] \"short term memory\" #>  #> $single_case #> [1] \"single case\" #>  #> $single_subject #> [1] \"single subject\" #>  #> $social_science #> [1] \"social science\" #>  #> $social_validity #> [1] \"social validit*\" #>  #> $special_education #> [1] \"special education\" #>  #> $special_needs #> [1] \"special needs\" #>  #> $speech_generating #> [1] \"speech generat*\" #>  #> $speech_recognition #> [1] \"speech recognit*\" #>  #> $standard_based #> [1] \"standard* based\" #>  #> $standardized_assessment #> [1] \"standardized assessment*\" #>  #> $standardized_test #> [1] \"standardized test*\" #>  #> $science_technology_engineering_and_mathematics #> [1] \"science technology engineering mathematics\"     #> [2] \"science technology engineering and mathematics\" #> [3] \"stem\"                                           #>  #> $story_problem #> [1] \"story problem*\" #>  #> $student_paced #> [1] \"student paced\" #>  #> $synchronous_learning #> [1] \"synchronous learning\" #>  #> $system_of_least_prompts #> [1] \"system of least prompt*\" #>  #> $systematic_instruction #> [1] \"systematic instruction*\" #>  #> $teacher_paced #> [1] \"teacher paced\" #>  #> $technology_assisted #> [1] \"technology assisted\" #>  #> $technology_based #> [1] \"technology based\" #>  #> $tenth_grade #> [1] \"tenth grade*\" \"10th grade*\"  #>  #> $third_grade #> [1] \"third grade*\" \"3rd grade*\"   #>  #> $three_dimensional #> [1] \"three dimension*\" #>  #> $three_step #> [1] \"three step*\" #>  #> $twelfth_grade #> [1] \"twelfth grade*\" \"12th grade*\"    #>  #> $two_step #> [1] \"two step*\" #>  #> $universal_design_for_learning #> [1] \"universal* design* for learning\" \"udl\"                             #>  #> $video_based #> [1] \"video based\" #>  #> $video_modeling #> [1] \"video model*\" #>  #> $video_prompting #> [1] \"video prompt*\" #>  #> $videodisc_instruction #> [1] \"videodisc* instruction*\" #>  #> $virtual_environment #> [1] \"virtual environment*\" #>  #> $virtual_learning #> [1] \"virtual learning\" #>  #> $virtual_manipulatives #> [1] \"virtual manipulative*\" #>  #> $virtual_reality #> [1] \"virtual realit*\" #>  #> $virtual_representational_abstract #> [1] \"virtual representational abstract\" \"vra\"                               #>  #> $visual_cue #> [1] \"visual cue*\" #>  #> $visual_image #> [1] \"visual image*\" #>  #> $visual_representation #> [1] \"visual representation*\" #>  #> $visual_support #> [1] \"visual support*\" #>  #> $whole_number #> [1] \"whole number*\" #>  #> $word_problem #> [1] \"word problem*\" #>  #> $working_memory #> [1] \"working memory\" #>  #> $attention_deficit_and_hyperactivity_disorder #> [1] \"attention deficit hyperactivity disorder*\"     #> [2] \"attention deficit and hyperactivity disorder*\" #> [3] \"adhd\"                                          #>  #> $autism_spectrum_disorder #> [1] \"autism spectrum disorder*\" \"asd\"                       #>  #> $dyscalculia #> [1] \"dyscalcul\" #>  #> $emotional_and_behavioral_disorder #> [1] \"emotional behavioral disorder*\"      \"emotional and behavioral disorder*\"  #> [3] \"emotional behavioral disabilit*\"     \"emotional and behavioral disabilit*\" #> [5] \"ebd\"                                 #>  #> $emotional_disturbance #> [1] \"emotional disturbance*\" #>  #> $fragile_x_syndrome #> [1] \"fragile x syndrome\" \"fxs\"                #>  #> $deaf_blindness #> [1] \"deaf-blind*\" \"deaf blind*\" #>  #> $hard_of_hearing #> [1] \"hard of hearing\" #>  #> $hearing_disability #> [1] \"hearing disabilit*\" #>  #> $hearing_disabled #> [1] \"hearing disabled\" #>  #> $hearing_impairment #> [1] \"hearing impairment*\" #>  #> $high_incidence_disability #> [1] \"high incidence disabilit*\" #>  #> $intellectual_disability #> [1] \"intellectual disabilit*\" #>  #> $learning_disability #> [1] \"learning disabilit*\" #>  #> $learning_disabled #> [1] \"learning disabled\" #>  #> $learning_handicapped #> [1] \"learning handicapped\" #>  #> $language_impairment #> [1] \"language impairment\" #>  #> $low_incidence_disability #> [1] \"low incidence disabilit*\" #>  #> $low_vision #> [1] \"low vision*\" #>  #> $mathematics_disability #> [1] \"math* disab*\"        \"disabilit* in math*\" #>  #> $mental_handicap #> [1] \"mental* handicap*\" #>  #> $mild_disabilities #> [1] \"mild* disabilit*\" #>  #> $mathematics_learning_disability #> [1] \"learning disabilit* in math*\" \"math* learning disabilit*\"    #>  #> $moderate_disability #> [1] \"moderate disabilit*\" #>  #> $multiple_disabilities #> [1] \"multiple disabilit*\" #>  #> $other_health_impairment #> [1] \"other health impairment\" \"ohi\"                     #>  #> $orthopedic_impairment #> [1] \"orthopedic impairment\" #>  #> $physical_disability #> [1] \"physical disabilit*\" #>  #> $severe_disability #> [1] \"severe* disabilit*\" #>  #> $Speech_or_language_impairment #> [1] \"Speech or language impairment\" #>  #> $Speech_impairment #> [1] \"Speech impairment\" #>  #> $traumatic_brain_injury #> [1] \"traumatic brain injury\" #>  #> $visual_disability #> [1] \"visual disabilit*\" #>  #> $visual_impairment #> [1] \"visual* impair*\" #>"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_2.html","id":null,"dir":"Reference","previous_headings":"","what":"Data-driven dictionary list 2 — dictionary_list_2","title":"Data-driven dictionary list 2 — dictionary_list_2","text":"Contains dictionary using wildcard values compound words SpecialEduTech","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data-driven dictionary list 2 — dictionary_list_2","text":"","code":"dictionary_list_2"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_2.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data-driven dictionary list 2 — dictionary_list_2","text":"object class list length 200.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data-driven dictionary list 2 — dictionary_list_2","text":"","code":"dictionary_list_2 #> $ability #> [1] \"abilities\" #>  #> $access #> [1] \"accesses\" #>  #> $accommodation #> [1] \"accommodations\" #>  #> $achievement #> [1] \"achievements\" #>  #> $activity #> [1] \"activities\" #>  #> $adapt #> [1] \"adapts\" #>  #> $addition #> [1] \"adding\" \"add\"    \"adds\"   \"added\"  #>  #> $agent #> [1] \"agents\" #>  #> $algebra #> [1] \"algebra*\" #>  #> $analysis #> [1] \"analyses\" #>  #> $anchored_instruction #> [1] \"anchor* instruction*\" #>  #> $app #> [1] \"apps\" #>  #> $application #> [1] \"applications\" #>  #> $approach #> [1] \"approaches\" #>  #> $area #> [1] \"areas\" #>  #> $article #> [1] \"articles\" #>  #> $artifact #> [1] \"artifacts\" #>  #> $assess #> [1] \"assessing\" \"assesses\"  \"assessed\"  #>  #> $assessment #> [1] \"assessments\" #>  #> $assign #> [1] \"assigning\" \"assigns\"   \"assigned\"  #>  #> $assignment #> [1] \"assignments\" #>  #> $basic_mathematics #> [1] \"basic math*\" #>  #> $battery #> [1] \"batter*\" #>  #> $benefit #> [1] \"benefits\" #>  #> $calculator #> [1] \"calculators\" #>  #> $case #> [1] \"cases\" #>  #> $challenge #> [1] \"challenges\" #>  #> $change #> [1] \"chaning\" \"changes\" \"changed\" #>  #> $child #> [1] \"children\" #>  #> $class #> [1] \"classes\" #>  #> $classroom #> [1] \"classrooms\" #>  #> $clicker #> [1] \"clickers\" #>  #> $communicate #> [1] \"communicating\" \"communicates\"  \"communicated\"  #>  #> $communication #> [1] \"communications\" #>  #> $computer #> [1] \"computers\" #>  #> $condition #> [1] \"conditions\" #>  #> $concept #> [1] \"concepts\" #>  #> $course #> [1] \"courses\" #>  #> $decimal #> [1] \"decimals\" #>  #> $deficit #> [1] \"deficits\" #>  #> $demonstrate #> [1] \"demonstrating\" \"demonstrates\"  \"demonstrated\"  #>  #> $describe #> [1] \"describing\" \"describes\"  \"described\"  #>  #> $develop #> [1] \"developing\" \"develops\"   \"developed\"  #>  #> $development #> [1] \"developments\" #>  #> $device #> [1] \"devices\" #>  #> $diagram #> [1] \"diagrams\" #>  #> $difference #> [1] \"differences\" #>  #> $difficulty #> [1] \"difficulties\" #>  #> $disability #> [1] \"disabilities\" #>  #> $discuss #> [1] \"discusses\" #>  #> $disorder #> [1] \"disorders\" #>  #> $district #> [1] \"districts\" #>  #> $division #> [1] \"dividing\" \"divide\"   \"divides\"  \"divided\"  #>  #> $domain #> [1] \"domains\" #>  #> $dyscalculia #> [1] \"dyscalcul*\" #>  #> $education #> [1] \"educations\" #>  #> $educator #> [1] \"educators\" #>  #> $effect #> [1] \"effects\"       \"effectiveness\" #>  #> $environment #> [1] \"environments\" #>  #> $error #> [1] \"errors\" #>  #> $evaluate #> [1] \"evaluating\" \"evaluates\"  \"evaluated\"  #>  #> $evaluation #> [1] \"evaluations\" #>  #> $examine #> [1] \"examining\" \"examines\"  \"examined\"  #>  #> $examination #> [1] \"examinations\" #>  #> $exist #> [1] \"exists\" #>  #> $experience #> [1] \"experienc*\" #>  #> $fact #> [1] \"facts\" #>  #> $feature #> [1] \"features\" #>  #> $field #> [1] \"fields\" #>  #> $find #> [1] \"finding\" \"finds\"   \"found\"   #>  #> $form #> [1] \"forms\" #>  #> $formula #> [1] \"formulas\" #>  #> $framework #> [1] \"frameworks\" #>  #> $fractions #> [1] \"fraction\" #>  #> $`function` #> [1] \"functions\" #>  #> $game #> [1] \"games\" #>  #> $geometry #> [1] \"geometr*\" #>  #> $grade #> [1] \"grades\" #>  #> $graph #> [1] \"graphs\" #>  #> $group #> [1] \"groups\" #>  #> $guideline #> [1] \"guidelines\" #>  #> $handheld #> [1] \"hand held*\" #>  #> $identify #> [1] \"identif*\" #>  #> $image #> [1] \"images\" #>  #> $impairment #> [1] \"impairments\" #>  #> $implication #> [1] \"implications\" #>  #> $improvement #> [1] \"improvements\" #>  #> $include #> [1] \"includ*\" #>  #> $individual #> [1] \"individuals\" #>  #> $instruction #> [1] \"instructions\" #>  #> $interface #> [1] \"interfaces\" #>  #> $intervention #> [1] \"interventions\" #>  #> $involve #> [1] \"involv*\" #>  #> $iPad #> [1] \"ipad*\" #>  #> $iPod #> [1] \"ipod*\" #>  #> $learner #> [1] \"learners\" #>  #> $learning #> [1] \"learn\"   \"learns\"  \"learned\" #>  #> $lesson #> [1] \"lessons\" #>  #> $level #> [1] \"levels\" #>  #> $literature #> [1] \"literatures\" #>  #> $Kindle #> [1] \"kindle*\" #>  #> $manipulatives #> [1] \"manipulative\" #>  #> $map #> [1] \"maps\" #>  #> $material #> [1] \"materials\" #>  #> $measure #> [1] \"measur*\" #>  #> $mathematics #> [1] \"math\" #>  #> $method #> [1] \"methods\" #>  #> $mode #> [1] \"modes\" #>  #> $model #> [1] \"models\"   \"modeled\"  \"modelled\" #>  #> $modeling #> [1] \"modelling\" #>  #> $multiplication #> [1] \"multiplying\" \"multiply\"    \"multiplies\"  \"multiplied\"  #>  #> $need #> [1] \"needs\" #>  #> $number #> [1] \"numbers\" #>  #> $on_task #> [1] \"on task\" #>  #> $opportunity #> [1] \"opportunities\" #>  #> $outcome #> [1] \"outcomes\" #>  #> $participant #> [1] \"participants\" #>  #> $peer #> [1] \"peers\" #>  #> $performance #> [1] \"performances\" #>  #> $phase #> [1] \"phases\" #>  #> $possibility #> [1] \"possibilities\" #>  #> $practice #> [1] \"practices\" #>  #> $preference #> [1] \"preferences\" #>  #> $present #> [1] \"presenting\" \"presents\"   \"presented\"  #>  #> $presentation #> [1] \"presentations\" #>  #> $print #> [1] \"prints\" #>  #> $probe #> [1] \"probes\" #>  #> $procedure #> [1] \"procedures\" #>  #> $process #> [1] \"processes\" #>  #> $problem #> [1] \"problems\" #>  #> $program #> [1] \"programs\" #>  #> $prompt #> [1] \"prompts\" #>  #> $provide #> [1] \"providing\" \"provides\"  \"provided\"  #>  #> $publish #> [1] \"publish*\" #>  #> $pupil #> [1] \"pupils\" #>  #> $quality #> [1] \"qualities\" #>  #> $question #> [1] \"questions\" #>  #> $rate #> [1] \"rates\" #>  #> $reading #> [1] \"read*\" #>  #> $recommendation #> [1] \"recommendations\" #>  #> $reinforcement #> [1] \"reinforcement*\" #>  #> $relation #> [1] \"relations\" #>  #> $represent #> [1] \"representing\" \"represents\"   \"represented\"  #>  #> $representation #> [1] \"representations\" #>  #> $require #> [1] \"requires\" #>  #> $resource #> [1] \"resources\" #>  #> $response #> [1] \"responses\" #>  #> $result #> [1] \"results\" #>  #> $review #> [1] \"review*\" #>  #> $robot #> [1] \"robots\" #>  #> $school #> [1] \"schools\" #>  #> $score #> [1] \"scores\" #>  #> $session #> [1] \"sessions\" #>  #> $setting #> [1] \"settings\" #>  #> $show #> [1] \"showing\" \"shows\"   \"showed\"  #>  #> $simulation #> [1] \"simulations\" #>  #> $shape #> [1] \"shapes\" #>  #> $skill #> [1] \"skills\" #>  #> $software #> [1] \"softwares\" #>  #> $solving #> [1] \"solve\"  \"solves\" \"solved\" #>  #> $standard #> [1] \"standards\" #>  #> $step #> [1] \"steps\" #>  #> $strategy #> [1] \"strategies\" #>  #> $student #> [1] \"students\" \"ss\"       #>  #> $study #> [1] \"studies\"  \"studying\" \"studied\"  #>  #> $subject #> [1] \"subjects\" #>  #> $subtraction #> [1] \"subtracting\" \"subtract\"    \"subtracted\"  #>  #> $support #> [1] \"support*\" #>  #> $symmetry #> [1] \"symmetries\" #>  #> $system #> [1] \"systems\" #>  #> $tablet #> [1] \"tablets\" #>  #> $task #> [1] \"tasks\" #>  #> $teacher #> [1] \"teachers\" #>  #> $teach #> [1] \"teaching\" \"teaches\"  \"taught\"   #>  #> $technology #> [1] \"technologies\" #>  #> $term #> [1] \"terms\" #>  #> $testing #> [1] \"test\"   \"tests\"  \"tested\" #>  #> $text #> [1] \"texts\" #>  #> $textbook #> [1] \"textbooks\" #>  #> $time #> [1] \"times\" #>  #> $tool #> [1] \"tools\" #>  #> $topic #> [1] \"topics\" #>  #> $treatment #> [1] \"treatments\" #>  #> $type #> [1] \"types\" #>  #> $understanding #> [1] \"understand\"  \"understood\"  \"understands\" #>  #> $unit #> [1] \"units\" #>  #> $universal_design #> [1] \"universal* design*\" #>  #> $use #> [1] \"using\" \"uses\"  \"used\"  #>  #> $user #> [1] \"users\" #>  #> $variable #> [1] \"variables\" #>  #> $video #> [1] \"videos\" #>  #> $virtual_abstract #> [1] \"virtual abstract\" #>  #> $virtual_representational #> [1] \"virtual representational\" #>  #> $visual #> [1] \"visuals\" #>  #> $way #> [1] \"ways\" #>  #> $web #> [1] \"web*\" #>  #> $week #> [1] \"weeks\" #>  #> $writer #> [1] \"writers\" #>  #> $writing #> [1] \"write\"   \"wrote\"   \"writes\"  \"written\" #>  #> $year #> [1] \"years\" #>"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/evaluate_optimal_topic_number.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate Optimal Number of Topics — evaluate_optimal_topic_number","title":"Evaluate Optimal Number of Topics — evaluate_optimal_topic_number","text":"function performs search optimal number topics (K) using stm::searchK visualizes diagnostics, including held-likelihood, residuals, semantic coherence, lower bound metrics.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/evaluate_optimal_topic_number.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate Optimal Number of Topics — evaluate_optimal_topic_number","text":"","code":"evaluate_optimal_topic_number(   dfm_object,   topic_range,   max.em.its = 75,   categorical_var = NULL,   continuous_var = NULL,   height = 600,   width = 800,   verbose = TRUE,   ... )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/evaluate_optimal_topic_number.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate Optimal Number of Topics — evaluate_optimal_topic_number","text":"dfm_object quanteda document-feature matrix (dfm). topic_range numeric vector specifying range topics (K) search . max.em.Maximum number EM iterations (default: 75). categorical_var optional character string categorical variable metadata. continuous_var optional character string continuous variable metadata. height height resulting Plotly plot pixels (default: 600). width width resulting Plotly plot pixels (default: 800). verbose Logical; TRUE, prints progress information. ... arguments passed stm::searchK.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/evaluate_optimal_topic_number.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate Optimal Number of Topics — evaluate_optimal_topic_number","text":"plotly object showing diagnostics number topics (K).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/evaluate_optimal_topic_number.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate Optimal Number of Topics — evaluate_optimal_topic_number","text":"","code":"if (interactive()) {   df <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))    tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    optimal_topic_range <- TextAnalysisR::evaluate_optimal_topic_number(                            dfm_object = dfm_object,                            topic_range = 5:30,                            max.em.its = 75,                            categorical_var = \"reference_type\",                            continuous_var = \"year\",                            height = 600,                            width = 800,                            verbose = TRUE)   print(optimal_topic_range) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","title":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","text":"function generates descriptive labels topic based top terms using OpenAI's ChatCompletion API.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","text":"","code":"generate_topic_labels(   top_topic_terms,   model = \"gpt-3.5-turbo\",   system = NULL,   user = NULL,   temperature = 0.5,   openai_api_key = NULL,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","text":"top_topic_terms data frame containing top terms topic. model character string specifying OpenAI model use (default: \"gpt-3.5-turbo\"). system character string containing system prompt OpenAI API. NULL, function uses default system prompt. user character string containing user prompt OpenAI API. NULL, function uses default user prompt. temperature numeric value controlling randomness output (default: 0.5). openai_api_key character string containing OpenAI API key. NULL, function attempts load key OPENAI_API_KEY environment variable .env file working directory. verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","text":"data frame containing top terms topic along generated labels.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","text":"","code":"if (interactive()) {   df <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))    tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    out <- quanteda::convert(dfm_object, to = \"stm\")  stm_15 <- stm::stm(   data = out$meta,   documents = out$documents,   vocab = out$vocab,   max.em.its = 75,   init.type = \"Spectral\",   K = 15,   prevalence = ~ reference_type + s(year),   verbose = TRUE)  top_topic_terms <- TextAnalysisR::select_top_topic_terms(   stm_model = stm_15,   top_term_n = 10,   verbose = TRUE   )  top_labeled_topic_terms <- TextAnalysisR::generate_topic_labels(   top_topic_terms,   model = \"gpt-3.5-turbo\",   temperature = 0.5,   openai_api_key = \"your_openai_api_key\",   verbose = TRUE) print(top_labeled_topic_terms)  # You can also load the Open AI API key from the .env file in the working directory as follows: # OPENAI_API_KEY=your_openai_api_key  top_labeled_topic_terms <- TextAnalysisR::generate_topic_labels(   top_topic_terms,   model = \"gpt-3.5-turbo\",   temperature = 0.5,   verbose = TRUE) print(top_labeled_topic_terms) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_frequency.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Word Frequency — plot_word_frequency","title":"Plot Word Frequency — plot_word_frequency","text":"Given document-feature matrix (dfm), function computes frequent terms creates ggplot-based visualization term frequencies.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_frequency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Word Frequency — plot_word_frequency","text":"","code":"plot_word_frequency(dfm_object, n = 20, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_frequency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Word Frequency — plot_word_frequency","text":"dfm_object quanteda dfm object. n number top terms display (default: 20). ... arguments passed quanteda.textstats::textstat_frequency.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_frequency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Word Frequency — plot_word_frequency","text":"ggplot object visualizing top terms frequency. plot shows term one axis frequency , points representing observed frequencies.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_frequency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Word Frequency — plot_word_frequency","text":"","code":"if (interactive()) {   df <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))    tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    word_frequency_plot <- TextAnalysisR::plot_word_frequency(dfm_object, n = 20)   print(word_frequency_plot) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/preprocess_texts.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Text Data — preprocess_texts","title":"Preprocess Text Data — preprocess_texts","text":"Preprocesses text data : Constructing corpus Tokenizing text words Converting lowercase Specifying minimum token length. Typically used constructing dfm fitting STM model.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/preprocess_texts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Text Data — preprocess_texts","text":"","code":"preprocess_texts(   united_tbl,   text_field = \"united_texts\",   min_char = 2,   remove_punct = TRUE,   remove_symbols = TRUE,   remove_numbers = TRUE,   remove_url = TRUE,   remove_separators = TRUE,   split_hyphens = TRUE,   split_tags = TRUE,   include_docvars = TRUE,   keep_acronyms = FALSE,   padding = FALSE,   verbose = FALSE,   ... )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/preprocess_texts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Text Data — preprocess_texts","text":"united_tbl data frame contains text data. text_field name column contains text data. min_char minimum number characters token included (default: 2). remove_punct Logical; remove punctuation text (default: TRUE). remove_symbols Logical; remove symbols text (default: TRUE). remove_numbers Logical; remove numbers text (default: TRUE). remove_url Logical; remove URLs text (default: TRUE). remove_separators Logical; remove separators text (default: TRUE). split_hyphens Logical; split hyphenated words separate tokens (default: TRUE). split_tags Logical; split tags separate tokens (default: TRUE). include_docvars Logical; include document variables tokens object (default: TRUE). keep_acronyms Logical; keep acronyms text (default: FALSE). padding Logical; add padding tokens object (default: FALSE). verbose Logical; print verbose output (default: FALSE). ... Additional arguments passed quanteda::tokens.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/preprocess_texts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Text Data — preprocess_texts","text":"tokens object contains preprocessed text data.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/preprocess_texts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess Text Data — preprocess_texts","text":"","code":"if (interactive()) { df <- TextAnalysisR::SpecialEduTech  united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))  tokens <- TextAnalysisR::preprocess_texts(united_tbl,                                          text_field = \"united_texts\",                                          min_char = 2,                                          remove_punct = TRUE,                                          remove_symbols = TRUE,                                          remove_numbers = TRUE,                                          remove_url = TRUE,                                          remove_separators = TRUE,                                          split_hyphens = TRUE,                                          split_tags = TRUE,                                          include_docvars = TRUE,                                          keep_acronyms = FALSE,                                          padding = FALSE,                                          verbose = FALSE) print(tokens) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_files.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Files — process_files","title":"Process Files — process_files","text":"function processes different types files text input based dataset choice.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_files.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Files — process_files","text":"","code":"process_files(dataset_choice, file_info = NULL, text_input = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_files.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Files — process_files","text":"dataset_choice character string indicating dataset choice. file_info data frame containing file information column named 'filepath' (default: NULL). text_input character string containing text input (default: NULL).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_files.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Files — process_files","text":"data frame containing processed data.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_files.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process Files — process_files","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::process_files(dataset_choice = \"Upload an Example Dataset\")   head(mydata)    file_info <- data.frame(filepath = \"inst/extdata/SpecialEduTech.xlsx\")   mydata <- TextAnalysisR::process_files(dataset_choice = \"Upload Your File\",                                           file_info = file_info)   head(mydata)    text_input <- paste0(\"The purpose of this study was to conduct a content analysis of \",                        \"research on technology use.\")   mydata <- TextAnalysisR::process_files(dataset_choice = \"Copy and Paste Text\",                                           text_input = text_input)   head(mydata) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/select_top_topic_terms.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Top Terms for Each Topic — select_top_topic_terms","title":"Select Top Terms for Each Topic — select_top_topic_terms","text":"function selects top terms topic based word probability distribution (beta).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/select_top_topic_terms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Top Terms for Each Topic — select_top_topic_terms","text":"","code":"select_top_topic_terms(stm_model, top_term_n = 10, verbose = TRUE, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/select_top_topic_terms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Top Terms for Each Topic — select_top_topic_terms","text":"stm_model STM model object. top_term_n number top terms display topic (default: 10). verbose Logical, TRUE, prints progress messages. ... arguments passed tidytext::tidy.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/select_top_topic_terms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Top Terms for Each Topic — select_top_topic_terms","text":"data frame containing top terms topic.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/select_top_topic_terms.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select Top Terms for Each Topic — select_top_topic_terms","text":"","code":"if (interactive()) {   df <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))    tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    stm_15 <- TextAnalysisR::create_stm_model(   dfm_object,   topic_n = 15,   max.em.its = 75,   categorical_var = \"reference_type\",   continuous_var = \"year\",   verbose = TRUE   )    out <- quanteda::convert(dfm_object, to = \"stm\")  stm_15 <- stm::stm(   data = out$meta,   documents = out$documents,   vocab = out$vocab,   max.em.its = 75,   init.type = \"Spectral\",   K = 15,   prevalence = ~ reference_type + s(year),   verbose = TRUE)  top_topic_terms <- TextAnalysisR::select_top_topic_terms(   stm_model = stm_15,   top_term_n = 10,   verbose = TRUE   ) print(top_topic_terms) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/SpecialEduTech.html","id":null,"dir":"Reference","previous_headings":"","what":"Special education technology bibliographic data — SpecialEduTech","title":"Special education technology bibliographic data — SpecialEduTech","text":"Contains bibliographic data journal articles dissertations use technology teaching mathematics students disabilities.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/SpecialEduTech.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Special education technology bibliographic data — SpecialEduTech","text":"","code":"SpecialEduTech"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/SpecialEduTech.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Special education technology bibliographic data — SpecialEduTech","text":"object class tbl_df (inherits tbl, data.frame) 490 rows 6 columns.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/SpecialEduTech.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Special education technology bibliographic data — SpecialEduTech","text":"","code":"SpecialEduTech #> # A tibble: 490 × 6 #>    reference_type  author                            year title keyword abstract #>    <chr>           <chr>                            <dbl> <chr> <chr>   <chr>    #>  1 journal_article Block, G. H.                      1980 Dysc… Arithm… Notes t… #>  2 thesis          Bukatman, K. L.                   1981 The … locus … This st… #>  3 journal_article Watkins, M. W., & Webb, C.        1981 Comp… Comput… Results… #>  4 journal_article Chaffin, J. D.                    1982 Arc-… Comput… The Arc… #>  5 journal_article Chaffin, J. D., Maxwell, B., & …  1982 ARC-… Electr… This ar… #>  6 thesis          Golden, C. K.                     1982 The … NA      The pur… #>  7 journal_article Neal, D.                          1982 A re… tradit… Discuss… #>  8 thesis          Englebert, B. B.                  1983 A st… microc… The pur… #>  9 thesis          Foster, K.                        1983 The … comput… The eff… #> 10 journal_article Pommer, L. T.                     1983 Usin… Comput… The art… #> # ℹ 480 more rows"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stm_15.html","id":null,"dir":"Reference","previous_headings":"","what":"An example structure of a structural topic model — stm_15","title":"An example structure of a structural topic model — stm_15","text":"Contains 15 topics, topic prevalences, etc. stm::stm.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stm_15.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An example structure of a structural topic model — stm_15","text":"","code":"stm_15"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stm_15.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"An example structure of a structural topic model — stm_15","text":"object class STM length 11.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stm_15.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"An example structure of a structural topic model — stm_15","text":"","code":"stm_15 #> A topic model with 15 topics, 488 documents and a 4511 word dictionary."},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stopwords_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Data-driven stopword list — stopwords_list","title":"Data-driven stopword list — stopwords_list","text":"Contains stopword list created based inverse document frequency SpecialEduTech","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stopwords_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data-driven stopword list — stopwords_list","text":"","code":"stopwords_list"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stopwords_list.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data-driven stopword list — stopwords_list","text":"object class character length 117.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stopwords_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data-driven stopword list — stopwords_list","text":"","code":"stopwords_list #>   [1] \"ability\"           \"able\"              \"across\"            #>   [4] \"activity\"          \"also\"              \"although\"          #>   [7] \"among\"             \"analysis\"          \"analyzed\"          #>  [10] \"approach\"          \"article\"           \"based\"             #>  [13] \"can\"               \"child\"             \"class\"             #>  [16] \"classroom\"         \"compared\"          \"completed\"         #>  [19] \"condition\"         \"conducted\"         \"content\"           #>  [22] \"control\"           \"current\"           \"data\"              #>  [25] \"demonstrate\"       \"describe\"          \"design\"            #>  [28] \"determine\"         \"disability\"        \"discussed\"         #>  [31] \"education\"         \"educator\"          \"effect\"            #>  [34] \"effective\"         \"examine\"           \"explored\"          #>  [37] \"find\"              \"findings\"          \"five\"              #>  [40] \"four\"              \"future\"            \"general\"           #>  [43] \"given\"             \"grade\"             \"group\"             #>  [46] \"help\"              \"however\"           \"i.e\"               #>  [49] \"implication\"       \"important\"         \"improve\"           #>  [52] \"improved\"          \"improvement\"       \"include\"           #>  [55] \"increase\"          \"increased\"         \"indicate\"          #>  [58] \"indicated\"         \"instruction\"       \"intervention\"      #>  [61] \"involve\"           \"learning\"          \"limitations\"       #>  [64] \"limited\"           \"literature\"        \"many\"              #>  [67] \"mathematics\"       \"may\"               \"method\"            #>  [70] \"often\"             \"one\"               \"outcome\"           #>  [73] \"overall\"           \"paper\"             \"participant\"       #>  [76] \"participated\"      \"performance\"       \"posttest\"          #>  [79] \"present\"           \"pretest\"           \"problem\"           #>  [82] \"provide\"           \"purpose\"           \"reading\"           #>  [85] \"regarding\"         \"research\"          \"researchers\"       #>  [88] \"result\"            \"revealed\"          \"review\"            #>  [91] \"school\"            \"science\"           \"setting\"           #>  [94] \"several\"           \"show\"              \"significant\"       #>  [97] \"significantly\"     \"skill\"             \"special_education\" #> [100] \"student\"           \"study\"             \"suggest\"           #> [103] \"support\"           \"teach\"             \"teacher\"           #> [106] \"technology\"        \"three\"             \"traditional\"       #> [109] \"treatment\"         \"two\"               \"use\"               #> [112] \"way\"               \"well\"              \"whether\"           #> [115] \"within\"            \"without\"           \"yet\""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/TextAnalysisR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"TextAnalysisR: A text mining workflow tool — TextAnalysisR-package","title":"TextAnalysisR: A text mining workflow tool — TextAnalysisR-package","text":"'TextAnalysisR' provides supporting workflow tool text mining analysis. web app incorporates 'quanteda' (text preprocessing), 'stm' (structural topic modeling), 'ggraph', 'widyr' (network analysis). 'tidytext' implemented tidy non-tidy format objects. R Shiny web app available 'TextAnalysisR::TextAnalysisR.app()' 'https://textanalysisr.org'. Functions provided completing word-topic probabilities, document-topic probabilities, estimated effects covariates topic prevalence, network analysis, similar tasks available web app.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/TextAnalysisR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"TextAnalysisR: A text mining workflow tool — TextAnalysisR-package","text":"Maintainer: Mikyung Shin shin.mikyung@gmail.com (ORCID)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/TextAnalysisR.app.html","id":null,"dir":"Reference","previous_headings":"","what":"Launch and browse the TextAnalysisR app — TextAnalysisR.app","title":"Launch and browse the TextAnalysisR app — TextAnalysisR.app","text":"Launch browse TextAnalysisR app.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/TextAnalysisR.app.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Launch and browse the TextAnalysisR app — TextAnalysisR.app","text":"","code":"TextAnalysisR.app()"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/TextAnalysisR.app.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Launch and browse the TextAnalysisR app — TextAnalysisR.app","text":"","code":"if (interactive()) {   library(TextAnalysisR)   TextAnalysisR.app() }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/topic_probability_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Per-Document Per-Topic Probabilities — topic_probability_plot","title":"Plot Per-Document Per-Topic Probabilities — topic_probability_plot","text":"function generates bar plot showing prevalence topic across documents.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/topic_probability_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Per-Document Per-Topic Probabilities — topic_probability_plot","text":"","code":"topic_probability_plot(   stm_model,   top_n = 10,   height = 800,   width = 1000,   verbose = TRUE,   ... )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/topic_probability_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Per-Document Per-Topic Probabilities — topic_probability_plot","text":"stm_model fitted STM model object. stm_model fitted Structural Topic Model created using stm::stm(). top_n number topics display, ordered mean prevalence. height height resulting Plotly plot, pixels (default: 800). width width resulting Plotly plot, pixels (default: 1000). verbose Logical, TRUE, prints progress messages. ... arguments passed tidytext::tidy.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/topic_probability_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Per-Document Per-Topic Probabilities — topic_probability_plot","text":"ggplot object showing bar plot topic prevalence. Topics ordered mean gamma value (average prevalence across documents).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/topic_probability_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Per-Document Per-Topic Probabilities — topic_probability_plot","text":"","code":"if (interactive()) {  df <- TextAnalysisR::SpecialEduTech   united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))   tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")   dfm_object <- quanteda::dfm(tokens)   out <- quanteda::convert(dfm_object, to = \"stm\")  stm_15 <- stm::stm(   data = out$meta,   documents = out$documents,   vocab = out$vocab,   max.em.its = 75,   init.type = \"Spectral\",   K = 15,   prevalence = ~ reference_type + s(year),   verbose = TRUE)  topic_probability_plot <- TextAnalysisR::topic_probability_plot(  stm_model= stm_15,  top_n = 10,  height = 800,  width = 1000,  verbose = TRUE)  print(topic_probability_plot) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/topic_probability_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Table for Per-Document Per-Topic Probabilities — topic_probability_table","title":"Create a Table for Per-Document Per-Topic Probabilities — topic_probability_table","text":"function generates table mean topic prevalence across documents.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/topic_probability_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Table for Per-Document Per-Topic Probabilities — topic_probability_table","text":"","code":"topic_probability_table(stm_model, top_n = 10, verbose = TRUE, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/topic_probability_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Table for Per-Document Per-Topic Probabilities — topic_probability_table","text":"stm_model fitted STM model object. top_n number topics display, ordered mean prevalence. verbose Logical, TRUE, prints progress messages. ... arguments passed tidytext::tidy.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/topic_probability_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Table for Per-Document Per-Topic Probabilities — topic_probability_table","text":"tibble containing columns topic gamma, topic factor representing topic (relabeled \"Topic X\" format), gamma mean topic prevalence across documents. Numeric values rounded three decimal places.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/topic_probability_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Table for Per-Document Per-Topic Probabilities — topic_probability_table","text":"","code":"if (interactive()) {  df <- TextAnalysisR::SpecialEduTech   united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))   tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")   dfm_object <- quanteda::dfm(tokens)   out <- quanteda::convert(dfm_object, to = \"stm\")  stm_15 <- stm::stm(   data = out$meta,   documents = out$documents,   vocab = out$vocab,   max.em.its = 75,   init.type = \"Spectral\",   K = 15,   prevalence = ~ reference_type + s(year),   verbose = TRUE)  topic_probability_table <- TextAnalysisR::topic_probability_table(    stm_model= stm_15,    top_n = 10,    verbose = TRUE)  print(topic_probability_table) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/unite_text_cols.html","id":null,"dir":"Reference","previous_headings":"","what":"Unite Text Columns — unite_text_cols","title":"Unite Text Columns — unite_text_cols","text":"function unites specified text columns data frame single column named \"united_texts\" retaining original columns.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/unite_text_cols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unite Text Columns — unite_text_cols","text":"","code":"unite_text_cols(df, listed_vars)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/unite_text_cols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unite Text Columns — unite_text_cols","text":"df data frame contains text data. listed_vars character vector column names united \"united_texts\".","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/unite_text_cols.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unite Text Columns — unite_text_cols","text":"data frame new column \"united_texts\" created uniting specified variables.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/unite_text_cols.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unite Text Columns — unite_text_cols","text":"","code":"if (interactive()) {   df <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))   print(united_tbl) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_correlation_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze and Visualize Word Correlation Networks — word_correlation_network","title":"Analyze and Visualize Word Correlation Networks — word_correlation_network","text":"function creates word correlation network based document-feature matrix (dfm).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_correlation_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze and Visualize Word Correlation Networks — word_correlation_network","text":"","code":"word_correlation_network(   dfm_object,   doc_var = NULL,   common_term_n = 130,   corr_n = 0.4,   top_node_n = 40,   nrows = 1,   height = 1000,   width = 900 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_correlation_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze and Visualize Word Correlation Networks — word_correlation_network","text":"dfm_object quanteda document-feature matrix (dfm). doc_var document-level metadata variable (default: NULL). common_term_n Minimum number common terms filtering terms (default: 30). corr_n Minimum correlation value filtering terms (default: 0.4). top_node_n Number top nodes display (default: 40). nrows Number rows display table (default: 1). height height resulting Plotly plot, pixels (default: 1000). width width resulting Plotly plot, pixels (default: 900).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_correlation_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze and Visualize Word Correlation Networks — word_correlation_network","text":"list containing Plotly plot, data frame network layout, igraph graph object.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_correlation_network.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analyze and Visualize Word Correlation Networks — word_correlation_network","text":"","code":"if (interactive()) {   df <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))    tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    word_correlation_network_results <- TextAnalysisR::word_correlation_network(                                       dfm_object,                                       doc_var = \"reference_type\",                                       common_term_n = 30,                                       corr_n = 0.4,                                       top_node_n = 40,                                       nrows = 1,                                       height = 1000,                                       width = 900)   print(word_correlation_network_results$plot)   print(word_correlation_network_results$table)   print(word_correlation_network_results$summary) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_co_occurrence_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze and Visualize Word Co-occurrence Networks — word_co_occurrence_network","title":"Analyze and Visualize Word Co-occurrence Networks — word_co_occurrence_network","text":"function creates word co-occurrence network based document-feature matrix (dfm).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_co_occurrence_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze and Visualize Word Co-occurrence Networks — word_co_occurrence_network","text":"","code":"word_co_occurrence_network(   dfm_object,   doc_var = NULL,   co_occur_n = 50,   top_node_n = 30,   nrows = 1,   height = 800,   width = 900 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_co_occurrence_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze and Visualize Word Co-occurrence Networks — word_co_occurrence_network","text":"dfm_object quanteda document-feature matrix (dfm). doc_var document-level metadata variable (default: NULL). co_occur_n Minimum number co-occurrences filtering terms (default: 50). top_node_n Number top nodes display (default: 30). nrows Number rows display table (default: 1). height height resulting Plotly plot, pixels (default: 800). width width resulting Plotly plot, pixels (default: 900).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_co_occurrence_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze and Visualize Word Co-occurrence Networks — word_co_occurrence_network","text":"list containing Plotly plot, data frame network layout, igraph graph object.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_co_occurrence_network.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analyze and Visualize Word Co-occurrence Networks — word_co_occurrence_network","text":"","code":"if (interactive()) {   df <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))    tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    word_co_occurrence_network_results <- TextAnalysisR::word_co_occurrence_network(                                         dfm_object,                                         doc_var = \"reference_type\",                                         co_occur_n = 50,                                         top_node_n = 30,                                         nrows = 1,                                         height = 800,                                         width = 900)   print(word_co_occurrence_network_results$plot)   print(word_co_occurrence_network_results$table)   print(word_co_occurrence_network_results$summary) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_frequency_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze and Visualize Word Frequencies Across a Continuous Variable — word_frequency_distribution","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — word_frequency_distribution","text":"function analyzes visualizes word frequencies across continuous variable.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_frequency_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — word_frequency_distribution","text":"","code":"word_frequency_distribution(   dfm_object,   continuous_variable,   selected_terms,   height = 500,   width = 900 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_frequency_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — word_frequency_distribution","text":"dfm_object quanteda document-feature matrix (dfm). continuous_variable continuous variable metadata. selected_terms vector terms analyze trends . height height resulting Plotly plot, pixels (default: 500). width width resulting Plotly plot, pixels (default: 900).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_frequency_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — word_frequency_distribution","text":"list containing Plotly objects tables results.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_frequency_distribution.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — word_frequency_distribution","text":"function requires fitted STM model object quanteda dfm object. continuous variable column metadata dfm object. selected terms vector terms analyze trends . required packages 'htmltools', 'splines', 'broom' (plus additional ones loaded internally).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_frequency_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — word_frequency_distribution","text":"","code":"if (interactive()) {   df <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))    tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    word_frequency_distribution_results <- TextAnalysisR::word_frequency_distribution(                              dfm_object,                              continuous_variable = \"year\",                              selected_terms = c(\"calculator\", \"computer\"),                              height = 500,                              width = 900)   print(word_frequency_distribution_results$plot)   print(word_frequency_distribution_results$table) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_probability_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Highest Word Probabilities for Each Topic — word_probability_plot","title":"Plot Highest Word Probabilities for Each Topic — word_probability_plot","text":"function provides visualization top terms topic, ordered word probability distribution topic (beta).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_probability_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Highest Word Probabilities for Each Topic — word_probability_plot","text":"","code":"word_probability_plot(   top_topic_terms,   topic_label = NULL,   ncol = 3,   height = 1200,   width = 800,   ... )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_probability_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Highest Word Probabilities for Each Topic — word_probability_plot","text":"top_topic_terms data frame containing top terms topic. topic_label character vector topic labels topic. NULL, function uses topic number. ncol number columns facet plot (default: 3). height height resulting Plotly plot, pixels (default: 1200). width width resulting Plotly plot, pixels (default: 800). ... Additional arguments passed plotly::layout.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_probability_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Highest Word Probabilities for Each Topic — word_probability_plot","text":"Plotly object showing facet-wrapped chart top terms topic, ordered per-topic probability (beta). facet represents topic.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_probability_plot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Highest Word Probabilities for Each Topic — word_probability_plot","text":"function uses ggplot2 package create facet-wrapped chart top terms topic,","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/word_probability_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Highest Word Probabilities for Each Topic — word_probability_plot","text":"","code":"if (interactive()) {  df <- TextAnalysisR::SpecialEduTech   united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c(\"title\", \"keyword\", \"abstract\"))   tokens <- TextAnalysisR::preprocess_texts(united_tbl, text_field = \"united_texts\")   dfm_object <- quanteda::dfm(tokens)   out <- quanteda::convert(dfm_object, to = \"stm\")  stm_15 <- stm::stm(   data = out$meta,   documents = out$documents,   vocab = out$vocab,   max.em.its = 75,   init.type = \"Spectral\",   K = 15,   prevalence = ~ reference_type + s(year),   verbose = TRUE)  top_topic_terms <- TextAnalysisR::select_top_topic_terms(   stm_model = stm_15,   top_term_n = 10,   verbose = TRUE   )  top_labeled_topic_terms <- TextAnalysisR::generate_topic_labels(   top_topic_terms = top_topic_terms,   model = \"gpt-3.5-turbo\",   temperature = 0.5,   openai_api_key = \"your_openai_api_key\",   verbose = TRUE) top_labeled_topic_terms   TextAnalysisR::plot_word_probabilities(   top_labeled_topic_terms,   topic_label = \"topic_label\",   ncol = 3,   height = 1200,   width = 800   )  TextAnalysisR::plot_word_probabilities(   top_topic_terms,   ncol = 3,   height = 1200,   width = 800   )    manual_labels <- c(\"1\" = \"Mathematical technology for students with LD\",                     \"2\" = \"STEM technology\",                     \"3\" = \"CAI for math problem solving\")  word_probability_plot <- TextAnalysisR::word_probability_plot(                          top_topic_terms,                          topic_label = manual_labels,                          ncol = 3,                          height = 1200,                          width = 800) print(word_probability_plot)  }"},{"path":"https://mshin77.github.io/TextAnalysisR/news/index.html","id":"textanalysisr-001-2023-10-18","dir":"Changelog","previous_headings":"","what":"TextAnalysisR 0.0.1 (2023-10-18)","title":"TextAnalysisR 0.0.1 (2023-10-18)","text":"CRAN Submission TextAnalysisR 0.0.1","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/news/index.html","id":"textanalysisr-002-2024-12-05","dir":"Changelog","previous_headings":"","what":"TextAnalysisR 0.0.2 (2024-12-05)","title":"TextAnalysisR 0.0.2 (2024-12-05)","text":"Added reference : Shin, M., Ok, M. W., Choo, S., Hossain, G., Bryant, D. P., & Kang, E. (2023) DESCRIPTION. Quoted software package names DESCRIPTION field. Improved documentation follow CRAN policies.","code":""}]
