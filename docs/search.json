[{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/cybersecurity.html","id":"api-key-storage","dir":"Articles","previous_headings":"","what":"API Key Storage","title":"Security","text":"App: Secure password field, session-storage Environment Variable: Encrypted (Advanced):","code":"Sys.setenv(OPENAI_API_KEY = \"sk-...\") keyring::key_set(\"openai_api\")"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/cybersecurity.html","id":"privacy","dir":"Articles","previous_headings":"","what":"Privacy","title":"Security","text":"100% local processing R package Optional local AI via Ollama HIPAA/FERPA compliant Works offline","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/function_index.html","id":"launch-app","dir":"Articles","previous_headings":"","what":"Launch App","title":"Function Reference Cheatsheet","text":"Shiny app provides interactive interface functions:","code":"run_app()"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/installation.html","id":"r-package","dir":"Articles","previous_headings":"","what":"R Package","title":"Installation","text":"Requirements: R >= 4.0, RStudio recommended","code":"install.packages(\"remotes\") remotes::install_github(\"mshin77/TextAnalysisR\") library(TextAnalysisR) run_app()"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/installation.html","id":"web-app","dir":"Articles","previous_headings":"","what":"Web App","title":"Installation","text":"Visit textanalysisr.org - installation needed. Note: Web version limited features (Python, AI, large files).","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/installation.html","id":"linguistic-analysis-spacy","dir":"Articles","previous_headings":"Optional Features","what":"Linguistic Analysis (spaCy)","title":"Installation","text":"lemmatization, POS tagging, named entity recognition:","code":"install.packages(\"spacyr\") spacyr::spacy_install()"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/installation.html","id":"python-features","dir":"Articles","previous_headings":"Optional Features","what":"Python Features","title":"Installation","text":"PDF tables, embeddings, AI-assisted analysis: Requires Python 3.9+ optionally Ollama local AI.","code":"setup_python_env()"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/lexical_analysis.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Lexical Analysis","text":"","code":"library(TextAnalysisR)  mydata <- SpecialEduTech united_tbl <- unite_cols(mydata, listed_vars = c(\"title\", \"keyword\", \"abstract\")) tokens <- prep_texts(united_tbl, text_field = \"united_texts\") dfm_object <- quanteda::dfm(tokens)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/lexical_analysis.html","id":"word-frequency","dir":"Articles","previous_headings":"","what":"Word Frequency","title":"Lexical Analysis","text":"Find distinctive words per document using Term Frequency-Inverse Document Frequency: TF-IDF weights terms frequent document rare across corpus, identifying distinctive vocabulary. Compare word usage groups: Keyness analysis identifies statistically significant differences word usage groups. N-grams sequences consecutive words frequently appear together. capture multi-word expressions like “machine learning” “New York City” carry meaning complete phrases. Types: Bigrams: 2-word sequences (e.g., “data analysis”) Trigrams: 3-word sequences (e.g., “natural language processing”) 4-grams & 5-grams: Longer phrases (e.g., “statistical significance test results”) Usage: Set minimum frequency (often phrases appear) lambda (collocation strength) detect meaningful multi-word expressions. Learn : Text Mining R - N-grams Chapter Part--speech (POS) tagging identifies grammatical category word. Requires Python spaCy. Tags (Universal Dependencies): NOUN, VERB, ADJ, ADV: Content words PROPN: Proper nouns (names) DET, ADP, PRON: Function words NUM, PUNCT: Numbers, punctuation Usage: Filter tags focus specific word types (e.g., nouns verbs content analysis). Learn : Universal Dependencies POS Tags Morphological analysis extracts grammatical features words. Uses Python spaCy via reticulate. Features: Usage: Analyze verb tenses temporal patterns, number agreement, grammatical complexity. Learn : spaCy Morphology Named Entity Recognition (NER) identifies classifies named entities text. Requires Python spaCy. Entity Types: PERSON, ORG: People, organizations GPE, LOC: Places, locations DATE, MONEY, PERCENT: Temporal, monetary values Usage: Filter entity type. Add custom entities qualitative coding. Learn : spaCy Named Entity Recognition","code":"plot_word_frequency(dfm_object, top_n = 20) keywords <- extract_keywords_tfidf(dfm_object, top_n = 10) plot_tfidf_keywords(keywords, n_docs = 5) keyness <- extract_keywords_keyness(   dfm_object,   target_group = \"Journal Article\",   reference_groups = \"Conference Paper\",   category_var = \"reference_type\" ) plot_keyness_keywords(keyness) tokens <- detect_multi_words(tokens, min_count = 10) parsed <- spacy_parse_full(texts, include_morphology = TRUE) # Returns columns: morph_Number, morph_Tense, morph_VerbForm, etc."},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/lexical_analysis.html","id":"co-occurrence","dir":"Articles","previous_headings":"Word Networks","what":"Co-occurrence","title":"Lexical Analysis","text":"","code":"semantic_cooccurrence_network(dfm_object, co_occur_n = 10)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/lexical_analysis.html","id":"correlation","dir":"Articles","previous_headings":"Word Networks","what":"Correlation","title":"Lexical Analysis","text":"Readability metrics quantify text complexity using statistical measures sentence structure word characteristics. Available Metrics: Usage Notes: Different formulas may produce slightly different grade level estimates formulas measure surface-level text features (word length, sentence length) Short texts may produce less reliable scores Learn : quanteda textstat_readability Documentation Lexical diversity measures vocabulary richness quantifying relationship unique words (types) total words (tokens). Available Metrics: Usage Notes: MTLD MATTR stable across different text lengths TTR sensitive text length - compare similar-length texts Maas, Yule K, Simpson D use inverse scales (lower = diverse) Learn : quanteda textstat_lexdiv Documentation","code":"semantic_correlation_network(dfm_object, min_cor = 0.3) readability <- calculate_text_readability(united_tbl$united_texts) plot_readability_distribution(readability) diversity <- lexical_diversity_analysis(dfm_object) plot_lexical_diversity_distribution(diversity)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/lexical_analysis.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Lexical Analysis","text":"Semantic Analysis Topic Modeling","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/multimodal_analysis.html","id":"local-ollama","dir":"Articles","previous_headings":"Setup","what":"Local (Ollama)","title":"Multimodal Analysis","text":"","code":"# Install from https://ollama.ai ollama pull llava"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/multimodal_analysis.html","id":"cloud-openai","dir":"Articles","previous_headings":"Setup","what":"Cloud (OpenAI)","title":"Multimodal Analysis","text":"","code":"Sys.setenv(OPENAI_API_KEY = \"sk-...\")"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/multimodal_analysis.html","id":"usage","dir":"Articles","previous_headings":"","what":"Usage","title":"Multimodal Analysis","text":"","code":"library(TextAnalysisR)  # Extract PDF with images result <- extract_pdf_multimodal(   \"document.pdf\",   vision_provider = \"ollama\"  # or \"openai\" )  # Use in analysis tokens <- prep_texts(result$combined_text)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/multimodal_analysis.html","id":"smart-extraction","dir":"Articles","previous_headings":"","what":"Smart Extraction","title":"Multimodal Analysis","text":"Auto-detects document type:","code":"result <- extract_pdf_smart(\"paper.pdf\", doc_type = \"auto\")"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/preprocessing.html","id":"workflow","dir":"Articles","previous_headings":"","what":"Workflow","title":"Preprocessing","text":"Unite combines multiple text columns single column analysis. Useful text content spread across multiple fields analyzed together. Examples: Survey Data: Combine multiple open-ended response columns Multi-field Text: Merge title, abstract, body fields Comments: Concatenate multiple comment note columns Usage: Select one multiple text columns combine. Columns concatenated spaces . united column becomes text source subsequent preprocessing analysis steps. Learn : tidyr Unite Function Tokenization segments continuous text individual units (tokens), typically words, converting unstructured text structured format computational analysis. Options: Lowercase: Convert text lowercase treat “Text” “text” identical Remove Punctuation: Strip punctuation marks like periods, commas, quotes Remove Numbers: Eliminate numeric digits (keep technical texts) Remove Symbols: Remove special characters (@, #, $, etc.) Remove URLs: Identify remove web addresses Usage: Select preprocessing options based analysis goals. Sentence segmentation splits text sentences tokenization sentence structure important (e.g., sentiment analysis). Learn : quanteda Tokens Documentation Stopwords common words (e.g., “”, “”, “”) appear frequently carry little meaningful content analysis. Removing reduces noise improves focus content-bearing words. Remove: Topic Modeling: Helps identify content themes removing function words Keyword Extraction: Ensures meaningful terms rise top Content Analysis: Focuses substantive vocabulary Usage: Use predefined stopword lists (e.g., Snowball) add custom words. sentiment analysis syntactic studies, consider keeping stopwords may carry important meaning. Learn : stopwords Package Documentation Lemmatization reduces words base dictionary form (lemma). example, “running”, “ran”, “runs” become “run”. groups related word forms together meaningful analysis. Comparison: Lemmatization: Uses linguistic knowledge produce valid dictionary words (studies → study) Stemming: Uses simple rules chop word endings (studies → studi) Advantage: Lemmatization produces readable, meaningful base forms Usage: Apply lemmatization tokenization consolidate word variants. Particularly useful topic modeling keyword extraction grouping related forms improves interpretability. Requires Python spaCy. Learn : spaCy Lemmatization Guide Document-Feature Matrix (DFM) mathematical representation rows documents, columns unique tokens (features), cells contain frequency counts. converts unstructured text structured numerical format computational analysis. Process: Tokenization: Text split individual tokens (words) Vocabulary: unique tokens form matrix columns Counting: document-token pair counted Sparse Matrix: Efficient storage format large corpora Usage: DFM foundation downstream analyses including keyword extraction, topic modeling, semantic analysis. Create preprocessing (tokenization, stopword removal, lemmatization). Learn : quanteda DFM Documentation","code":"library(TextAnalysisR)  # 1. Load data mydata <- SpecialEduTech  # 2. Combine text columns united_tbl <- unite_cols(mydata, listed_vars = c(\"title\", \"keyword\", \"abstract\"))  # 3. Tokenize and clean tokens <- prep_texts(   united_tbl,   text_field = \"united_texts\",   remove_punct = TRUE,   remove_numbers = TRUE )  # 4. Remove stopwords tokens_clean <- quanteda::tokens_remove(tokens, quanteda::stopwords(\"en\"))  # 5. Create document-feature matrix dfm_object <- quanteda::dfm(tokens_clean) tokens_clean <- quanteda::tokens_remove(tokens, quanteda::stopwords(\"en\")) dfm_object <- quanteda::dfm(tokens_clean)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/preprocessing.html","id":"multi-word-expressions","dir":"Articles","previous_headings":"","what":"Multi-word Expressions","title":"Preprocessing","text":"Detect phrases like “machine learning”:","code":"tokens <- detect_multi_words(tokens, min_count = 10)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/preprocessing.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Preprocessing","text":"Lexical Analysis Semantic Analysis Topic Modeling","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/articles/python_environment.html","id":"quick-setup","dir":"Articles","previous_headings":"","what":"Quick Setup","title":"Python Environment","text":"Uses conda available, otherwise virtualenv.","code":"library(TextAnalysisR) setup_python_env()"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/python_environment.html","id":"check-status","dir":"Articles","previous_headings":"","what":"Check Status","title":"Python Environment","text":"","code":"check_python_env()"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/python_environment.html","id":"another-python-already-initialized","dir":"Articles","previous_headings":"Common Issues","what":"“Another Python already initialized”","title":"Python Environment","text":"Set preferred environment .Rprofile: restart R.","code":"Sys.setenv(RETICULATE_PYTHON_ENV = \"textanalysisr-env\")"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/python_environment.html","id":"environment-in-onedrive","dir":"Articles","previous_headings":"Common Issues","what":"Environment in OneDrive","title":"Python Environment","text":"Avoid OneDrive paths. Use:","code":"setup_python_env(method = \"virtualenv\", envpath = \"C:/Python/envs\")"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/python_environment.html","id":"deep-learning-optional","dir":"Articles","previous_headings":"","what":"Deep Learning (Optional)","title":"Python Environment","text":"embeddings neural sentiment:","code":"pip install sentence-transformers transformers torch"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/python_environment.html","id":"diagnostics","dir":"Articles","previous_headings":"","what":"Diagnostics","title":"Python Environment","text":"","code":"library(reticulate) py_config() conda_list()"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/quickstart.html","id":"install","dir":"Articles","previous_headings":"","what":"Install","title":"Getting Started","text":"","code":"install.packages(\"remotes\") remotes::install_github(\"mshin77/TextAnalysisR\")"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/quickstart.html","id":"launch-app","dir":"Articles","previous_headings":"","what":"Launch App","title":"Getting Started","text":"visit textanalysisr.org web version.","code":"library(TextAnalysisR) run_app()"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/quickstart.html","id":"quick-example","dir":"Articles","previous_headings":"","what":"Quick Example","title":"Getting Started","text":"","code":"library(TextAnalysisR)  # Load data mydata <- SpecialEduTech  # Combine text columns united_tbl <- unite_cols(mydata, listed_vars = c(\"title\", \"keyword\", \"abstract\"))  # Preprocess tokens <- prep_texts(united_tbl, text_field = \"united_texts\") dfm_object <- quanteda::dfm(tokens)  # Visualize plot_word_frequency(dfm_object, top_n = 20)"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/quickstart.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Getting Started","text":"Installation - Full setup guide Preprocessing - Prepare text data Lexical Analysis - Word patterns Semantic Analysis - Meaning similarity Topic Modeling - Discover themes","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Semantic Analysis","text":"","code":"library(TextAnalysisR)  mydata <- SpecialEduTech united_tbl <- unite_cols(mydata, listed_vars = c(\"title\", \"keyword\", \"abstract\")) tokens <- prep_texts(united_tbl, text_field = \"united_texts\") dfm_object <- quanteda::dfm(tokens)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"document-similarity","dir":"Articles","previous_headings":"","what":"Document Similarity","title":"Semantic Analysis","text":"Semantic analysis measures document similarity using different approaches capture meaning, simple vocabulary matching deep neural representations. Methods: Usage: Choose method based analysis goals. Words n-grams faster interpretable. Embeddings capture deeper meaning require computation. methods use cosine similarity comparison. Learn : Sentence Transformers Documentation","code":"similarity <- semantic_similarity_analysis(   texts = united_tbl$united_texts,   method = \"cosine\" )"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"lexicon-based-no-python","dir":"Articles","previous_headings":"Sentiment Analysis","what":"Lexicon-based (no Python)","title":"Semantic Analysis","text":"","code":"sentiment <- sentiment_lexicon_analysis(dfm_object, lexicon = \"afinn\") plot_sentiment_distribution(sentiment$document_sentiment)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"neural-requires-python","dir":"Articles","previous_headings":"Sentiment Analysis","what":"Neural (requires Python)","title":"Semantic Analysis","text":"","code":"sentiment <- sentiment_embedding_analysis(united_tbl$united_texts)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"document-clustering","dir":"Articles","previous_headings":"","what":"Document Clustering","title":"Semantic Analysis","text":"Clustering groups documents similar semantic content categories. Documents within cluster similar documents clusters. Algorithms: Usage: Choose discovery mode (Automatic, Manual, Advanced). Select semantic feature space algorithm. Automatic mode finds optimal cluster count. Use visualizations quality metrics evaluate results. Learn : scikit-learn Clustering Guide Dimensionality reduction transforms high-dimensional data 2D 3D visualizations preserving structure relationships documents. Algorithms: Usage: Select semantic feature space (words, n-grams, embeddings), choose reduction method. Adjust parameters (perplexity, neighbors, dimensions) based data size structure. Use visual exploration clustering. Learn : scikit-learn Manifold Learning","code":"clusters <- semantic_document_clustering(   texts = united_tbl$united_texts,   n_clusters = 5 )  # AI-generated labels (optional) labels <- generate_cluster_labels(   clusters$cluster_keywords,   provider = \"ollama\"  # or \"openai\" ) reduced <- reduce_dimensions(embeddings, method = \"umap\", n_components = 2) plot_semantic_viz(reduced)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"network-analysis","dir":"Articles","previous_headings":"","what":"Network Analysis","title":"Semantic Analysis","text":"Visualize semantic relationships interactive networks community detection.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"feature-spaces","dir":"Articles","previous_headings":"Network Analysis","what":"Feature Spaces","title":"Semantic Analysis","text":"Networks support three feature spaces based distributional semantics:","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"word-co-occurrence-network","dir":"Articles","previous_headings":"Network Analysis","what":"Word Co-occurrence Network","title":"Semantic Analysis","text":"","code":"network <- semantic_cooccurrence_network(   dfm_object,   co_occur_n = 10,                    # Minimum co-occurrence count   top_node_n = 30,                    # Top nodes to display   node_label_size = 22,               # Font size (12-40)   feature_type = \"words\",             # \"words\", \"ngrams\", or \"embeddings\"   embedding_sim_threshold = 0.5,      # For embeddings: similarity cutoff   community_method = \"leiden\"         # Community detection algorithm )  network$plot   # Interactive visNetwork plot network$table  # Node metrics (degree, eigenvector, community) network$stats  # 9 network statistics"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"word-correlation-network","dir":"Articles","previous_headings":"Network Analysis","what":"Word Correlation Network","title":"Semantic Analysis","text":"","code":"corr_network <- semantic_correlation_network(   dfm_object,   common_term_n = 20,                 # Minimum term frequency   corr_n = 0.4,                       # Minimum correlation threshold   feature_type = \"words\",   community_method = \"leiden\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"category-specific-analysis","dir":"Articles","previous_headings":"Network Analysis","what":"Category-Specific Analysis","title":"Semantic Analysis","text":"Enable per-category networks Shiny app generate separate networks category, displayed tabbed interface. network returns comprehensive statistics: feature_type = \"embeddings\", networks show document--document relationships: Nodes = Documents (words) Edges = Document pairs cosine similarity threshold Edge weight = Similarity score Adjust embedding_sim_threshold (0.3-0.9) control network density: - Higher threshold → fewer, stronger connections - Lower threshold → connections, may noisy Community detection identifies clusters semantically related nodes. Learn : igraph Community Detection","code":"doc_network <- semantic_cooccurrence_network(   dfm_object,   feature_type = \"embeddings\",   embeddings = my_embeddings,         # Pre-computed embedding matrix   embedding_sim_threshold = 0.6,      # Only connect highly similar docs   top_node_n = 50 )"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"temporal-analysis","dir":"Articles","previous_headings":"","what":"Temporal Analysis","title":"Semantic Analysis","text":"Track themes time: Learn : Sentence Transformers Models","code":"temporal <- temporal_semantic_analysis(   texts = united_tbl$united_texts,   timestamps = united_tbl$year )"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/semantic_analysis.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Semantic Analysis","text":"Topic Modeling","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/articles/topic_modeling.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Topic Modeling","text":"","code":"library(TextAnalysisR)  mydata <- SpecialEduTech united_tbl <- unite_cols(mydata, listed_vars = c(\"title\", \"keyword\", \"abstract\")) tokens <- prep_texts(united_tbl, text_field = \"united_texts\") dfm_object <- quanteda::dfm(tokens)"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/topic_modeling.html","id":"find-optimal-topics","dir":"Articles","previous_headings":"","what":"Find Optimal Topics","title":"Topic Modeling","text":"STM discovers latent topics using probabilistic modeling incorporating document metadata covariates. models topics vary across documents based metadata like time, author, category. Best : Metadata analysis: Relate topics document characteristics Covariate effects: Test metadata affects topics Interpretability: Clear word-probability distributions Quality Metrics: Semantic Coherence: Measures word co-occurrence within topics (Mimno et al., 2011) Exclusivity: Measures unique words topic Held-Likelihood: Predictive performance unseen documents Learn : Structural Topic Model | STM Vignette Uses transformer embeddings capture semantic meaning, applies UMAP dimensionality reduction HDBSCAN clustering discover topics. Creates semantically coherent topics based deep contextual understanding. Best : Semantic coherence: Capture meaning beyond word co-occurrence Short texts: Tweets, reviews, survey responses Multilingual: Handles multiple languages Key Components: Sentence Transformers: Generate contextual embeddings (e.g., -MiniLM-L6-v2) UMAP: Dimensionality reduction preserving local structure HDBSCAN: Density-based clustering topic discovery c-TF-IDF: Class-based TF-IDF topic keyword extraction Learn : BERTopic | Sentence-BERT Combines strengths STM embedding-based approaches. Uses transformer embeddings semantic understanding maintaining STM’s ability model covariate relationships provide probabilistic topic assignments. Best : Best worlds: Need semantic coherence covariate modeling Complex research: Testing hypotheses metadata affects semantically-defined topics Validation: Compare validate findings across different methodological approaches Quality Metrics: Semantic Coherence: often top words co-occur documents (higher better) Exclusivity: unique words topic (higher better) Silhouette Score: Cluster separation embedding topics (-1 1, higher better) Alignment Score: Agreement STM embedding topic assignments Adjusted Rand Index: Clustering agreement corrected chance Learn : Structural Topic Model | BERTopic","code":"find_optimal_k(dfm_object, topic_range = 5:30) out <- quanteda::convert(dfm_object, to = \"stm\")  model <- stm::stm(   documents = out$documents,   vocab = out$vocab,   K = 15,   prevalence = ~ reference_type + s(year),   data = out$meta )  terms <- get_topic_terms(model, top_term_n = 10) results <- fit_embedding_topics(   texts = united_tbl$united_texts,   n_topics = 15 ) results <- fit_hybrid_model(   texts = united_tbl$united_texts,   metadata = united_tbl[, c(\"reference_type\", \"year\")],   n_topics_stm = 15 )"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/topic_modeling.html","id":"ai-topic-labels","dir":"Articles","previous_headings":"","what":"AI Topic Labels","title":"Topic Modeling","text":"Use : STM: Academic research metadata, covariate effects, interpretability priority Embedding: Short texts, multilingual, semantic similarity frequency Hybrid: High-stakes research, methodological validation, comprehensive analysis","code":"labels <- generate_topic_labels(   terms,   provider = \"ollama\"  # or \"openai\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/articles/topic_modeling.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next Steps","title":"Topic Modeling","text":"Semantic Analysis Python Environment (embedding-based methods)","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/articles/web_accessibility.html","id":"visual-features","dir":"Articles","previous_headings":"","what":"Visual Features","title":"Accessibility","text":"Dark mode toggle High contrast (4.5:1 ratio) 200% zoom support Reduced motion option","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/articles/web_accessibility.html","id":"screen-reader-support","dir":"Articles","previous_headings":"","what":"Screen Reader Support","title":"Accessibility","text":"Descriptive button labels Chart text descriptions Status announcements","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/articles/web_accessibility.html","id":"language-support","dir":"Articles","previous_headings":"","what":"Language Support","title":"Accessibility","text":"100+ languages via Google Translate Auto-detected text--speech","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Mikyung Shin. Author, maintainer.            Illinois State University","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Shin, M. (2025). TextAnalysisR: Text mining workflow tool (Version 0.0.3) [R package]. https://github.com/mshin77/TextAnalysisR","code":"@Manual{,   title = {TextAnalysisR: Text mining workflow tool},   author = {Mikyung Shin},   year = {2025},   note = {R package version 0.0.3},   url = {https://mshin77.github.io/TextAnalysisR}, }"},{"path":"https://mshin77.github.io/TextAnalysisR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Text Mining Workflow Tool","text":"development version GitHub :","code":"install.packages(\"devtools\") devtools::install_github(\"mshin77/TextAnalysisR\")"},{"path":"https://mshin77.github.io/TextAnalysisR/index.html","id":"load-the-textanalysisr-package","dir":"","previous_headings":"","what":"Load the TextAnalysisR Package","title":"Text Mining Workflow Tool","text":"","code":"library(TextAnalysisR)"},{"path":"https://mshin77.github.io/TextAnalysisR/index.html","id":"alternatively-launch-and-browse-the-shiny-app","dir":"","previous_headings":"","what":"Alternatively, Launch and Browse the Shiny App","title":"Text Mining Workflow Tool","text":"Access web app https://www.textanalysisr.org. Launch browse app local computer:","code":"run_app()"},{"path":"https://mshin77.github.io/TextAnalysisR/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting Started","title":"Text Mining Workflow Tool","text":"See Quick Start tutorials.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Text Mining Workflow Tool","text":"Shin, M. (2025). TextAnalysisR: text mining workflow tool (R package version 0.0.3) [Computer software]. https://mshin77.github.io/TextAnalysisR Shin, M. (2025). TextAnalysisR: text mining workflow tool [Web application]. https://www.textanalysisr.org","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/acronym.html","id":null,"dir":"Reference","previous_headings":"","what":"Acronym List — acronym","title":"Acronym List — acronym","text":"dataset containing common acronyms used text processing","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/acronym.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Acronym List — acronym","text":"character vector acronyms","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/ai_content_generation.html","id":null,"dir":"Reference","previous_headings":"","what":"AI-Assisted Content Generation from Topics — ai_content_generation","title":"AI-Assisted Content Generation from Topics — ai_content_generation","text":"Functions generating various types content topic model terms using Large Language Models (LLMs). Supports OpenAI Ollama backends.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_contrastive_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze Contrastive Similarity (Alias) — analyze_contrastive_similarity","title":"Analyze Contrastive Similarity (Alias) — analyze_contrastive_similarity","text":"Alias analyze_similarity_gaps. Identifies unique items, missing content, cross-category opportunities based similarity thresholds.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_contrastive_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze Contrastive Similarity (Alias) — analyze_contrastive_similarity","text":"","code":"analyze_contrastive_similarity(   similarity_data,   ref_var = \"ref_id\",   other_var = \"other_id\",   similarity_var = \"similarity\",   category_var = \"other_category\",   ref_label_var = NULL,   other_label_var = NULL,   unique_threshold = 0.6,   cross_policy_min = 0.6,   cross_policy_max = 0.8 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_contrastive_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze Contrastive Similarity (Alias) — analyze_contrastive_similarity","text":"similarity_data data frame cross-category similarities, containing: ref_var Reference item identifier other_var Comparison item identifier similarity_var Similarity score category_var Category comparison item ref_var Name column reference item IDs (default: \"ref_id\"). other_var Name column comparison item IDs (default: \"other_id\"). similarity_var Name column similarity values (default: \"similarity\"). category_var Name column category information (default: \"other_category\"). ref_label_var Optional column reference item labels (output). other_label_var Optional column comparison item labels (output). unique_threshold Threshold reference items considered unique (default: 0.6). cross_policy_min Minimum similarity cross-policy opportunities (default: 0.6). cross_policy_max Maximum similarity cross-policy opportunities (default: 0.8).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_contrastive_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze Contrastive Similarity (Alias) — analyze_contrastive_similarity","text":"list containing unique_items, missing_items, cross_policy, summary_stats.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_document_clustering.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze Document Clustering — analyze_document_clustering","title":"Analyze Document Clustering — analyze_document_clustering","text":"Complete document clustering analysis dimensionality reduction optional clustering","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_document_clustering.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze Document Clustering — analyze_document_clustering","text":"","code":"analyze_document_clustering(   feature_matrix,   method = \"UMAP\",   clustering_method = \"none\",   ... )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_document_clustering.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze Document Clustering — analyze_document_clustering","text":"feature_matrix Feature matrix (documents x features) method Dimensionality reduction method (\"PCA\", \"t-SNE\", \"UMAP\") clustering_method Clustering method (\"none\", \"kmeans\", \"hierarchical\", \"dbscan\", \"hdbscan\") ... Additional parameters methods","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_document_clustering.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze Document Clustering — analyze_document_clustering","text":"List containing coordinates, clusters, method info, quality metrics","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_semantic_evolution.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze Semantic Evolution — analyze_semantic_evolution","title":"Analyze Semantic Evolution — analyze_semantic_evolution","text":"Analyzes semantic evolution patterns temporal results","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_semantic_evolution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze Semantic Evolution — analyze_semantic_evolution","text":"","code":"analyze_semantic_evolution(temporal_results, verbose = FALSE, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_semantic_evolution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze Semantic Evolution — analyze_semantic_evolution","text":"temporal_results Temporal analysis results verbose Logical indicating whether print progress messages ... Additional parameters","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_semantic_evolution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze Semantic Evolution — analyze_semantic_evolution","text":"List containing evolution analysis","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_sentiment.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze Text Sentiment — analyze_sentiment","title":"Analyze Text Sentiment — analyze_sentiment","text":"Performs sentiment analysis text data using syuzhet package. Returns sentiment scores classifications.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_sentiment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze Text Sentiment — analyze_sentiment","text":"","code":"analyze_sentiment(texts, method = \"syuzhet\", doc_ids = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_sentiment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze Text Sentiment — analyze_sentiment","text":"texts Character vector texts analyze method Sentiment analysis method: \"syuzhet\", \"bing\", \"afinn\", \"nrc\" (default: \"syuzhet\") doc_ids Optional character vector document identifiers (default: NULL)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_sentiment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze Text Sentiment — analyze_sentiment","text":"data frame columns: document Document identifier text Original text sentiment_score Numeric sentiment score sentiment Classification: \"positive\", \"negative\", \"neutral\"","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_sentiment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analyze Text Sentiment — analyze_sentiment","text":"","code":"if (FALSE) { # \\dontrun{ texts <- c(   \"This research shows promising results for students.\",   \"The intervention had no significant effect.\",   \"Students struggled with the complex material.\" ) results <- analyze_sentiment(texts) print(results) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_similarity_gaps.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze Similarity Gaps Between Categories — analyze_similarity_gaps","title":"Analyze Similarity Gaps Between Categories — analyze_similarity_gaps","text":"Identifies unique items, missing content, cross-category learning opportunities based similarity thresholds. Useful gap analysis policy documents, topic comparisons, cross-category similarity study.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_similarity_gaps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze Similarity Gaps Between Categories — analyze_similarity_gaps","text":"","code":"analyze_similarity_gaps(   similarity_data,   ref_var = \"ref_id\",   other_var = \"other_id\",   similarity_var = \"similarity\",   category_var = \"other_category\",   ref_label_var = NULL,   other_label_var = NULL,   unique_threshold = 0.6,   cross_policy_min = 0.6,   cross_policy_max = 0.8 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_similarity_gaps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze Similarity Gaps Between Categories — analyze_similarity_gaps","text":"similarity_data data frame cross-category similarities, containing: ref_var Reference item identifier other_var Comparison item identifier similarity_var Similarity score category_var Category comparison item ref_var Name column reference item IDs (default: \"ref_id\"). other_var Name column comparison item IDs (default: \"other_id\"). similarity_var Name column similarity values (default: \"similarity\"). category_var Name column category information (default: \"other_category\"). ref_label_var Optional column reference item labels (output). other_label_var Optional column comparison item labels (output). unique_threshold Threshold reference items considered unique (default: 0.6). cross_policy_min Minimum similarity cross-policy opportunities (default: 0.6). cross_policy_max Maximum similarity cross-policy opportunities (default: 0.8).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_similarity_gaps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze Similarity Gaps Between Categories — analyze_similarity_gaps","text":"list containing: unique_items Data frame reference items low similarity (unique content) missing_items Data frame comparison items low similarity (content gaps) cross_policy Data frame items moderate similarity (learning opportunities) summary_stats Summary statistics category","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_similarity_gaps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analyze Similarity Gaps Between Categories — analyze_similarity_gaps","text":"","code":"if (FALSE) { # \\dontrun{ # After extracting cross-category similarities gap_analysis <- analyze_similarity_gaps(   similarity_data = cross_sims,   ref_var = \"ref_id\",   other_var = \"other_id\",   similarity_var = \"similarity\",   category_var = \"other_category\",   unique_threshold = 0.6 )  print(gap_analysis$unique_items) print(gap_analysis$missing_items) print(gap_analysis$summary_stats) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_topic_evolution.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze Semantic Evolution — analyze_topic_evolution","title":"Analyze Semantic Evolution — analyze_topic_evolution","text":"Analyzes semantic patterns evolve time.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_topic_evolution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze Semantic Evolution — analyze_topic_evolution","text":"","code":"analyze_topic_evolution(temporal_results, verbose = TRUE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_topic_evolution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze Semantic Evolution — analyze_topic_evolution","text":"temporal_results Results temporal analysis. verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/analyze_topic_evolution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze Semantic Evolution — analyze_topic_evolution","text":"Evolution patterns.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/apply_standard_plotly_layout.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply Standard Plotly Layout — apply_standard_plotly_layout","title":"Apply Standard Plotly Layout — apply_standard_plotly_layout","text":"Applies consistent layout styling plotly plots following TextAnalysisR design standards. ensures plots uniform fonts, colors, margins, interactive features.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/apply_standard_plotly_layout.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply Standard Plotly Layout — apply_standard_plotly_layout","text":"","code":"apply_standard_plotly_layout(   plot,   title = NULL,   xaxis_title = NULL,   yaxis_title = NULL,   margin = list(t = 60, b = 80, l = 80, r = 40),   show_legend = FALSE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/apply_standard_plotly_layout.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply Standard Plotly Layout — apply_standard_plotly_layout","text":"plot plotly plot object title Plot title text (optional) xaxis_title X-axis title (optional) yaxis_title Y-axis title (optional) margin List margins: list(t, b, l, r) pixels (default: list(t = 60, b = 80, l = 80, r = 40)) show_legend Logical, whether show legend (default: FALSE)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/apply_standard_plotly_layout.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply Standard Plotly Layout — apply_standard_plotly_layout","text":"plotly plot object standardized layout","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/apply_standard_plotly_layout.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Apply Standard Plotly Layout — apply_standard_plotly_layout","text":"Design standards applied: Title: 18px Roboto, #0c1f4a Axis titles: 16px Roboto, #0c1f4a Axis tick labels: 16px Roboto, #3B3B3B Hover tooltips: 16px Roboto WCAG AA compliant colors","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/apply_standard_plotly_layout.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply Standard Plotly Layout — apply_standard_plotly_layout","text":"","code":"if (FALSE) { # \\dontrun{ library(plotly) p <- plot_ly(x = 1:10, y = rnorm(10), type = \"scatter\", mode = \"markers\") p %>% apply_standard_plotly_layout(   title = \"My Plot\",   xaxis_title = \"X Values\",   yaxis_title = \"Y Values\" ) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/assess_hybrid_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Assess Hybrid Model Stability via Bootstrap — assess_hybrid_stability","title":"Assess Hybrid Model Stability via Bootstrap — assess_hybrid_stability","text":"Evaluates stability hybrid topic model running bootstrap resampling. helps identify topics robust may artifacts specific sample. Based research recommendations topic model validation.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/assess_hybrid_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assess Hybrid Model Stability via Bootstrap — assess_hybrid_stability","text":"","code":"assess_hybrid_stability(   texts,   n_topics = 10,   n_bootstrap = 5,   sample_proportion = 0.8,   embedding_model = \"all-MiniLM-L6-v2\",   seed = 123,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/assess_hybrid_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assess Hybrid Model Stability via Bootstrap — assess_hybrid_stability","text":"texts character vector texts analyze. n_topics Number topics (default: 10). n_bootstrap Number bootstrap iterations (default: 5). sample_proportion Proportion documents sample (default: 0.8). embedding_model Embedding model name (default: \"-MiniLM-L6-v2\"). seed Random seed reproducibility. verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/assess_hybrid_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assess Hybrid Model Stability via Bootstrap — assess_hybrid_stability","text":"list containing stability metrics: topic_stability: Per-topic stability scores (0-1) mean_stability: Overall stability score keyword_stability: Stability top keywords per topic alignment_stability: Stability STM-embedding alignment bootstrap_results: Detailed results bootstrap run","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/assess_hybrid_stability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assess Hybrid Model Stability via Bootstrap — assess_hybrid_stability","text":"","code":"if (FALSE) { # \\dontrun{   stability <- assess_hybrid_stability(     texts = my_texts,     n_topics = 10,     n_bootstrap = 5,     verbose = TRUE   )    # View topic stability scores   stability$topic_stability } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_assignment_consistency.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Assignment Consistency — calculate_assignment_consistency","title":"Calculate Assignment Consistency — calculate_assignment_consistency","text":"Calculates consistency two sets assignments","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_assignment_consistency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Assignment Consistency — calculate_assignment_consistency","text":"","code":"calculate_assignment_consistency(assignments1, assignments2, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_assignment_consistency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Assignment Consistency — calculate_assignment_consistency","text":"assignments1 First set assignments assignments2 Second set assignments ... Additional parameters","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_assignment_consistency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Assignment Consistency — calculate_assignment_consistency","text":"List containing consistency metrics","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_clustering_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Clustering Quality Metrics — calculate_clustering_metrics","title":"Calculate Clustering Quality Metrics — calculate_clustering_metrics","text":"Calculates common clustering evaluation metrics including Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Index.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_clustering_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Clustering Quality Metrics — calculate_clustering_metrics","text":"","code":"calculate_clustering_metrics(   clusters,   data_matrix,   dist_matrix = NULL,   metrics = \"all\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_clustering_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Clustering Quality Metrics — calculate_clustering_metrics","text":"clusters Integer vector cluster assignments data_matrix Numeric matrix data points (rows = observations, cols = features) dist_matrix Optional distance matrix. NULL, computed data_matrix metrics Character vector metrics calculate. Options: \"silhouette\", \"davies_bouldin\", \"calinski_harabasz\", \"\" (default)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_clustering_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Clustering Quality Metrics — calculate_clustering_metrics","text":"named list containing: silhouette Silhouette score (-1 1, higher better) davies_bouldin Davies-Bouldin index (lower better) calinski_harabasz Calinski-Harabasz index (higher better) n_clusters Number clusters cluster_sizes Table cluster sizes","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_clustering_metrics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate Clustering Quality Metrics — calculate_clustering_metrics","text":"Silhouette Score: Measures similar object cluster compared clusters. Range: -1 1, higher better. Davies-Bouldin Index: Average similarity cluster similar cluster. Lower values indicate better clustering. Calinski-Harabasz Index: Ratio -cluster within-cluster variance. Higher values indicate better-defined clusters.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_clustering_metrics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Clustering Quality Metrics — calculate_clustering_metrics","text":"","code":"if (FALSE) { # \\dontrun{ # Generate sample data set.seed(123) data <- rbind(   matrix(rnorm(100, mean = 0), ncol = 2),   matrix(rnorm(100, mean = 3), ncol = 2) ) clusters <- c(rep(1, 50), rep(2, 50))  metrics <- calculate_clustering_metrics(clusters, data) print(metrics) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_coherence.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Semantic Coherence — calculate_coherence","title":"Validate Semantic Coherence — calculate_coherence","text":"Validates semantic coherence topic assignments.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_coherence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Semantic Coherence — calculate_coherence","text":"","code":"calculate_coherence(embeddings, topic_assignments)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_coherence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Semantic Coherence — calculate_coherence","text":"embeddings Document embeddings. topic_assignments Topic assignments.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_coherence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Semantic Coherence — calculate_coherence","text":"Coherence metrics.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_consistency.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Assignment Consistency — calculate_consistency","title":"Calculate Assignment Consistency — calculate_consistency","text":"Calculates consistency different assignment methods.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_consistency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Assignment Consistency — calculate_consistency","text":"","code":"calculate_consistency(semantic_assignments, stm_assignments)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_consistency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Assignment Consistency — calculate_consistency","text":"semantic_assignments Semantic topic assignments. stm_assignments STM topic assignments.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_consistency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Assignment Consistency — calculate_consistency","text":"Consistency metrics.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_contrast_ratio.html","id":null,"dir":"Reference","previous_headings":"","what":"Web Accessibility Utility Functions — calculate_contrast_ratio","title":"Web Accessibility Utility Functions — calculate_contrast_ratio","text":"Functions ensuring WCAG 2.1 Level AA compliance Shiny application Calculates contrast ratio two colors according WCAG 2.1 standards using relative luminance formula W3C guidelines. Used verify text/background color combinations meet accessibility requirements.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_contrast_ratio.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Web Accessibility Utility Functions — calculate_contrast_ratio","text":"","code":"calculate_contrast_ratio(foreground, background)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_contrast_ratio.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Web Accessibility Utility Functions — calculate_contrast_ratio","text":"foreground Foreground color (hex format, e.g., \"#111827\") background Background color (hex format, e.g., \"#ffffff\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_contrast_ratio.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Web Accessibility Utility Functions — calculate_contrast_ratio","text":"Numeric contrast ratio (1-21)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_contrast_ratio.html","id":"wcag-level-aa-compliance","dir":"Reference","previous_headings":"","what":"WCAG 2.1 Level AA Compliance","title":"Web Accessibility Utility Functions — calculate_contrast_ratio","text":"package follows Web Content Accessibility Guidelines (WCAG) 2.1 Level AA: 1.1.1 Non-text Content (Level ): Alt text images visualizations 1.4.3 Contrast Minimum (Level AA): 4.5:1 ratio normal text, 3:1 large text/UI 2.1.1 Keyboard (Level ): Full keyboard navigation support 2.4.1 Bypass Blocks (Level ): Skip navigation links 3.1.1 Language Page (Level ): Page language identification 4.1.2 Name, Role, Value (Level ): ARIA labels roles Calculate Color Contrast Ratio","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_contrast_ratio.html","id":"wcag-requirements","dir":"Reference","previous_headings":"","what":"WCAG Requirements","title":"Web Accessibility Utility Functions — calculate_contrast_ratio","text":"Normal text: Minimum 4.5:1 (Level AA) Large text (18pt+ 14pt+ bold): Minimum 3:1 (Level AA) UI components graphics: Minimum 3:1 (Level AA)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_contrast_ratio.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Web Accessibility Utility Functions — calculate_contrast_ratio","text":"","code":"if (FALSE) { # \\dontrun{ calculate_contrast_ratio(\"#111827\", \"#ffffff\")  # Returns ~16:1 (Pass) calculate_contrast_ratio(\"#6b7280\", \"#4a5568\")  # Returns ~2.8:1 (Fail) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_cosine_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Cosine Similarity Matrix — calculate_cosine_similarity","title":"Calculate Cosine Similarity Matrix — calculate_cosine_similarity","text":"Calculates cosine similarity pairs rows matrix.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_cosine_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Cosine Similarity Matrix — calculate_cosine_similarity","text":"","code":"calculate_cosine_similarity(matrix_data)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_cosine_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Cosine Similarity Matrix — calculate_cosine_similarity","text":"matrix_data numeric matrix rows represent documents/observations","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_cosine_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Cosine Similarity Matrix — calculate_cosine_similarity","text":"square similarity matrix values -1 1","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_cross_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Cross-Matrix Cosine Similarity — calculate_cross_similarity","title":"Calculate Cross-Matrix Cosine Similarity — calculate_cross_similarity","text":"Calculates cosine similarity two different embedding matrices, useful comparing documents/topics across different categories groups.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_cross_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Cross-Matrix Cosine Similarity — calculate_cross_similarity","text":"","code":"calculate_cross_similarity(   embeddings1,   embeddings2,   labels1 = NULL,   labels2 = NULL,   normalize = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_cross_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Cross-Matrix Cosine Similarity — calculate_cross_similarity","text":"embeddings1 numeric matrix rows items columns embedding dimensions. embeddings2 numeric matrix rows items columns embedding dimensions. Must number columns embeddings1. labels1 Optional character vector labels items embeddings1. labels2 Optional character vector labels items embeddings2. normalize Logical, whether L2-normalize embeddings computing similarity (default: TRUE).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_cross_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Cross-Matrix Cosine Similarity — calculate_cross_similarity","text":"list containing: similarity_matrix Matrix cosine similarities (nrow(embeddings1) x nrow(embeddings2)) similarity_df Long-format data frame columns: row_idx, col_idx, similarity, optionally label1, label2","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_cross_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Cross-Matrix Cosine Similarity — calculate_cross_similarity","text":"","code":"if (FALSE) { # \\dontrun{ # Generate embeddings for two groups emb1 <- TextAnalysisR::generate_embeddings(c(\"text a\", \"text b\"), verbose = FALSE) emb2 <- TextAnalysisR::generate_embeddings(c(\"text c\", \"text d\", \"text e\"), verbose = FALSE)  # Calculate cross-similarity result <- calculate_cross_similarity(   emb1, emb2,   labels1 = c(\"A\", \"B\"),   labels2 = c(\"C\", \"D\", \"E\") ) print(result$similarity_matrix) print(result$similarity_df) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_document_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Document Similarity — calculate_document_similarity","title":"Calculate Document Similarity — calculate_document_similarity","text":"Calculates similarity documents using traditional NLP methods modern embedding-based approaches. Comprehensive metrics automatically computed unless disabled.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_document_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Document Similarity — calculate_document_similarity","text":"","code":"calculate_document_similarity(   texts,   document_feature_type = \"words\",   semantic_ngram_range = 2,   similarity_method = \"cosine\",   use_embeddings = FALSE,   embedding_model = \"all-MiniLM-L6-v2\",   calculate_metrics = TRUE,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_document_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Document Similarity — calculate_document_similarity","text":"texts character vector texts compare. document_feature_type Feature extraction type: \"words\", \"ngrams\", \"embeddings\", \"topics\". semantic_ngram_range Integer, n-gram range ngram features (default: 2). similarity_method Similarity calculation method: \"cosine\", \"jaccard\", \"euclidean\", \"manhattan\". use_embeddings Logical, use embedding-based similarity (default: FALSE). embedding_model Sentence transformer model name (default: \"-MiniLM-L6-v2\"). calculate_metrics Logical, compute comprehensive similarity metrics (default: TRUE). verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_document_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Document Similarity — calculate_document_similarity","text":"list containing: similarity_matrix N x N similarity matrix feature_matrix Document feature matrix used calculation method_info Information method used metrics Comprehensive similarity metrics (calculate_metrics = TRUE) execution_time Time taken analysis","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_document_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Document Similarity — calculate_document_similarity","text":"","code":"if (interactive()) {   texts <- c(     \"Assistive technology supports learning for students with disabilities.\",     \"Technology aids help disabled students with their education.\",     \"Machine learning algorithms improve predictive accuracy.\"   )    result <- calculate_document_similarity(     texts = texts,     document_feature_type = \"words\",     similarity_method = \"cosine\"   )    print(result$similarity_matrix)   print(result$metrics) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_eval_metrics_internal.html","id":null,"dir":"Reference","previous_headings":"","what":"Comprehensive Evaluation Metrics Calculator — calculate_eval_metrics_internal","title":"Comprehensive Evaluation Metrics Calculator — calculate_eval_metrics_internal","text":"Calculates comprehensive evaluation metrics topic models including neural coherence, LLM-based coherence, semantic diversity, topic stability measures.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_eval_metrics_internal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Comprehensive Evaluation Metrics Calculator — calculate_eval_metrics_internal","text":"","code":"calculate_eval_metrics_internal(result, texts, selected_metrics)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_eval_metrics_internal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Comprehensive Evaluation Metrics Calculator — calculate_eval_metrics_internal","text":"result Topic modeling result object texts Original text documents selected_metrics Vector metrics calculate","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_eval_metrics_internal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Comprehensive Evaluation Metrics Calculator — calculate_eval_metrics_internal","text":"List containing calculated evaluation metrics","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_keyword_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Keyword Stability — calculate_keyword_stability","title":"Calculate Keyword Stability — calculate_keyword_stability","text":"Calculates stability two sets topic keywords.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_keyword_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Keyword Stability — calculate_keyword_stability","text":"","code":"calculate_keyword_stability(keywords1, keywords2)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_keyword_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Keyword Stability — calculate_keyword_stability","text":"keywords1 First set keywords. keywords2 Second set keywords.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_keyword_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Keyword Stability — calculate_keyword_stability","text":"Stability score.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Comprehensive Metrics — calculate_metrics","title":"Calculate Comprehensive Metrics — calculate_metrics","text":"Calculates comprehensive similarity metrics including statistical measures network properties. Internal function used document_similarity_analysis.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Comprehensive Metrics — calculate_metrics","text":"","code":"calculate_metrics(similarity_matrix, labels = NULL, method_info = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Comprehensive Metrics — calculate_metrics","text":"similarity_matrix similarity matrix. labels Optional vector labels clustering metrics. method_info Optional method information.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Comprehensive Metrics — calculate_metrics","text":"list comprehensive metrics.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_semantic_drift.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Semantic Drift — calculate_semantic_drift","title":"Calculate Semantic Drift — calculate_semantic_drift","text":"Calculates semantic drift across time periods","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_semantic_drift.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Semantic Drift — calculate_semantic_drift","text":"","code":"calculate_semantic_drift(temporal_results, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_semantic_drift.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Semantic Drift — calculate_semantic_drift","text":"temporal_results Temporal analysis results ... Additional parameters","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_semantic_drift.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Semantic Drift — calculate_semantic_drift","text":"List containing drift metrics","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_similarity_robust.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Similarity Robust — calculate_similarity_robust","title":"Calculate Similarity Robust — calculate_similarity_robust","text":"Calculates document similarity fallback methods diagnostics. Attempts embeddings first, falls back Jaccard similarity needed.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_similarity_robust.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Similarity Robust — calculate_similarity_robust","text":"","code":"calculate_similarity_robust(   texts,   method = \"embeddings\",   embedding_model = \"all-MiniLM-L6-v2\",   cache_embeddings = TRUE,   min_word_length = 3,   doc_names = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_similarity_robust.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Similarity Robust — calculate_similarity_robust","text":"texts Character vector texts method Similarity method (\"embeddings\" \"jaccard\") embedding_model Model name embeddings (default: \"-MiniLM-L6-v2\") cache_embeddings Logical, cache embeddings (default: TRUE) min_word_length Minimum word length Jaccard (default: 3) doc_names Optional document names","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_similarity_robust.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Similarity Robust — calculate_similarity_robust","text":"List containing similarity matrix, method used, embeddings, diagnostics","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_similarity_robust.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Similarity Robust — calculate_similarity_robust","text":"","code":"if (FALSE) { # \\dontrun{ texts <- c(   \"Assistive technology supports learning.\",   \"Technology helps students with disabilities.\",   \"Machine learning improves accuracy.\" )  result <- calculate_similarity_robust(texts) print(result$similarity_matrix) print(result$diagnostics) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_text_readability.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Text Readability — calculate_text_readability","title":"Calculate Text Readability — calculate_text_readability","text":"Calculates multiple readability metrics texts including Flesch Reading Ease, Flesch-Kincaid Grade Level, Gunning FOG index, others. Optionally includes lexical diversity metrics sentence statistics.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_text_readability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Text Readability — calculate_text_readability","text":"","code":"calculate_text_readability(   texts,   metrics = c(\"flesch\", \"flesch_kincaid\", \"gunning_fog\"),   include_lexical_diversity = TRUE,   include_sentence_stats = TRUE,   dfm_for_lexdiv = NULL,   doc_names = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_text_readability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Text Readability — calculate_text_readability","text":"texts Character vector texts analyze metrics Character vector readability metrics calculate. Options: \"flesch\", \"flesch_kincaid\", \"gunning_fog\", \"smog\", \"ari\", \"coleman_liau\" include_lexical_diversity Logical, include TTR MTLD (default: TRUE) include_sentence_stats Logical, include average sentence length (default: TRUE) dfm_for_lexdiv Optional pre-computed DFM lexical diversity calculation doc_names Optional character vector document names","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_text_readability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Text Readability — calculate_text_readability","text":"data frame document names readability scores","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_text_readability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Text Readability — calculate_text_readability","text":"","code":"if (FALSE) { # \\dontrun{ texts <- c(   \"This is simple text.\",   \"This sentence contains more complex vocabulary and structure.\" ) readability <- calculate_text_readability(texts) print(readability) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_probability.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Topic Probabilities — calculate_topic_probability","title":"Calculate Topic Probabilities — calculate_topic_probability","text":"Extracts summarizes topic probabilities (gamma values) STM model, returning formatted data table mean topic prevalence.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_probability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Topic Probabilities — calculate_topic_probability","text":"","code":"calculate_topic_probability(stm_model, top_n = 10, verbose = TRUE, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_probability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Topic Probabilities — calculate_topic_probability","text":"stm_model fitted STM model object stm::stm(). top_n Number top topics display prevalence (default: 10). verbose Logical, TRUE prints progress messages (default: TRUE). ... Additional arguments passed tidytext::tidy().","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_probability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Topic Probabilities — calculate_topic_probability","text":"DT::datatable showing topics mean gamma (prevalence) values, rounded 3 decimal places.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_probability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Topic Probabilities — calculate_topic_probability","text":"","code":"if (interactive()) {   data <- TextAnalysisR::SpecialEduTech   united <- unite_cols(data, c(\"title\", \"keyword\", \"abstract\"))   tokens <- prep_texts(united, text_field = \"united_texts\")   dfm_obj <- quanteda::dfm(tokens)   stm_data <- quanteda::convert(dfm_obj, to = \"stm\")    topic_model <- stm::stm(     documents = stm_data$documents,     vocab = stm_data$vocab,     K = 10,     verbose = FALSE   )    prob_table <- calculate_topic_probability(topic_model, top_n = 10)   print(prob_table) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_quality.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Semantic Topic Quality Metrics — calculate_topic_quality","title":"Calculate Semantic Topic Quality Metrics — calculate_topic_quality","text":"Internal function calculate quality metrics semantic topic modeling results.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_quality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Semantic Topic Quality Metrics — calculate_topic_quality","text":"","code":"calculate_topic_quality(   embeddings,   topic_assignments,   similarity_matrix = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_quality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Semantic Topic Quality Metrics — calculate_topic_quality","text":"embeddings Document embeddings matrix. topic_assignments Vector topic assignments. similarity_matrix Optional similarity matrix.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_quality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Semantic Topic Quality Metrics — calculate_topic_quality","text":"list quality metrics.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_stability.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Topic Stability — calculate_topic_stability","title":"Calculate Topic Stability — calculate_topic_stability","text":"Calculates stability topics across time periods.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_stability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Topic Stability — calculate_topic_stability","text":"","code":"calculate_topic_stability(temporal_results)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_stability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Topic Stability — calculate_topic_stability","text":"temporal_results Results temporal analysis.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_topic_stability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Topic Stability — calculate_topic_stability","text":"Stability metrics.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_word_frequency.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze and Visualize Word Frequencies Across a Continuous Variable — calculate_word_frequency","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — calculate_word_frequency","text":"function analyzes visualizes word frequencies across continuous variable.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_word_frequency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — calculate_word_frequency","text":"","code":"calculate_word_frequency(   dfm_object,   continuous_variable,   selected_terms,   height = 500,   width = 900 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_word_frequency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — calculate_word_frequency","text":"dfm_object quanteda document-feature matrix (dfm). continuous_variable continuous variable metadata. selected_terms vector terms analyze trends . height height resulting Plotly plot, pixels (default: 500). width width resulting Plotly plot, pixels (default: 900).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_word_frequency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — calculate_word_frequency","text":"list containing Plotly objects tables results.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_word_frequency.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — calculate_word_frequency","text":"function requires fitted STM model object quanteda dfm object. continuous variable column metadata dfm object. selected terms vector terms analyze trends . required packages 'htmltools', 'splines', 'broom' (plus additional ones loaded internally).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/calculate_word_frequency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analyze and Visualize Word Frequencies Across a Continuous Variable — calculate_word_frequency","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_cols(     mydata,     listed_vars = c(\"title\", \"keyword\", \"abstract\")   )    tokens <- TextAnalysisR::prep_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    word_freq_results <- TextAnalysisR::calculate_word_frequency(     dfm_object,     continuous_variable = \"year\",     selected_terms = c(\"calculator\", \"computer\"),     height = 500,     width = 900   )   print(word_freq_results$plot)   print(word_freq_results$table) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/call_ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Call Ollama for Text Generation — call_ollama","title":"Call Ollama for Text Generation — call_ollama","text":"Sends prompt Ollama returns generated text.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/call_ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call Ollama for Text Generation — call_ollama","text":"","code":"call_ollama(   prompt,   model = \"phi3:mini\",   temperature = 0.3,   max_tokens = 512,   timeout = 60,   verbose = FALSE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/call_ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call Ollama for Text Generation — call_ollama","text":"prompt Character string containing prompt. model Character string specifying Ollama model (default: \"phi3:mini\"). temperature Numeric value controlling randomness (default: 0.3). max_tokens Maximum number tokens generate (default: 512). timeout Timeout seconds request (default: 60). verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/call_ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call Ollama for Text Generation — call_ollama","text":"Character string generated text, NULL failed.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/call_ollama.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Call Ollama for Text Generation — call_ollama","text":"","code":"if (FALSE) { # \\dontrun{ response <- call_ollama(   prompt = \"Summarize these keywords: machine learning, neural networks, AI\",   model = \"phi3:mini\" ) print(response) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/call_openai_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Call OpenAI Chat Completion API — call_openai_chat","title":"Call OpenAI Chat Completion API — call_openai_chat","text":"Internal function call OpenAI's chat completion API.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/call_openai_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call OpenAI Chat Completion API — call_openai_chat","text":"","code":"call_openai_chat(   system_prompt,   user_prompt,   model = \"gpt-3.5-turbo\",   temperature = 0,   max_tokens = 150,   api_key )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/call_openai_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call OpenAI Chat Completion API — call_openai_chat","text":"system_prompt System message chat. user_prompt User message/query. model Model use (default: \"gpt-3.5-turbo\"). temperature Sampling temperature (default: 0). max_tokens Maximum tokens response (default: 150). api_key OpenAI API key.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/call_openai_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call OpenAI Chat Completion API — call_openai_chat","text":"Character string model's response.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_alt_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Alt Text Presence — check_alt_text","title":"Check Alt Text Presence — check_alt_text","text":"Validates images visualizations alternative text descriptions. Required WCAG 1.1.1 (Non-text Content). Note: Decorative images use empty alt text (alt=\"\") indicate ignored assistive technology.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_alt_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Alt Text Presence — check_alt_text","text":"","code":"check_alt_text(alt_text, element_type = \"image\", decorative = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_alt_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Alt Text Presence — check_alt_text","text":"alt_text Alternative text description element_type Type element (e.g., \"plot\", \"image\", \"icon\") decorative Logical, TRUE element purely decorative","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_alt_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Alt Text Presence — check_alt_text","text":"Logical TRUE valid, FALSE warning missing/inadequate","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_alt_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Alt Text Presence — check_alt_text","text":"","code":"if (FALSE) { # \\dontrun{ check_alt_text(\"Bar chart showing word frequency\", \"plot\")  # TRUE check_alt_text(\"\", \"plot\")  # FALSE (informative content needs alt text) check_alt_text(\"\", \"icon\", decorative = TRUE)  # TRUE (decorative is OK) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_feature.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Feature Status — check_feature","title":"Check Feature Status — check_feature","text":"Checks specific optional feature available current environment.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_feature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Feature Status — check_feature","text":"","code":"check_feature(feature)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_feature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Feature Status — check_feature","text":"feature Character: \"python\", \"ollama\", \"langgraph\", \"pdf_tables\", \"embeddings\"","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_feature.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Feature Status — check_feature","text":"Logical TRUE feature available","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_feature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Feature Status — check_feature","text":"","code":"if (check_feature(\"ollama\")) {   # Use AI-powered labeling } #> NULL"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_multimodal_prerequisites.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Multimodal Prerequisites — check_multimodal_prerequisites","title":"Check Multimodal Prerequisites — check_multimodal_prerequisites","text":"Checks prerequisites multimodal PDF extraction returns detailed status setup instructions.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_multimodal_prerequisites.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Multimodal Prerequisites — check_multimodal_prerequisites","text":"","code":"check_multimodal_prerequisites(   vision_provider = \"ollama\",   vision_model = NULL,   api_key = NULL,   envname = \"langgraph-env\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_multimodal_prerequisites.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Multimodal Prerequisites — check_multimodal_prerequisites","text":"vision_provider Character: \"ollama\" \"openai\" vision_model Character: Model name (optional) api_key Character: API key OpenAI (using openai provider) envname Character: Python environment name","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_multimodal_prerequisites.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Multimodal Prerequisites — check_multimodal_prerequisites","text":"List : ready: Logical - TRUE prerequisites met missing: Character vector missing components instructions: Character - Detailed setup instructions details: List component-specific status","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Ollama is Available — check_ollama","title":"Check if Ollama is Available — check_ollama","text":"Checks Ollama installed running local machine.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Ollama is Available — check_ollama","text":"","code":"check_ollama(verbose = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Ollama is Available — check_ollama","text":"verbose Logical, TRUE, prints status messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Ollama is Available — check_ollama","text":"Logical indicating whether Ollama available.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_ollama.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if Ollama is Available — check_ollama","text":"","code":"if (FALSE) { # \\dontrun{ if (check_ollama()) {   message(\"Ollama is ready!\") } } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_python_env.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Python Environment Status — check_python_env","title":"Check Python Environment Status — check_python_env","text":"Checks Python environment available properly configured.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_python_env.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Python Environment Status — check_python_env","text":"","code":"check_python_env(envname = \"textanalysisr-env\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_python_env.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Python Environment Status — check_python_env","text":"envname Character string name virtual environment (default: \"textanalysisr-env\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_python_env.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Python Environment Status — check_python_env","text":"List status information: available: Logical, TRUE environment exists active: Logical, TRUE environment currently active packages: List installed package versions","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_python_env.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Python Environment Status — check_python_env","text":"","code":"if (FALSE) { # \\dontrun{ status <- check_python_env() print(status) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_rate_limit.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Rate Limit — check_rate_limit","title":"Check Rate Limit — check_rate_limit","text":"Check Rate Limit","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_rate_limit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Rate Limit — check_rate_limit","text":"","code":"check_rate_limit(   session_token,   user_requests,   max_requests = 100,   window_seconds = 3600 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_rate_limit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Rate Limit — check_rate_limit","text":"session_token Shiny session token user_requests Reactive value storing request history max_requests Maximum requests allowed time window window_seconds Time window seconds (default: 3600 = 1 hour)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_rate_limit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Rate Limit — check_rate_limit","text":"TRUE within limit, stops error exceeded","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_vision_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Vision Model Availability — check_vision_models","title":"Check Vision Model Availability — check_vision_models","text":"Check required vision models available multimodal processing.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_vision_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Vision Model Availability — check_vision_models","text":"","code":"check_vision_models(provider = \"ollama\", api_key = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_vision_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Vision Model Availability — check_vision_models","text":"provider Character: \"ollama\" \"openai\" api_key Character: API key (OpenAI)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_vision_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Vision Model Availability — check_vision_models","text":"List availability status recommendations","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_vision_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Vision Model Availability — check_vision_models","text":"","code":"if (FALSE) { # \\dontrun{ # Check Ollama vision models status <- check_vision_models(\"ollama\") print(status$message)  # Check OpenAI access status <- check_vision_models(\"openai\", api_key = Sys.getenv(\"OPENAI_API_KEY\")) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_wcag_contrast.html","id":null,"dir":"Reference","previous_headings":"","what":"Check WCAG Contrast Compliance — check_wcag_contrast","title":"Check WCAG Contrast Compliance — check_wcag_contrast","text":"Validates color combination meets WCAG 2.1 Level AA contrast requirements.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_wcag_contrast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check WCAG Contrast Compliance — check_wcag_contrast","text":"","code":"check_wcag_contrast(foreground, background, large_text = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_wcag_contrast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check WCAG Contrast Compliance — check_wcag_contrast","text":"foreground Foreground color (hex format) background Background color (hex format) large_text Logical, TRUE text large (18pt+ 14pt+ bold)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_wcag_contrast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check WCAG Contrast Compliance — check_wcag_contrast","text":"Logical TRUE compliant, FALSE ","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_wcag_contrast.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check WCAG Contrast Compliance — check_wcag_contrast","text":"","code":"if (FALSE) { # \\dontrun{ check_wcag_contrast(\"#111827\", \"#ffffff\")  # TRUE (16:1 ratio) check_wcag_contrast(\"#6b7280\", \"#4a5568\")  # FALSE (2.8:1 ratio) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_web_deployment.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Deployment Environment — check_web_deployment","title":"Check Deployment Environment — check_web_deployment","text":"Detects whether app running web server (shinyapps.io, Posit Connect) versus locally via run_app().","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_web_deployment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Deployment Environment — check_web_deployment","text":"","code":"check_web_deployment()"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_web_deployment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Deployment Environment — check_web_deployment","text":"Logical TRUE running web server, FALSE local","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/check_web_deployment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Deployment Environment — check_web_deployment","text":"","code":"if (check_web_deployment()) {   message(\"Running on web - some features disabled\") }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/clear_lexdiv_cache.html","id":null,"dir":"Reference","previous_headings":"","what":"Clear Lexical Diversity Cache — clear_lexdiv_cache","title":"Clear Lexical Diversity Cache — clear_lexdiv_cache","text":"Clears internal cache used lexical diversity calculations. Call function need free memory ensure fresh calculations.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/clear_lexdiv_cache.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clear Lexical Diversity Cache — clear_lexdiv_cache","text":"","code":"clear_lexdiv_cache()"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/clear_lexdiv_cache.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clear Lexical Diversity Cache — clear_lexdiv_cache","text":"Invisible NULL","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/cluster_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Embedding-based Document Clustering — cluster_embeddings","title":"Embedding-based Document Clustering — cluster_embeddings","text":"function performs clustering analysis using various methods, ordered simple comprehensive: k-means (simplest), hierarchical (intermediate), UMAP+DBSCAN (comprehensive).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/cluster_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedding-based Document Clustering — cluster_embeddings","text":"","code":"cluster_embeddings(   data_matrix,   method = \"kmeans\",   n_clusters = 0,   umap_neighbors = 15,   umap_min_dist = 0.1,   umap_n_components = 10,   umap_metric = \"cosine\",   dbscan_eps = 0,   dbscan_min_samples = 5,   seed = 123,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/cluster_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedding-based Document Clustering — cluster_embeddings","text":"data_matrix numeric matrix rows represent documents columns represent features. method clustering method. Options: \"kmeans\", \"hierarchical\", \"umap_dbscan\". n_clusters number clusters (k-means hierarchical). 0, optimal number determined automatically. umap_neighbors number neighbors UMAP (default: 15). umap_min_dist minimum distance UMAP (default: 0.1). umap_n_components number UMAP components (default: 10). umap_metric metric UMAP (default: \"cosine\"). dbscan_eps eps parameter DBSCAN. 0, optimal value determined automatically. dbscan_min_samples minimum samples DBSCAN (default: 5). seed Random seed reproducibility (default: 123). verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/cluster_embeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Embedding-based Document Clustering — cluster_embeddings","text":"list containing cluster assignments, method used, quality metrics.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/cluster_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Embedding-based Document Clustering — cluster_embeddings","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_cols(     mydata,     listed_vars = c(\"title\", \"keyword\", \"abstract\")   )    tokens <- TextAnalysisR::prep_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    data_matrix <- as.matrix(dfm_object)    kmeans_result <- TextAnalysisR::cluster_embeddings(     data_matrix,     method = \"kmeans\",     n_clusters = 5   )   print(kmeans_result)    hierarchical_result <- TextAnalysisR::cluster_embeddings(     data_matrix,     method = \"hierarchical\",     n_clusters = 5   )   print(hierarchical_result)    umap_dbscan_result <- TextAnalysisR::cluster_embeddings(     data_matrix,     method = \"umap_dbscan\",     umap_neighbors = 15,     umap_min_dist = 0.1   )   print(umap_dbscan_result) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/combine_keywords_weighted.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine Topic Keywords with Semantic Weighting — combine_keywords_weighted","title":"Combine Topic Keywords with Semantic Weighting — combine_keywords_weighted","text":"Combines keywords STM embedding-based topics using weighted term co-associations semantic similarity. Based ensemble topic modeling research (Belford et al., 2018).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/combine_keywords_weighted.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine Topic Keywords with Semantic Weighting — combine_keywords_weighted","text":"","code":"combine_keywords_weighted(   stm_words,   stm_probs = NULL,   embed_words,   embed_ranks = NULL,   n_keywords = 10,   stm_weight = 0.5 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/combine_keywords_weighted.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine Topic Keywords with Semantic Weighting — combine_keywords_weighted","text":"stm_words Character vector STM topic words. stm_probs Numeric vector word probabilities STM. embed_words Character vector embedding topic words. embed_ranks Numeric vector word ranks (1 = top word). n_keywords Number keywords return (default: 10). stm_weight Weight STM words (default: 0.5).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/combine_keywords_weighted.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine Topic Keywords with Semantic Weighting — combine_keywords_weighted","text":"list containing: combined_words: Combined keyword list word_scores: Score word source: Source word (stm, embedding, )","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/compute_hybrid_quality_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Hybrid Model Quality Metrics — compute_hybrid_quality_metrics","title":"Compute Hybrid Model Quality Metrics — compute_hybrid_quality_metrics","text":"Computes quality metrics hybrid topic models including semantic coherence, exclusivity, silhouette scores. Based research recommendations topic model evaluation (Roberts et al., Mimno et al.).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/compute_hybrid_quality_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Hybrid Model Quality Metrics — compute_hybrid_quality_metrics","text":"","code":"compute_hybrid_quality_metrics(   stm_model,   stm_documents,   embedding_result,   embeddings = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/compute_hybrid_quality_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Hybrid Model Quality Metrics — compute_hybrid_quality_metrics","text":"stm_model fitted STM model object. stm_documents STM-formatted documents. embedding_result Result fit_embedding_topics(). embeddings Document embeddings matrix (optional, silhouette).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/compute_hybrid_quality_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Hybrid Model Quality Metrics — compute_hybrid_quality_metrics","text":"list containing quality metrics: stm_coherence: Semantic coherence per STM topic stm_exclusivity: Exclusivity per STM topic stm_coherence_mean: Mean semantic coherence stm_exclusivity_mean: Mean exclusivity embedding_silhouette: Silhouette scores embedding clusters embedding_silhouette_mean: Mean silhouette score combined_quality: Overall quality score","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/compute_topic_alignment.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Topic Alignment Using Cosine Similarity — compute_topic_alignment","title":"Compute Topic Alignment Using Cosine Similarity — compute_topic_alignment","text":"Computes alignment STM embedding-based topics using cosine similarity topic-word distributions document-topic assignments. method follows research best practices cross-model topic alignment.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/compute_topic_alignment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Topic Alignment Using Cosine Similarity — compute_topic_alignment","text":"","code":"compute_topic_alignment(stm_model, embedding_result, stm_vocab, texts)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/compute_topic_alignment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Topic Alignment Using Cosine Similarity — compute_topic_alignment","text":"stm_model fitted STM model object. embedding_result Result fit_embedding_topics(). stm_vocab Vocabulary STM conversion. texts Original texts computing embedding topic centroids.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/compute_topic_alignment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Topic Alignment Using Cosine Similarity — compute_topic_alignment","text":"list containing alignment metrics: alignment_matrix: Cosine similarity matrix topics best_matches: Best matching embedding topic STM topic alignment_scores: Alignment score per topic overall_alignment: Mean alignment across topics assignment_agreement: Agreement document assignments correlation: Correlation assignment vectors","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_analysis_datatable.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Formatted Analysis Data Table — create_analysis_datatable","title":"Create Formatted Analysis Data Table — create_analysis_datatable","text":"Creates consistently formatted DT::datatable analysis results export buttons optional numeric formatting.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_analysis_datatable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Formatted Analysis Data Table — create_analysis_datatable","text":"","code":"create_analysis_datatable(   data,   colnames = NULL,   numeric_cols = NULL,   digits = 3,   export_formats = c(\"copy\", \"csv\", \"excel\", \"pdf\", \"print\"),   page_length = 25,   font_size = \"16px\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_analysis_datatable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Formatted Analysis Data Table — create_analysis_datatable","text":"data Data frame display colnames Optional character vector column names display numeric_cols Optional character vector numeric columns round digits Number digits rounding numeric columns (default: 3) export_formats Character vector export formats (default: c('copy', 'csv', 'excel', 'pdf', 'print')) page_length Number rows per page (default: 25) font_size Font size table cells (default: \"16px\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_analysis_datatable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Formatted Analysis Data Table — create_analysis_datatable","text":"DT::datatable object","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_analysis_datatable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Formatted Analysis Data Table — create_analysis_datatable","text":"","code":"if (FALSE) { # \\dontrun{ df <- data.frame(term = c(\"word1\", \"word2\"), score = c(0.123456, 0.789012)) create_analysis_datatable(df, numeric_cols = \"score\", digits = 3) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_empty_plot_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Empty Plot with Message — create_empty_plot_message","title":"Create Empty Plot with Message — create_empty_plot_message","text":"Creates empty plotly plot displaying centered message. Useful showing status messages, error states, empty data notifications.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_empty_plot_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Empty Plot with Message — create_empty_plot_message","text":"","code":"create_empty_plot_message(message, color = \"#6B7280\", font_size = 16)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_empty_plot_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Empty Plot with Message — create_empty_plot_message","text":"message Character string message display color Text color (default: \"#6B7280\") font_size Font size pixels (default: 16)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_empty_plot_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Empty Plot with Message — create_empty_plot_message","text":"plotly object centered message annotation","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_empty_plot_message.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Empty Plot with Message — create_empty_plot_message","text":"","code":"if (FALSE) { # \\dontrun{ create_empty_plot_message(\"No data available\") create_empty_plot_message(\"Click 'Run Analysis' to begin\", color = \"#337ab7\") } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_label_selection_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Label Selection UI Data — create_label_selection_data","title":"Create Label Selection UI Data — create_label_selection_data","text":"Creates structured list rendering label selection UI Shiny.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_label_selection_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Label Selection UI Data — create_label_selection_data","text":"","code":"create_label_selection_data(label_candidates)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_label_selection_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Label Selection UI Data — create_label_selection_data","text":"label_candidates List generate_topic_labels_langgraph()","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_label_selection_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Label Selection UI Data — create_label_selection_data","text":"List topic objects, : topic_number: Integer top_terms: Character vector candidates: List candidate objects","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_label_selection_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Label Selection UI Data — create_label_selection_data","text":"","code":"if (FALSE) { # \\dontrun{ result <- generate_topic_labels_langgraph(...) ui_data <- create_label_selection_data(result$label_candidates) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_message_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Message Data Table — create_message_table","title":"Create Message Data Table — create_message_table","text":"Creates formatted DT::datatable displaying informational message. Useful showing status messages place empty tables.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_message_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Message Data Table — create_message_table","text":"","code":"create_message_table(message, font_size = \"16px\", color = \"#6c757d\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_message_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Message Data Table — create_message_table","text":"message Character string message display font_size Font size (default: \"16px\") color Text color (default: \"#6c757d\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_message_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Message Data Table — create_message_table","text":"DT::datatable object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_message_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Message Data Table — create_message_table","text":"","code":"if (FALSE) { # \\dontrun{ create_message_table(\"No data available. Please run analysis first.\") } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_sr_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Screen Reader Text — create_sr_text","title":"Create Screen Reader Text — create_sr_text","text":"Generates visually hidden text screen readers (WCAG 4.1.2).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_sr_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Screen Reader Text — create_sr_text","text":"","code":"create_sr_text(text)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_sr_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Screen Reader Text — create_sr_text","text":"text Text read screen readers","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_sr_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Screen Reader Text — create_sr_text","text":"HTML span sr-class","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_sr_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Screen Reader Text — create_sr_text","text":"","code":"if (FALSE) { # \\dontrun{ create_sr_text(\"Loading results, please wait\") } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_standard_ggplot_theme.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Standard ggplot2 Theme — create_standard_ggplot_theme","title":"Create Standard ggplot2 Theme — create_standard_ggplot_theme","text":"Returns standardized ggplot2 theme matching TextAnalysisR design standards.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_standard_ggplot_theme.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Standard ggplot2 Theme — create_standard_ggplot_theme","text":"","code":"create_standard_ggplot_theme(base_size = 14)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_standard_ggplot_theme.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Standard ggplot2 Theme — create_standard_ggplot_theme","text":"base_size Base font size (default: 14)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_standard_ggplot_theme.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Standard ggplot2 Theme — create_standard_ggplot_theme","text":"ggplot2 theme object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/create_standard_ggplot_theme.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Standard ggplot2 Theme — create_standard_ggplot_theme","text":"","code":"if (FALSE) { # \\dontrun{ library(ggplot2) ggplot(mtcars, aes(mpg, wt)) +   geom_point() +   create_standard_ggplot_theme() } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/cross_analysis_validation.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross Analysis Validation — cross_analysis_validation","title":"Cross Analysis Validation — cross_analysis_validation","text":"Performs cross-validation text analysis results","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/cross_analysis_validation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross Analysis Validation — cross_analysis_validation","text":"","code":"cross_analysis_validation(results, verbose = FALSE, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/cross_analysis_validation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross Analysis Validation — cross_analysis_validation","text":"results Analysis results object validate verbose Logical indicating whether print progress messages ... Additional parameters","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/cross_analysis_validation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross Analysis Validation — cross_analysis_validation","text":"List containing validation status metrics","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_multi_words.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect Multi-Word Expressions — detect_multi_words","title":"Detect Multi-Word Expressions — detect_multi_words","text":"function detects multi-word expressions (collocations) specified sizes appear least specified number times provided tokens.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_multi_words.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect Multi-Word Expressions — detect_multi_words","text":"","code":"detect_multi_words(tokens, size = 2:5, min_count = 2)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_multi_words.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect Multi-Word Expressions — detect_multi_words","text":"tokens tokens object quanteda package. size numeric vector specifying sizes collocations detect (default: 2:5). min_count minimum number occurrences collocation considered (default: 2).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_multi_words.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect Multi-Word Expressions — detect_multi_words","text":"character vector detected collocations.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_multi_words.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect Multi-Word Expressions — detect_multi_words","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_cols(     mydata,     listed_vars = c(\"title\", \"keyword\", \"abstract\")   )    tokens <- TextAnalysisR::prep_texts(united_tbl, text_field = \"united_texts\")    collocations <- TextAnalysisR::detect_multi_words(tokens, size = 2:5, min_count = 2)   print(collocations) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect PDF Content Type — detect_pdf_content_type","title":"Detect PDF Content Type — detect_pdf_content_type","text":"Analyzes PDF determine contains readable text.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect PDF Content Type — detect_pdf_content_type","text":"","code":"detect_pdf_content_type(file_path)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect PDF Content Type — detect_pdf_content_type","text":"file_path Character string path PDF file","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect PDF Content Type — detect_pdf_content_type","text":"Character string: \"text\" \"unknown\"","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Detect PDF Content Type — detect_pdf_content_type","text":"Attempts text extraction using pdftools. Returns \"text\" successful, \"unknown\" extraction fails PDF empty. table extraction PDFs, use extract_tables_from_pdf_py.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect PDF Content Type — detect_pdf_content_type","text":"","code":"if (FALSE) { # \\dontrun{ pdf_path <- \"path/to/document.pdf\" content_type <- detect_pdf_content_type(pdf_path) print(content_type) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type_py.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect PDF Content Type using Python — detect_pdf_content_type_py","title":"Detect PDF Content Type using Python — detect_pdf_content_type_py","text":"Analyzes PDF determine contains primarily tabular data text.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type_py.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect PDF Content Type using Python — detect_pdf_content_type_py","text":"","code":"detect_pdf_content_type_py(file_path, envname = \"textanalysisr-env\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type_py.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect PDF Content Type using Python — detect_pdf_content_type_py","text":"file_path Character string path PDF file envname Character string, name Python virtual environment (default: \"langgraph-env\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type_py.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect PDF Content Type using Python — detect_pdf_content_type_py","text":"Character string: \"tabular\", \"text\", \"unknown\"","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/detect_pdf_content_type_py.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detect PDF Content Type using Python — detect_pdf_content_type_py","text":"","code":"if (FALSE) { # \\dontrun{ setup_langgraph_env()  pdf_path <- \"path/to/document.pdf\" content_type <- detect_pdf_content_type_py(pdf_path) print(content_type) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_1.html","id":null,"dir":"Reference","previous_headings":"","what":"Dictionary List 1 — dictionary_list_1","title":"Dictionary List 1 — dictionary_list_1","text":"dataset containing dictionary terms text analysis","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_1.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dictionary List 1 — dictionary_list_1","text":"character vector dictionary terms","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_2.html","id":null,"dir":"Reference","previous_headings":"","what":"Dictionary List 2 — dictionary_list_2","title":"Dictionary List 2 — dictionary_list_2","text":"dataset containing additional dictionary terms text analysis","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/dictionary_list_2.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dictionary List 2 — dictionary_list_2","text":"character vector dictionary terms","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/export_document_clustering.html","id":null,"dir":"Reference","previous_headings":"","what":"Export Document Clustering Analysis — export_document_clustering","title":"Export Document Clustering Analysis — export_document_clustering","text":"Export document clustering analysis results CSV","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/export_document_clustering.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export Document Clustering Analysis — export_document_clustering","text":"","code":"export_document_clustering(   coordinates,   clusters = NULL,   labels = NULL,   doc_ids = NULL,   file_path )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/export_document_clustering.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export Document Clustering Analysis — export_document_clustering","text":"coordinates Document coordinates clusters Cluster assignments (optional) labels Cluster labels (optional) doc_ids Document IDs file_path Path save CSV file","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_cross_category_similarities.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Cross-Category Similarities from Full Similarity Matrix — extract_cross_category_similarities","title":"Extract Cross-Category Similarities from Full Similarity Matrix — extract_cross_category_similarities","text":"Given full similarity matrix category information, extracts pairwise similarities reference category categories long-format data frame suitable visualization analysis.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_cross_category_similarities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Cross-Category Similarities from Full Similarity Matrix — extract_cross_category_similarities","text":"","code":"extract_cross_category_similarities(   similarity_matrix,   docs_data,   reference_category,   compare_categories = NULL,   category_var = \"category\",   id_var = \"display_name\",   name_var = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_cross_category_similarities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Cross-Category Similarities from Full Similarity Matrix — extract_cross_category_similarities","text":"similarity_matrix square similarity matrix (n x n). docs_data data frame containing document metadata least: category_var Column indicating category membership id_var Column unique document identifiers reference_category Character string specifying reference category compare . compare_categories Character vector categories compare reference. NULL, compares categories except reference. category_var Name column containing category information (default: \"category\"). id_var Name column containing document IDs (default: \"display_name\"). name_var Optional name column display names (default: NULL, uses id_var).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_cross_category_similarities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Cross-Category Similarities from Full Similarity Matrix — extract_cross_category_similarities","text":"data frame columns: ref_id Reference document ID ref_name Reference document name (name_var provided) other_id Comparison document ID other_name Comparison document name (name_var provided) other_category Category comparison document similarity Cosine similarity value","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_cross_category_similarities.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Cross-Category Similarities from Full Similarity Matrix — extract_cross_category_similarities","text":"","code":"if (FALSE) { # \\dontrun{ # After calculating full similarity matrix similarity_result <- TextAnalysisR::calculate_document_similarity(   texts = docs$text,   document_feature_type = \"embeddings\" )  cross_sims <- extract_cross_category_similarities(   similarity_matrix = similarity_result$similarity_matrix,   docs_data = docs,   reference_category = \"SLD\",   compare_categories = c(\"Other Disability\", \"General\"),   category_var = \"category\",   id_var = \"display_name\",   name_var = \"doc_name\" ) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_keywords_keyness.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Keywords Using Statistical Keyness — extract_keywords_keyness","title":"Extract Keywords Using Statistical Keyness — extract_keywords_keyness","text":"Extracts distinctive keywords comparing document groups using log-likelihood ratio (G-squared).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_keywords_keyness.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Keywords Using Statistical Keyness — extract_keywords_keyness","text":"","code":"extract_keywords_keyness(dfm, target, top_n = 20, measure = \"lr\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_keywords_keyness.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Keywords Using Statistical Keyness — extract_keywords_keyness","text":"dfm quanteda dfm object target Target document indices logical vector top_n Number top keywords extract (default: 20) measure Keyness measure: \"lr\" (log-likelihood) \"chi2\" (default: \"lr\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_keywords_keyness.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Keywords Using Statistical Keyness — extract_keywords_keyness","text":"Data frame columns: Keyword, Keyness_Score","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_keywords_keyness.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Keywords Using Statistical Keyness — extract_keywords_keyness","text":"","code":"if (FALSE) { # \\dontrun{ library(quanteda) corp <- corpus(c(\"positive text\", \"negative text\", \"positive words\")) dfm_obj <- dfm(tokens(corp)) # Compare first document vs rest keywords <- extract_keywords_keyness(dfm_obj, target = 1) print(keywords) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_keywords_tfidf.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Keywords Using TF-IDF — extract_keywords_tfidf","title":"Extract Keywords Using TF-IDF — extract_keywords_tfidf","text":"Extracts top keywords document-feature matrix using TF-IDF weighting.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_keywords_tfidf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Keywords Using TF-IDF — extract_keywords_tfidf","text":"","code":"extract_keywords_tfidf(dfm, top_n = 20, normalize = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_keywords_tfidf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Keywords Using TF-IDF — extract_keywords_tfidf","text":"dfm quanteda dfm object top_n Number top keywords extract (default: 20) normalize Logical, whether normalize TF-IDF scores 0-1 range (default: FALSE)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_keywords_tfidf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Keywords Using TF-IDF — extract_keywords_tfidf","text":"Data frame columns: Keyword, TF_IDF_Score, Frequency","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_keywords_tfidf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Keywords Using TF-IDF — extract_keywords_tfidf","text":"","code":"if (FALSE) { # \\dontrun{ library(quanteda) corp <- corpus(c(\"text analysis\", \"data mining\", \"text mining\")) dfm_obj <- dfm(tokens(corp)) keywords <- extract_keywords_tfidf(dfm_obj, top_n = 5) print(keywords) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_named_entities.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Named Entities from Tokens — extract_named_entities","title":"Extract Named Entities from Tokens — extract_named_entities","text":"Uses spaCy extract named entities (NER) tokenized text. Returns data frame token-level entity annotations.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_named_entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Named Entities from Tokens — extract_named_entities","text":"","code":"extract_named_entities(   tokens,   include_pos = TRUE,   include_lemma = TRUE,   model = \"en_core_web_sm\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_named_entities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Named Entities from Tokens — extract_named_entities","text":"tokens quanteda tokens object character vector texts. include_pos Logical; include POS tags (default: TRUE). include_lemma Logical; include lemmatized forms (default: TRUE). model Character; spaCy model use (default: \"en_core_web_sm\").","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_named_entities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Named Entities from Tokens — extract_named_entities","text":"data frame columns: doc_id: Document identifier token: Original token entity: Named entity type (e.g., PERSON, ORG, GPE) pos: Universal POS tag (include_pos = TRUE) lemma: Lemmatized form (include_lemma = TRUE)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_named_entities.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Named Entities from Tokens — extract_named_entities","text":"function requires spacyr package working Python environment spaCy installed. spaCy initialized, function attempt initialize specified model.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_named_entities.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Named Entities from Tokens — extract_named_entities","text":"","code":"if (FALSE) { # \\dontrun{ tokens <- quanteda::tokens(\"Apple Inc. was founded by Steve Jobs in California.\") entity_data <- extract_named_entities(tokens) print(entity_data) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_multimodal.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract PDF with Multimodal Analysis — extract_pdf_multimodal","title":"Extract PDF with Multimodal Analysis — extract_pdf_multimodal","text":"Extract text visual content PDFs, converting everything text downstream analysis existing workflow.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_multimodal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract PDF with Multimodal Analysis — extract_pdf_multimodal","text":"","code":"extract_pdf_multimodal(   file_path,   vision_provider = \"ollama\",   vision_model = NULL,   api_key = NULL,   describe_images = TRUE,   envname = \"langgraph-env\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_multimodal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract PDF with Multimodal Analysis — extract_pdf_multimodal","text":"file_path Character string path PDF file vision_provider Character: \"ollama\" (local, default) \"openai\" (cloud) vision_model Character: Model name Ollama: \"llava\", \"llava:13b\", \"bakllava\" OpenAI: \"gpt-4-vision-preview\", \"gpt-4o\" api_key Character: OpenAI API key (required vision_provider=\"openai\") describe_images Logical: Convert images text descriptions (default: TRUE) envname Character: Python environment name (default: \"langgraph-env\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_multimodal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract PDF with Multimodal Analysis — extract_pdf_multimodal","text":"List : success: Logical combined_text: Character string content text analysis text_content: List text chunks image_descriptions: List image descriptions num_images: Integer count processed images vision_provider: Character indicating provider used message: Character status message","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_multimodal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract PDF with Multimodal Analysis — extract_pdf_multimodal","text":"Workflow Integration: Extracts text using Marker (preserves equations, tables, structure) Detects images/charts/diagrams PDF Uses vision LLM describe visual content text Merges text + descriptions → single text corpus Feed existing text analysis pipeline Vision Provider Options: Ollama (Default - Local & Free): Privacy: Everything runs locally Cost: Free Setup: Requires Ollama installed + vision model pulled Models: llava, bakllava, llava-phi3 OpenAI (Optional - Cloud): Privacy: Data sent OpenAI Cost: Paid (user's API key) Setup: Just provide API key Models: gpt-4-vision-preview, gpt-4o","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_multimodal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract PDF with Multimodal Analysis — extract_pdf_multimodal","text":"","code":"if (FALSE) { # \\dontrun{ # Local analysis with Ollama (free, private) result <- extract_pdf_multimodal(\"research_paper.pdf\")  # Access combined text for analysis text_for_analysis <- result$combined_text  # Use in existing workflow corpus <- prep_texts(text_for_analysis) topics <- fit_semantic_model(corpus, k = 5)  # Optional: Use OpenAI for better accuracy result <- extract_pdf_multimodal(   \"paper.pdf\",   vision_provider = \"openai\",   vision_model = \"gpt-4o\",   api_key = Sys.getenv(\"OPENAI_API_KEY\") ) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_smart.html","id":null,"dir":"Reference","previous_headings":"","what":"Smart PDF Extraction with Auto-Detection — extract_pdf_smart","title":"Smart PDF Extraction with Auto-Detection — extract_pdf_smart","text":"Automatically detects document type chooses best extraction method: Academic papers → Nougat (equations) Documents visuals → Multimodal extraction General documents → Marker","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_smart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Smart PDF Extraction with Auto-Detection — extract_pdf_smart","text":"","code":"extract_pdf_smart(   file_path,   doc_type = \"auto\",   vision_provider = \"ollama\",   vision_model = NULL,   api_key = NULL,   envname = \"langgraph-env\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_smart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Smart PDF Extraction with Auto-Detection — extract_pdf_smart","text":"file_path Character string path PDF file doc_type Character: \"auto\" (default), \"academic\", \"general\" vision_provider Character: \"ollama\" (default) \"openai\" vision_model Character: Model name vision analysis api_key Character: API key cloud providers envname Character: Python environment name","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_smart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Smart PDF Extraction with Auto-Detection — extract_pdf_smart","text":"List extracted content ready text analysis","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pdf_smart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Smart PDF Extraction with Auto-Detection — extract_pdf_smart","text":"","code":"if (FALSE) { # \\dontrun{ # Auto-detect and extract result <- extract_pdf_smart(\"document.pdf\")  # Feed to text analysis corpus <- prep_texts(result$combined_text) topics <- fit_semantic_model(corpus, k = 10)  # Force academic extraction (with equations) result <- extract_pdf_smart(\"paper.pdf\", doc_type = \"academic\") } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pos_tags.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Part-of-Speech Tags from Tokens — extract_pos_tags","title":"Extract Part-of-Speech Tags from Tokens — extract_pos_tags","text":"Uses spaCy extract part--speech (POS) tags tokenized text. Returns data frame token-level POS annotations.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pos_tags.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Part-of-Speech Tags from Tokens — extract_pos_tags","text":"","code":"extract_pos_tags(   tokens,   include_lemma = TRUE,   include_entity = FALSE,   include_dependency = FALSE,   model = \"en_core_web_sm\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pos_tags.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Part-of-Speech Tags from Tokens — extract_pos_tags","text":"tokens quanteda tokens object character vector texts. include_lemma Logical; include lemmatized forms (default: TRUE). include_entity Logical; include named entity recognition (default: FALSE). include_dependency Logical; include dependency parsing (default: FALSE). model Character; spaCy model use (default: \"en_core_web_sm\").","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pos_tags.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Part-of-Speech Tags from Tokens — extract_pos_tags","text":"data frame columns: doc_id: Document identifier sentence_id: Sentence number within document token_id: Token position within sentence token: Original token pos: Universal POS tag (e.g., NOUN, VERB, ADJ) tag: Detailed POS tag (e.g., NN, VBD, JJ) lemma: Lemmatized form (include_lemma = TRUE) entity: Named entity type (include_entity = TRUE) head_token_id: Head token dependency tree (include_dependency = TRUE) dep_rel: Dependency relation type, e.g., nsubj, dobj (include_dependency = TRUE)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pos_tags.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Part-of-Speech Tags from Tokens — extract_pos_tags","text":"function requires spacyr package working Python environment spaCy installed. spaCy initialized, function attempt initialize specified model.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_pos_tags.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Part-of-Speech Tags from Tokens — extract_pos_tags","text":"","code":"if (FALSE) { # \\dontrun{ tokens <- quanteda::tokens(\"The quick brown fox jumps over the lazy dog.\") pos_data <- extract_pos_tags(tokens) print(pos_data) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_tables_from_pdf_py.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Tables from PDF using Python — extract_tables_from_pdf_py","title":"Extract Tables from PDF using Python — extract_tables_from_pdf_py","text":"Extracts tabular data PDF using pdfplumber (Python). Java required - pure Python solution.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_tables_from_pdf_py.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Tables from PDF using Python — extract_tables_from_pdf_py","text":"","code":"extract_tables_from_pdf_py(   file_path,   pages = NULL,   envname = \"textanalysisr-env\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_tables_from_pdf_py.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Tables from PDF using Python — extract_tables_from_pdf_py","text":"file_path Character string path PDF file pages Integer vector page numbers process (NULL = pages) envname Character string, name Python virtual environment (default: \"langgraph-env\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_tables_from_pdf_py.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Tables from PDF using Python — extract_tables_from_pdf_py","text":"Data frame extracted table data Returns NULL tables found extraction fails","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_tables_from_pdf_py.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Tables from PDF using Python — extract_tables_from_pdf_py","text":"Uses pdfplumber Python library reticulate. Works complex table layouts without Java dependency.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_tables_from_pdf_py.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Tables from PDF using Python — extract_tables_from_pdf_py","text":"","code":"if (FALSE) { # \\dontrun{ setup_langgraph_env()  pdf_path <- \"path/to/table_document.pdf\" table_data <- extract_tables_from_pdf_py(pdf_path) head(table_data) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Text from PDF — extract_text_from_pdf","title":"Extract Text from PDF — extract_text_from_pdf","text":"Extracts text content PDF file using pdftools package.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Text from PDF — extract_text_from_pdf","text":"","code":"extract_text_from_pdf(file_path)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Text from PDF — extract_text_from_pdf","text":"file_path Character string path PDF file","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Text from PDF — extract_text_from_pdf","text":"Data frame columns: page (integer), text (character) Returns NULL extraction fails PDF empty","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Text from PDF — extract_text_from_pdf","text":"Uses pdftools::pdf_text() extract text page. Preserves page structure cleans whitespace. Works best text-based PDFs (scanned images).","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Text from PDF — extract_text_from_pdf","text":"","code":"if (FALSE) { # \\dontrun{ pdf_path <- \"path/to/document.pdf\" text_data <- extract_text_from_pdf(pdf_path) head(text_data) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf_py.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Text from PDF using Python — extract_text_from_pdf_py","title":"Extract Text from PDF using Python — extract_text_from_pdf_py","text":"Extracts text content PDF file using pdfplumber (Python). Java required - uses Python environment.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf_py.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Text from PDF using Python — extract_text_from_pdf_py","text":"","code":"extract_text_from_pdf_py(file_path, envname = \"textanalysisr-env\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf_py.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Text from PDF using Python — extract_text_from_pdf_py","text":"file_path Character string path PDF file envname Character string, name Python virtual environment (default: \"langgraph-env\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf_py.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Text from PDF using Python — extract_text_from_pdf_py","text":"Data frame columns: page (integer), text (character) Returns NULL extraction fails PDF empty","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf_py.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Text from PDF using Python — extract_text_from_pdf_py","text":"Uses pdfplumber Python library reticulate. Requires Python environment setup. See setup_langgraph_env().","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/extract_text_from_pdf_py.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Text from PDF using Python — extract_text_from_pdf_py","text":"","code":"if (FALSE) { # \\dontrun{ setup_langgraph_env()  pdf_path <- \"path/to/document.pdf\" text_data <- extract_text_from_pdf_py(pdf_path) head(text_data) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/find_optimal_k.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Optimal Number of Topics — find_optimal_k","title":"Find Optimal Number of Topics — find_optimal_k","text":"Searches optimal number topics (K) using stm::searchK. Produces diagnostic plots help select best K value.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/find_optimal_k.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Optimal Number of Topics — find_optimal_k","text":"","code":"find_optimal_k(   dfm_object,   topic_range,   max.em.its = 75,   emtol = 1e-04,   cores = 1,   categorical_var = NULL,   continuous_var = NULL,   height = 600,   width = 800,   verbose = TRUE,   ... )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/find_optimal_k.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Optimal Number of Topics — find_optimal_k","text":"dfm_object quanteda dfm object used topic modeling. topic_range vector K values test (e.g., 2:10). max.em.Maximum number EM iterations (default: 75). emtol Convergence tolerance EM algorithm (default: 1e-04). Higher values (e.g., 1e-03) speed fitting may reduce precision. cores Number CPU cores use parallel processing (default: 1). Set higher values faster searchK multi-core systems. categorical_var Optional categorical variable(s) prevalence. continuous_var Optional continuous variable(s) prevalence. height Plot height pixels (default: 600). width Plot width pixels (default: 800). verbose Logical indicating whether print progress (default: TRUE). ... Additional arguments passed stm::searchK.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/find_optimal_k.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Optimal Number of Topics — find_optimal_k","text":"list containing search results diagnostic plots.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/find_topic_matches.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Similar Topics — find_topic_matches","title":"Find Similar Topics — find_topic_matches","text":"function finds similar topics given query using semantic similarity analysis. works semantic topic models traditional STM models creating topic representations using transformer embeddings calculating cosine similarity scores.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/find_topic_matches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Similar Topics — find_topic_matches","text":"","code":"find_topic_matches(   topic_model,   query,   top_n = 10,   method = \"cosine\",   embedding_model = \"all-MiniLM-L6-v2\",   include_terms = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/find_topic_matches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Similar Topics — find_topic_matches","text":"topic_model topic model object (semantic topic model STM model). query character string representing query topic. top_n number similar topics return (default: 10). method similarity method: \"cosine\", \"euclidean\", \"embedding\". embedding_model embedding model use query encoding (default: \"-MiniLM-L6-v2\"). include_terms Logical, whether include topic terms similarity calculation (default: TRUE).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/find_topic_matches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Similar Topics — find_topic_matches","text":"list containing similar topics similarity scores.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/find_topic_matches.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Similar Topics — find_topic_matches","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::SpecialEduTech   united_tbl <- TextAnalysisR::unite_cols(     mydata,     listed_vars = c(\"title\", \"keyword\", \"abstract\")   )   texts <- united_tbl$united_texts    topic_model <- TextAnalysisR::fit_embedding_topics(     texts = texts,     method = \"semantic_style\",     n_topics = 8   )    similar_topics <- TextAnalysisR::find_similar_topics(     topic_model = topic_model,     query = \"mathematical learning\",     top_n = 5   )    print(similar_topics) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_embedding_topics.html","id":null,"dir":"Reference","previous_headings":"","what":"Embedding-based Topic Modeling — fit_embedding_topics","title":"Embedding-based Topic Modeling — fit_embedding_topics","text":"function performs embedding-based topic modeling using transformer embeddings specialized clustering techniques. primary method uses BERTopic library, combines transformer embeddings UMAP dimensionality reduction HDBSCAN clustering optimal topic discovery. approach creates semantically coherent topics compared traditional methods leveraging deep learning embeddings.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_embedding_topics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedding-based Topic Modeling — fit_embedding_topics","text":"","code":"fit_embedding_topics(   texts,   method = \"umap_hdbscan\",   n_topics = 10,   embedding_model = \"all-MiniLM-L6-v2\",   clustering_method = \"kmeans\",   similarity_threshold = 0.7,   min_topic_size = 3,   umap_neighbors = 15,   umap_min_dist = 0,   umap_n_components = 5,   representation_method = \"c-tfidf\",   diversity = 0.5,   reduce_outliers = TRUE,   seed = 123,   verbose = TRUE,   precomputed_embeddings = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_embedding_topics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedding-based Topic Modeling — fit_embedding_topics","text":"texts character vector texts analyze. method topic modeling method: \"umap_hdbscan\" (uses BERTopic), \"embedding_clustering\", \"hierarchical_semantic\". n_topics number topics identify. UMAP+HDBSCAN, use NULL \"auto\" automatic determination, specify integer. embedding_model embedding model use (default: \"-MiniLM-L6-v2\"). clustering_method clustering method embedding-based approach: \"kmeans\", \"hierarchical\", \"dbscan\", \"hdbscan\". similarity_threshold similarity threshold topic assignment (default: 0.7). min_topic_size minimum number documents per topic (default: 3). umap_neighbors number neighbors UMAP dimensionality reduction (default: 15). umap_min_dist minimum distance UMAP (default: 0.0). Use 0.0 tight, well-separated clusters. Use 0.1+ visualization purposes. Range: 0.0-0.99. umap_n_components number UMAP components (default: 5). representation_method method topic representation: \"c-tfidf\", \"tfidf\", \"mmr\", \"frequency\" (default: \"c-tfidf\"). diversity Topic diversity parameter 0 1 (default: 0.5). reduce_outliers Logical, TRUE, reduces outliers HDBSCAN clustering (default: TRUE). seed Random seed reproducibility (default: 123). verbose Logical, TRUE, prints progress messages. precomputed_embeddings Optional matrix pre-computed document embeddings. provided, skips embedding generation improved performance. Must number rows length texts.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_embedding_topics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Embedding-based Topic Modeling — fit_embedding_topics","text":"list containing topic assignments, topic keywords, quality metrics.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_embedding_topics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Embedding-based Topic Modeling — fit_embedding_topics","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::SpecialEduTech   united_tbl <- TextAnalysisR::unite_cols(     mydata,     listed_vars = c(\"title\", \"keyword\", \"abstract\")   )   texts <- united_tbl$united_texts    # Embedding-based topic modeling (powered by BERTopic)   result <- TextAnalysisR::fit_embedding_topics(     texts = texts,     method = \"umap_hdbscan\",     n_topics = 8,     min_topic_size = 3   )    print(result$topic_assignments)   print(result$topic_keywords) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_hybrid_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Hybrid Topic Model — fit_hybrid_model","title":"Fit Hybrid Topic Model — fit_hybrid_model","text":"Fits hybrid topic model combining STM embedding-based methods. function integrates structural topic modeling (STM) semantic embeddings enhanced topic discovery. STM component provides statistical rigor covariate modeling capabilities, embedding component adds semantic coherence. Effect Estimation: Covariate effects topic prevalence can estimated using STM component via stm::estimateEffect(). embedding component provides semantically meaningful topic representations support direct covariate modeling.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_hybrid_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Hybrid Topic Model — fit_hybrid_model","text":"","code":"fit_hybrid_model(   texts,   metadata = NULL,   n_topics_stm = 10,   embedding_model = \"all-MiniLM-L6-v2\",   stm_prevalence = NULL,   stm_init_type = \"Spectral\",   compute_quality = TRUE,   stm_weight = 0.5,   verbose = TRUE,   seed = 123 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_hybrid_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Hybrid Topic Model — fit_hybrid_model","text":"texts character vector texts analyze. metadata Optional data frame document metadata STM covariate modeling. n_topics_stm Number topics STM (default: 10). embedding_model Embedding model name (default: \"-MiniLM-L6-v2\"). stm_prevalence Formula STM prevalence covariates (e.g., ~ category + s(year, df=3)). stm_init_type STM initialization type (default: \"Spectral\"). compute_quality Logical, TRUE, computes quality metrics (default: TRUE). stm_weight Weight STM keyword combination, 0-1 (default: 0.5). verbose Logical, TRUE, prints progress messages. seed Random seed reproducibility.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_hybrid_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Hybrid Topic Model — fit_hybrid_model","text":"list containing: stm_result: STM model output (use effect estimation) embedding_result: embedding-based topic model output alignment: Comprehensive alignment metrics including cosine similarity, assignment agreement, correlation, Adjusted Rand Index quality_metrics: Quality metrics including coherence, exclusivity, silhouette scores, combined quality score combined_topics: Integrated topic representations weighted keywords stm_data: STM-formatted data (needed effect estimation) metadata: Metadata used modeling","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_hybrid_model.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Fit Hybrid Topic Model — fit_hybrid_model","text":"covariate effect estimation, use stm::estimateEffect() stm_result$model component stm_data$meta metadata.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_hybrid_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit Hybrid Topic Model — fit_hybrid_model","text":"","code":"if (FALSE) { # \\dontrun{   texts <- c(\"Computer-assisted instruction improves math skills for students with disabilities\",              \"Assistive technology supports reading comprehension for learning disabled students\",              \"Mobile devices enhance communication for students with autism spectrum disorder\")    hybrid_model <- fit_hybrid_model(     texts = texts,     n_topics_stm = 3,     compute_quality = TRUE,     verbose = TRUE   )    # View alignment metrics   hybrid_model$alignment$overall_alignment   hybrid_model$alignment$adjusted_rand_index    # View quality metrics   hybrid_model$quality_metrics$stm_coherence_mean   hybrid_model$quality_metrics$combined_quality    # View combined keywords with source attribution   hybrid_model$combined_topics[[1]]$combined_keywords } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_semantic_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Semantic Model — fit_semantic_model","title":"Fit Semantic Model — fit_semantic_model","text":"Performs comprehensive semantic analysis including similarity, dimensionality reduction, clustering. high-level wrapper function.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_semantic_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Semantic Model — fit_semantic_model","text":"","code":"fit_semantic_model(   texts,   analysis_types = c(\"similarity\", \"dimensionality_reduction\", \"clustering\"),   document_feature_type = \"embeddings\",   similarity_method = \"cosine\",   use_embeddings = TRUE,   embedding_model = \"all-MiniLM-L6-v2\",   dimred_method = \"UMAP\",   clustering_method = \"umap_dbscan\",   n_components = 2,   n_clusters = 5,   seed = 123,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_semantic_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Semantic Model — fit_semantic_model","text":"texts character vector texts analyze. analysis_types Types analysis perform: \"similarity\", \"dimensionality_reduction\", \"clustering\". document_feature_type Feature extraction type (default: \"embeddings\"). similarity_method Similarity calculation method (default: \"cosine\"). use_embeddings Logical, use embedding-based approaches (default: TRUE). embedding_model Sentence transformer model name (default: \"-MiniLM-L6-v2\"). dimred_method Dimensionality reduction method: \"PCA\", \"t-SNE\", \"UMAP\" (default: \"UMAP\"). clustering_method Clustering method: \"kmeans\", \"hierarchical\", \"umap_dbscan\" (default: \"umap_dbscan\"). n_components Number dimensions reduction (default: 2). n_clusters Number clusters (default: 5). seed Random seed reproducibility (default: 123). verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_semantic_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Semantic Model — fit_semantic_model","text":"list containing results requested analyses.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_semantic_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit Semantic Model — fit_semantic_model","text":"","code":"if (interactive()) {   texts <- c(     \"Assistive technology supports learning.\",     \"Technology aids students with disabilities.\",     \"Machine learning improves predictions.\"   )    results <- fit_semantic_model(     texts = texts,     analysis_types = c(\"similarity\", \"clustering\")   )    print(results) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_temporal_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Temporal Topic Model — fit_temporal_model","title":"Fit Temporal Topic Model — fit_temporal_model","text":"Analyzes topics evolve time fitting topic models different time periods tracking semantic changes.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_temporal_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Temporal Topic Model — fit_temporal_model","text":"","code":"fit_temporal_model(   texts,   dates,   time_windows = \"yearly\",   embeddings = NULL,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_temporal_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Temporal Topic Model — fit_temporal_model","text":"texts character vector text documents analyze. dates vector dates corresponding document (converted Date). time_windows Time grouping strategy: \"yearly\", \"monthly\", \"quarterly\" (default: \"yearly\"). embeddings Optional pre-computed embeddings matrix. NULL, embeddings generated. verbose Logical indicating whether print progress messages (default: TRUE).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/fit_temporal_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Temporal Topic Model — fit_temporal_model","text":"list containing temporal analysis results topic evolution patterns.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/format_label_candidates.html","id":null,"dir":"Reference","previous_headings":"","what":"Format Label Candidates for Display — format_label_candidates","title":"Format Label Candidates for Display — format_label_candidates","text":"Helper function format LangGraph label candidates display Shiny UI.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/format_label_candidates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format Label Candidates for Display — format_label_candidates","text":"","code":"format_label_candidates(label_candidates)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/format_label_candidates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format Label Candidates for Display — format_label_candidates","text":"label_candidates List label candidate objects generate_topic_labels_langgraph()","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/format_label_candidates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format Label Candidates for Display — format_label_candidates","text":"Data frame columns: topic_index: Integer, topic number top_terms: Character, comma-separated top terms label: Character, suggested label reasoning: Character, LLM explanation candidate_number: Integer, candidate rank (1-3)","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/format_label_candidates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format Label Candidates for Display — format_label_candidates","text":"","code":"if (FALSE) { # \\dontrun{ result <- generate_topic_labels_langgraph(...) df <- format_label_candidates(result$label_candidates) print(df) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_aria_label.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate ARIA Label — generate_aria_label","title":"Generate ARIA Label — generate_aria_label","text":"Creates accessible ARIA label UI elements.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_aria_label.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate ARIA Label — generate_aria_label","text":"","code":"generate_aria_label(element_type, action, context = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_aria_label.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate ARIA Label — generate_aria_label","text":"element_type Type element (e.g., \"button\", \"input\", \"plot\") action Action purpose (e.g., \"analyze\", \"download\", \"visualize\") context Additional context (optional)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_aria_label.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate ARIA Label — generate_aria_label","text":"Character string ARIA label","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_aria_label.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate ARIA Label — generate_aria_label","text":"","code":"if (FALSE) { # \\dontrun{ generate_aria_label(\"button\", \"analyze\", \"readability\") # Returns: \"Analyze readability button\" } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_cluster_labels.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Cluster Labels with AI — generate_cluster_labels","title":"Generate Cluster Labels with AI — generate_cluster_labels","text":"Generates descriptive labels clusters using either Ollama (local, default) OpenAI's API. running locally, Ollama preferred privacy cost-free operation.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_cluster_labels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Cluster Labels with AI — generate_cluster_labels","text":"","code":"generate_cluster_labels(   cluster_keywords,   provider = \"auto\",   model = NULL,   temperature = 0.3,   max_tokens = 50,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_cluster_labels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Cluster Labels with AI — generate_cluster_labels","text":"cluster_keywords List keywords cluster. provider AI provider use: \"auto\" (default), \"ollama\", \"openai\". \"auto\" use Ollama available, otherwise OpenAI. model Model name. Ollama (default: \"phi3:mini\"). OpenAI (default: \"gpt-3.5-turbo\"). temperature Temperature parameter (default: 0.3). max_tokens Maximum tokens response (default: 50). verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_cluster_labels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Cluster Labels with AI — generate_cluster_labels","text":"list generated labels.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_cluster_labels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Cluster Labels with AI — generate_cluster_labels","text":"","code":"if (FALSE) { # \\dontrun{ keywords <- list(\"1\" = c(\"machine\", \"learning\", \"neural\"), \"2\" = c(\"data\", \"analysis\")) labels_ollama <- generate_cluster_labels(keywords, provider = \"ollama\") labels_openai <- generate_cluster_labels(keywords, provider = \"openai\") } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_cluster_labels_auto.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Cluster Labels — generate_cluster_labels_auto","title":"Generate Cluster Labels — generate_cluster_labels_auto","text":"Generate descriptive labels document clusters","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_cluster_labels_auto.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Cluster Labels — generate_cluster_labels_auto","text":"","code":"generate_cluster_labels_auto(   feature_matrix,   clusters,   method = \"tfidf\",   n_terms = 3 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_cluster_labels_auto.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Cluster Labels — generate_cluster_labels_auto","text":"feature_matrix Feature matrix used clustering clusters Cluster assignments method Label generation method (\"tfidf\", \"representative\", \"frequent\") n_terms Number terms per label","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_cluster_labels_auto.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Cluster Labels — generate_cluster_labels_auto","text":"Named list cluster labels","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings — generate_embeddings","title":"Generate Embeddings — generate_embeddings","text":"Generates embeddings texts using sentence transformers.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings — generate_embeddings","text":"","code":"generate_embeddings(texts, model = \"all-MiniLM-L6-v2\", verbose = TRUE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings — generate_embeddings","text":"texts character vector texts. model Sentence transformer model name (default: \"-MiniLM-L6-v2\"). verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_embeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings — generate_embeddings","text":"matrix embeddings.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_semantic_topic_keywords.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Semantic Topic Keywords (c-TF-IDF) — generate_semantic_topic_keywords","title":"Generate Semantic Topic Keywords (c-TF-IDF) — generate_semantic_topic_keywords","text":"Generate keywords topics using c-TF-IDF (class-based TF-IDF), similar BERTopic. method treats documents topic single document calculates TF-IDF scores relative topics.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_semantic_topic_keywords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Semantic Topic Keywords (c-TF-IDF) — generate_semantic_topic_keywords","text":"","code":"generate_semantic_topic_keywords(   texts,   topic_assignments,   n_keywords = 10,   method = \"c-tfidf\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_semantic_topic_keywords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Semantic Topic Keywords (c-TF-IDF) — generate_semantic_topic_keywords","text":"texts character vector texts. topic_assignments vector topic assignments. n_keywords number keywords extract per topic (default: 10). method representation method: \"c-tfidf\" (default), \"tfidf\", \"mmr\", \"frequency\".","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_semantic_topic_keywords.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Semantic Topic Keywords (c-TF-IDF) — generate_semantic_topic_keywords","text":"list keywords topic.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_survey_items.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Survey Items from Topic Terms — generate_survey_items","title":"Generate Survey Items from Topic Terms — generate_survey_items","text":"Convenience wrapper generate_topic_content content_type = \"survey_item\". Generates Likert-scale survey items scale development.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_survey_items.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Survey Items from Topic Terms — generate_survey_items","text":"","code":"generate_survey_items(   topic_terms_df,   topic_var = \"topic\",   term_var = \"term\",   weight_var = \"beta\",   provider = c(\"openai\", \"ollama\"),   model = \"gpt-3.5-turbo\",   temperature = 0,   system_prompt = NULL,   user_prompt_template = NULL,   max_tokens = 150,   api_key = NULL,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_survey_items.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Survey Items from Topic Terms — generate_survey_items","text":"topic_terms_df data frame topic terms, containing columns topic identifier, term, optionally term weight (beta). topic_var Name column containing topic identifiers (default: \"topic\"). term_var Name column containing terms (default: \"term\"). weight_var Name column containing term weights (default: \"beta\"). provider LLM provider: \"openai\" \"ollama\" (default: \"openai\"). model Model name. OpenAI: \"gpt-3.5-turbo\", \"gpt-4\", etc. Ollama: \"llama3\", \"mistral\", etc. temperature Sampling temperature (0-2). Lower = deterministic (default: 0). system_prompt Custom system prompt. NULL, uses default content_type. user_prompt_template Custom user prompt template {terms} placeholder. NULL, uses default content_type. max_tokens Maximum tokens response (default: 150). api_key OpenAI API key. NULL, reads OPENAI_API_KEY environment variable. verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_survey_items.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Survey Items from Topic Terms — generate_survey_items","text":"data frame generated survey items joined original topic terms.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_survey_items.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Survey Items from Topic Terms — generate_survey_items","text":"","code":"if (FALSE) { # \\dontrun{ survey_items <- generate_survey_items(   topic_terms_df = top_terms,   provider = \"openai\",   model = \"gpt-3.5-turbo\" ) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_content.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Content from Topic Terms — generate_topic_content","title":"Generate Content from Topic Terms — generate_topic_content","text":"Uses Large Language Models (LLMs) generate various types content based topic model terms. Supports multiple content types optimized default prompts, fully custom prompts.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_content.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Content from Topic Terms — generate_topic_content","text":"","code":"generate_topic_content(   topic_terms_df,   content_type = c(\"survey_item\", \"research_question\", \"theme_description\",     \"policy_recommendation\", \"interview_question\", \"custom\"),   topic_var = \"topic\",   term_var = \"term\",   weight_var = \"beta\",   provider = c(\"openai\", \"ollama\"),   model = \"gpt-3.5-turbo\",   temperature = 0,   system_prompt = NULL,   user_prompt_template = NULL,   max_tokens = 150,   api_key = NULL,   output_var = NULL,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_content.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Content from Topic Terms — generate_topic_content","text":"topic_terms_df data frame topic terms, containing columns topic identifier, term, optionally term weight (beta). content_type Type content generate. One : \"survey_item\" Likert-scale survey items scale development \"research_question\" Research questions literature review \"theme_description\" Theme descriptions qualitative analysis \"policy_recommendation\" Policy recommendations policy analysis \"interview_question\" Interview questions qualitative research \"custom\" Custom content using user-provided prompts topic_var Name column containing topic identifiers (default: \"topic\"). term_var Name column containing terms (default: \"term\"). weight_var Name column containing term weights (default: \"beta\"). provider LLM provider: \"openai\" \"ollama\" (default: \"openai\"). model Model name. OpenAI: \"gpt-3.5-turbo\", \"gpt-4\", etc. Ollama: \"llama3\", \"mistral\", etc. temperature Sampling temperature (0-2). Lower = deterministic (default: 0). system_prompt Custom system prompt. NULL, uses default content_type. user_prompt_template Custom user prompt template {terms} placeholder. NULL, uses default content_type. max_tokens Maximum tokens response (default: 150). api_key OpenAI API key. NULL, reads OPENAI_API_KEY environment variable. output_var Name output column (default: based content_type). verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_content.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Content from Topic Terms — generate_topic_content","text":"data frame generated content joined original topic terms.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_content.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Content from Topic Terms — generate_topic_content","text":"function generates one piece content per unique topic. content type optimized default prompts, can overridden custom prompts. OpenAI, requires API key set via api_key parameter OPENAI_API_KEY environment variable (can loaded .env file). Ollama, requires local Ollama installation specified model.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_content.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Content from Topic Terms — generate_topic_content","text":"","code":"if (FALSE) { # \\dontrun{ # Generate survey items survey_items <- generate_topic_content(   topic_terms_df = top_terms,   content_type = \"survey_item\",   provider = \"openai\",   model = \"gpt-3.5-turbo\" )  # Generate research questions research_qs <- generate_topic_content(   topic_terms_df = top_terms,   content_type = \"research_question\",   provider = \"ollama\",   model = \"llama3\" )  # Generate with custom prompt custom_content <- generate_topic_content(   topic_terms_df = top_terms,   content_type = \"custom\",   system_prompt = \"You are an expert in educational policy...\",   user_prompt_template = \"Based on {terms}, generate a learning objective:\" ) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_keywords.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Topic Keywords — generate_topic_keywords","title":"Generate Topic Keywords — generate_topic_keywords","text":"Internal function generate keywords topics using TF-IDF analysis.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_keywords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Topic Keywords — generate_topic_keywords","text":"","code":"generate_topic_keywords(texts, topic_assignments, n_keywords = 10)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_keywords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Topic Keywords — generate_topic_keywords","text":"texts character vector texts. topic_assignments vector topic assignments. n_keywords number keywords extract per topic.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_keywords.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Topic Keywords — generate_topic_keywords","text":"list keywords topic.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","title":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","text":"function generates descriptive labels topic based top terms using OpenAI's ChatCompletion API.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","text":"","code":"generate_topic_labels(   top_topic_terms,   model = \"gpt-3.5-turbo\",   system = NULL,   user = NULL,   temperature = 0.5,   openai_api_key = NULL,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","text":"top_topic_terms data frame containing top terms topic. model character string specifying OpenAI model use (default: \"gpt-3.5-turbo\"). system character string containing system prompt OpenAI API. NULL, function uses default system prompt. user character string containing user prompt OpenAI API. NULL, function uses default user prompt. temperature numeric value controlling randomness output (default: 0.5). openai_api_key character string containing OpenAI API key. NULL, function attempts load key OPENAI_API_KEY environment variable .env file working directory. verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","text":"data frame containing top terms topic along generated labels.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Topic Labels Using OpenAI's API — generate_topic_labels","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_cols(     mydata,     listed_vars = c(\"title\", \"keyword\", \"abstract\")   )    tokens <- TextAnalysisR::prep_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    out <- quanteda::convert(dfm_object, to = \"stm\")  stm_15 <- stm::stm(   data = out$meta,   documents = out$documents,   vocab = out$vocab,   max.em.its = 75,   init.type = \"Spectral\",   K = 15,   prevalence = ~ reference_type + s(year),   verbose = TRUE)  top_topic_terms <- TextAnalysisR::get_topic_terms(   stm_model = stm_15,   top_term_n = 10,   verbose = TRUE   )  top_labeled_topic_terms <- TextAnalysisR::generate_topic_labels(   top_topic_terms,   model = \"gpt-3.5-turbo\",   temperature = 0.5,   openai_api_key = \"your_openai_api_key\",   verbose = TRUE) print(top_labeled_topic_terms)  top_labeled_topic_terms <- TextAnalysisR::generate_topic_labels(   top_topic_terms,   model = \"gpt-3.5-turbo\",   temperature = 0.5,   verbose = TRUE) print(top_labeled_topic_terms) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels_langgraph.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Topic Labels with LLM Assistance — generate_topic_labels_langgraph","title":"Generate Topic Labels with LLM Assistance — generate_topic_labels_langgraph","text":"Uses LangGraph workflow generate multiple label candidates topics using local LLM (Ollama). Provides human---loop review suggestions.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels_langgraph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Topic Labels with LLM Assistance — generate_topic_labels_langgraph","text":"","code":"generate_topic_labels_langgraph(   topic_terms,   num_topics,   ollama_model = \"llama3\",   ollama_base_url = \"http://localhost:11434\",   envname = \"textanalysisr-env\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels_langgraph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Topic Labels with LLM Assistance — generate_topic_labels_langgraph","text":"topic_terms List character vectors, vector contains top terms topic (STM topic model) num_topics Integer, number topics ollama_model Character string, name Ollama model use (default: \"llama3\") ollama_base_url Character string, base URL Ollama API (default: \"http://localhost:11434\") envname Character string, name Python virtual environment (default: \"langgraph-env\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels_langgraph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Topic Labels with LLM Assistance — generate_topic_labels_langgraph","text":"List : success: Logical, TRUE workflow completed successfully label_candidates: List label candidate objects topic validation_metrics: Validation metrics (available) error: Error message (failed)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels_langgraph.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Topic Labels with LLM Assistance — generate_topic_labels_langgraph","text":"function: Initializes LangGraph Python environment Calls Python workflow generate label candidates Returns structured results display Shiny UI Allows human review selection labels workflow uses StateGraph nodes : Label generation (LLM) Validation (LLM) Conditional revision based quality metrics","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/generate_topic_labels_langgraph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Topic Labels with LLM Assistance — generate_topic_labels_langgraph","text":"","code":"if (FALSE) { # \\dontrun{ topic_terms <- list(   c(\"education\", \"student\", \"learning\", \"teacher\", \"school\"),   c(\"health\", \"medical\", \"patient\", \"doctor\", \"treatment\"),   c(\"environment\", \"climate\", \"carbon\", \"emissions\", \"energy\") )  result <- generate_topic_labels_langgraph(   topic_terms = topic_terms,   num_topics = 3,   ollama_model = \"llama3\" )  if (result$success) {   print(result$label_candidates) } } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_dfm.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Available Document-Feature Matrix with Fallback — get_available_dfm","title":"Get Available Document-Feature Matrix with Fallback — get_available_dfm","text":"Returns first non-NULL DFM priority fallback chain. Useful multiple DFM processing stages exist need processed available version.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_dfm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Available Document-Feature Matrix with Fallback — get_available_dfm","text":"","code":"get_available_dfm(   dfm_lemma = NULL,   dfm_outcome = NULL,   dfm_final = NULL,   dfm_init = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_dfm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Available Document-Feature Matrix with Fallback — get_available_dfm","text":"dfm_lemma Optional lemmatized DFM (highest priority) dfm_outcome Optional preprocessed DFM (medium priority) dfm_final Optional final processed DFM (medium-low priority) dfm_init Optional initial DFM (lowest priority)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_dfm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Available Document-Feature Matrix with Fallback — get_available_dfm","text":"first non-NULL DFM priority chain, NULL NULL","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_dfm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Available Document-Feature Matrix with Fallback — get_available_dfm","text":"Priority order (highest lowest): dfm_lemma - Lemmatized tokens (processed) dfm_outcome - Preprocessed tokens dfm_final - Final processed version dfm_init - Initial unprocessed tokens","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_dfm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Available Document-Feature Matrix with Fallback — get_available_dfm","text":"","code":"if (FALSE) { # \\dontrun{ dfm1 <- quanteda::dfm(quanteda::tokens(\"assistive technology supports learning\")) result <- get_available_dfm(dfm_init = dfm1) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_tokens.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Available Tokens with Fallback — get_available_tokens","title":"Get Available Tokens with Fallback — get_available_tokens","text":"Returns first non-NULL tokens object priority fallback chain. Useful multiple token processing stages exist need processed available version.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_tokens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Available Tokens with Fallback — get_available_tokens","text":"","code":"get_available_tokens(   final_tokens = NULL,   processed_tokens = NULL,   preprocessed_tokens = NULL,   united_tbl = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_tokens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Available Tokens with Fallback — get_available_tokens","text":"final_tokens Optional fully processed tokens (highest priority) processed_tokens Optional partially processed tokens preprocessed_tokens Optional early-stage preprocessed tokens united_tbl Optional data frame united_texts column (lowest priority, tokenized)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_tokens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Available Tokens with Fallback — get_available_tokens","text":"first non-NULL tokens priority chain, NULL NULL","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_tokens.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Available Tokens with Fallback — get_available_tokens","text":"Priority order (highest lowest): final_tokens - Fully processed tokens processed_tokens - Partially processed tokens preprocessed_tokens - Early stage preprocessed tokens united_tbl - Raw text (tokenized used)","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_available_tokens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Available Tokens with Fallback — get_available_tokens","text":"","code":"if (FALSE) { # \\dontrun{ tokens <- get_available_tokens(   final_tokens = my_final_tokens,   processed_tokens = my_processed_tokens ) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_content_type_prompt.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Default System Prompt for Content Type — get_content_type_prompt","title":"Get Default System Prompt for Content Type — get_content_type_prompt","text":"Returns default system prompt given content type.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_content_type_prompt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Default System Prompt for Content Type — get_content_type_prompt","text":"","code":"get_content_type_prompt(content_type)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_content_type_prompt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Default System Prompt for Content Type — get_content_type_prompt","text":"content_type Type content generate.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_content_type_prompt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Default System Prompt for Content Type — get_content_type_prompt","text":"Character string system prompt.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_content_type_user_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Default User Prompt Template for Content Type — get_content_type_user_template","title":"Get Default User Prompt Template for Content Type — get_content_type_user_template","text":"Returns default user prompt template given content type.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_content_type_user_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Default User Prompt Template for Content Type — get_content_type_user_template","text":"","code":"get_content_type_user_template(content_type)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_content_type_user_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Default User Prompt Template for Content Type — get_content_type_user_template","text":"content_type Type content generate.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_content_type_user_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Default User Prompt Template for Content Type — get_content_type_user_template","text":"Character string user prompt template containing {terms} placeholder.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_dfm_setup_instructions.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate DFM Setup Instructions Text — get_dfm_setup_instructions","title":"Generate DFM Setup Instructions Text — get_dfm_setup_instructions","text":"Generates standardized text instructions creating DFM. Used console output verbatim text displays.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_dfm_setup_instructions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate DFM Setup Instructions Text — get_dfm_setup_instructions","text":"","code":"get_dfm_setup_instructions(feature_name = \"this feature\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_dfm_setup_instructions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate DFM Setup Instructions Text — get_dfm_setup_instructions","text":"feature_name Name feature requiring DFM (default: \"feature\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_dfm_setup_instructions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate DFM Setup Instructions Text — get_dfm_setup_instructions","text":"Character vector instruction lines","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_dfm_setup_instructions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate DFM Setup Instructions Text — get_dfm_setup_instructions","text":"","code":"if (FALSE) { # \\dontrun{ output$instructions <- renderPrint({   cat(get_dfm_setup_instructions(\"keyword extraction\"), sep = \"\\n\") }) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_dt_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Standard DataTable Options — get_dt_options","title":"Get Standard DataTable Options — get_dt_options","text":"Returns standardized DT::datatable options consistent table formatting across TextAnalysisR application.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_dt_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Standard DataTable Options — get_dt_options","text":"","code":"get_dt_options(scroll_y = \"400px\", page_length = 25, show_buttons = TRUE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_dt_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Standard DataTable Options — get_dt_options","text":"scroll_y Vertical scroll height (default: \"400px\") page_length Number rows per page (default: 25) show_buttons Whether show export buttons (default: TRUE","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_dt_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Standard DataTable Options — get_dt_options","text":"list DT options","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_dt_options.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Standard DataTable Options — get_dt_options","text":"","code":"if (FALSE) { # \\dontrun{ DT::datatable(my_data, options = get_dt_options()) DT::datatable(my_data, options = get_dt_options(scroll_y = \"300px\")) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_feature_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Feature Status — get_feature_status","title":"Get Feature Status — get_feature_status","text":"Returns availability status optional features.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_feature_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Feature Status — get_feature_status","text":"","code":"get_feature_status()"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_feature_status.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Feature Status — get_feature_status","text":"Named list feature availability","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_feature_status.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Feature Status — get_feature_status","text":"","code":"# \\donttest{ status <- get_feature_status() print(status) #> $python #> [1] FALSE #>  #> $ollama #> [1] TRUE #>  #> $langgraph #> [1] FALSE #>  #> $pdf_tables #> [1] FALSE #>  #> $embeddings #> [1] TRUE #>  #> $sentiment_deep #> [1] TRUE #>  #> $web #> [1] FALSE #>  #> $local #> [1] TRUE #>  # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_plotly_hover_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Standard Plotly Hover Label Configuration — get_plotly_hover_config","title":"Get Standard Plotly Hover Label Configuration — get_plotly_hover_config","text":"Returns standardized hover label styling plotly plots.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_plotly_hover_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Standard Plotly Hover Label Configuration — get_plotly_hover_config","text":"","code":"get_plotly_hover_config(bgcolor = \"#ffffff\", fontcolor = \"#0c1f4a\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_plotly_hover_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Standard Plotly Hover Label Configuration — get_plotly_hover_config","text":"bgcolor Background color (default: \"#ffffff\") fontcolor Font color (default: \"#0c1f4a\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_plotly_hover_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Standard Plotly Hover Label Configuration — get_plotly_hover_config","text":"list hover label configuration parameters","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_plotly_hover_config.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Standard Plotly Hover Label Configuration — get_plotly_hover_config","text":"","code":"if (FALSE) { # \\dontrun{ hover_config <- get_plotly_hover_config() plot_ly(..., hoverlabel = hover_config) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_recommended_ollama_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Recommended Ollama Model — get_recommended_ollama_model","title":"Get Recommended Ollama Model — get_recommended_ollama_model","text":"Returns recommended Ollama model based available.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_recommended_ollama_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Recommended Ollama Model — get_recommended_ollama_model","text":"","code":"get_recommended_ollama_model(   preferred_models = c(\"phi3:mini\", \"llama3.1:8b\", \"mistral:7b\", \"tinyllama\"),   verbose = FALSE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_recommended_ollama_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Recommended Ollama Model — get_recommended_ollama_model","text":"preferred_models Character vector preferred models priority order. verbose Logical, TRUE, prints status messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_recommended_ollama_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Recommended Ollama Model — get_recommended_ollama_model","text":"Character string recommended model, NULL none available.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_recommended_ollama_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Recommended Ollama Model — get_recommended_ollama_model","text":"","code":"if (FALSE) { # \\dontrun{ model <- get_recommended_ollama_model() print(model) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_sentiment_color.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Sentiment Color Gradient — get_sentiment_color","title":"Generate Sentiment Color Gradient — get_sentiment_color","text":"Generates color based sentiment score using gradient red (negative) gray (neutral) green (positive).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_sentiment_color.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Sentiment Color Gradient — get_sentiment_color","text":"","code":"get_sentiment_color(score)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_sentiment_color.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Sentiment Color Gradient — get_sentiment_color","text":"score Numeric sentiment score (typically -1 1)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_sentiment_color.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Sentiment Color Gradient — get_sentiment_color","text":"Hex color string","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_sentiment_color.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Sentiment Color Gradient — get_sentiment_color","text":"","code":"get_sentiment_color(-0.8)  # Red #> [1] \"#A35A44\" get_sentiment_color(0)     # Gray #> [1] \"#4BB543\" get_sentiment_color(0.8)   # Green #> [1] \"#1CB875\""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_sentiment_colors.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Sentiment Color Palette — get_sentiment_colors","title":"Get Sentiment Color Palette — get_sentiment_colors","text":"Returns standardized color mapping sentiment analysis.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_sentiment_colors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Sentiment Color Palette — get_sentiment_colors","text":"","code":"get_sentiment_colors()"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_sentiment_colors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Sentiment Color Palette — get_sentiment_colors","text":"Named vector colors","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_spacy_nlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Get spaCy NLP Processor — get_spacy_nlp","title":"Get spaCy NLP Processor — get_spacy_nlp","text":"Internal function get initialized spaCy processor.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_spacy_nlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get spaCy NLP Processor — get_spacy_nlp","text":"","code":"get_spacy_nlp(model = \"en_core_web_sm\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_spacy_nlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get spaCy NLP Processor — get_spacy_nlp","text":"model Model use initialized","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_spacy_nlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get spaCy NLP Processor — get_spacy_nlp","text":"SpacyNLP Python object","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_prevalence.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Topic Prevalence (Gamma) from STM Model — get_topic_prevalence","title":"Get Topic Prevalence (Gamma) from STM Model — get_topic_prevalence","text":"Extracts topic prevalence values (gamma/theta) fitted STM model, returning mean prevalence topic data frame.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_prevalence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Topic Prevalence (Gamma) from STM Model — get_topic_prevalence","text":"","code":"get_topic_prevalence(stm_model, category = NULL, include_theta = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_prevalence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Topic Prevalence (Gamma) from STM Model — get_topic_prevalence","text":"stm_model fitted STM model object stm::stm(). category Optional character string add category column. include_theta Logical, TRUE includes document-topic matrix (default: FALSE).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_prevalence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Topic Prevalence (Gamma) from STM Model — get_topic_prevalence","text":"data frame columns: topic Topic number gamma Mean topic prevalence across documents category Category label (provided)","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_prevalence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Topic Prevalence (Gamma) from STM Model — get_topic_prevalence","text":"","code":"if (FALSE) { # \\dontrun{ # Fit STM model topic_model <- stm::stm(documents, vocab, K = 10)  # Get topic prevalence prevalence <- get_topic_prevalence(topic_model)  # With category label prevalence_sld <- get_topic_prevalence(topic_model, category = \"SLD\") } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_terms.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Top Terms for Each Topic — get_topic_terms","title":"Select Top Terms for Each Topic — get_topic_terms","text":"function selects top terms topic based word probability distribution (beta).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_terms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Top Terms for Each Topic — get_topic_terms","text":"","code":"get_topic_terms(stm_model, top_term_n = 10, verbose = TRUE, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_terms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Top Terms for Each Topic — get_topic_terms","text":"stm_model STM model object. top_term_n number top terms display topic (default: 10). verbose Logical, TRUE, prints progress messages. ... arguments passed tidytext::tidy.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_terms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Top Terms for Each Topic — get_topic_terms","text":"data frame containing top terms topic.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_terms.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select Top Terms for Each Topic — get_topic_terms","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_cols(     mydata,     listed_vars = c(\"title\", \"keyword\", \"abstract\")   )    tokens <- TextAnalysisR::prep_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    stm_15 <- TextAnalysisR::create_stm_model(   dfm_object,   topic_n = 15,   max.em.its = 75,   categorical_var = \"reference_type\",   continuous_var = \"year\",   verbose = TRUE   )    out <- quanteda::convert(dfm_object, to = \"stm\")  stm_15 <- stm::stm(   data = out$meta,   documents = out$documents,   vocab = out$vocab,   max.em.its = 75,   init.type = \"Spectral\",   K = 15,   prevalence = ~ reference_type + s(year),   verbose = TRUE)  top_topic_terms <- TextAnalysisR::get_topic_terms(   stm_model = stm_15,   top_term_n = 10,   verbose = TRUE   ) print(top_topic_terms) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_texts.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Topic Terms to Text Strings — get_topic_texts","title":"Convert Topic Terms to Text Strings — get_topic_texts","text":"Concatenates top terms topic text strings suitable embedding generation. Useful creating topic representations semantic similarity analysis.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_texts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Topic Terms to Text Strings — get_topic_texts","text":"","code":"get_topic_texts(   top_terms_df,   topic_var = \"topic\",   term_var = \"term\",   weight_var = NULL,   sep = \" \",   top_n = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_texts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Topic Terms to Text Strings — get_topic_texts","text":"top_terms_df data frame containing top terms topics, typically output get_topic_terms. topic_var Name column containing topic identifiers (default: \"topic\"). term_var Name column containing terms (default: \"term\"). weight_var Optional name column term weights (e.g., \"beta\"). provided, terms ordered weight concatenation. sep Separator terms (default: \" \"). top_n Optional number top terms include per topic (default: NULL, uses ).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_texts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Topic Terms to Text Strings — get_topic_texts","text":"character vector topic text strings, one per topic, ordered topic number.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/get_topic_texts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert Topic Terms to Text Strings — get_topic_texts","text":"","code":"if (FALSE) { # \\dontrun{ # Get topic terms from STM model top_terms <- TextAnalysisR::get_topic_terms(stm_model, top_term_n = 10)  # Convert to text strings for embedding topic_texts <- get_topic_texts(top_terms)  # Generate embeddings topic_embeddings <- TextAnalysisR::generate_embeddings(topic_texts) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/identify_topic_trends.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify Topic Trends — identify_topic_trends","title":"Identify Topic Trends — identify_topic_trends","text":"Identifies trending topics temporal results","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/identify_topic_trends.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify Topic Trends — identify_topic_trends","text":"","code":"identify_topic_trends(temporal_results, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/identify_topic_trends.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identify Topic Trends — identify_topic_trends","text":"temporal_results Temporal analysis results ... Additional parameters","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/identify_topic_trends.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Identify Topic Trends — identify_topic_trends","text":"List containing identified trends","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/import_files.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Files — import_files","title":"Process Files — import_files","text":"function processes different types files text input based dataset choice.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/import_files.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Files — import_files","text":"","code":"import_files(dataset_choice, file_info = NULL, text_input = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/import_files.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Files — import_files","text":"dataset_choice character string indicating dataset choice. file_info data frame containing file information column named 'filepath' (default: NULL). text_input character string containing text input (default: NULL).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/import_files.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Files — import_files","text":"data frame containing processed data.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/import_files.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process Files — import_files","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::SpecialEduTech   mydata <- TextAnalysisR::import_files(dataset_choice = \"Upload an Example Dataset\")   head(mydata)    file_info <- data.frame(filepath = \"inst/extdata/SpecialEduTech.xlsx\")   mydata <- TextAnalysisR::import_files(dataset_choice = \"Upload Your File\",                                           file_info = file_info)   head(mydata)     text_input <- paste(\"Virtual manipulatives for algebra instruction\",                       \"manipulatives mathematics learning disability\",                       \"This study examined virtual manipulatives effects on\",                       \"students with learning disabilities\")   mydata <- TextAnalysisR::import_files(dataset_choice = \"Copy and Paste Text\",                                           text_input = text_input)   head(mydata) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_langgraph.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize LangGraph for Current Session — init_langgraph","title":"Initialize LangGraph for Current Session — init_langgraph","text":"Initializes LangGraph/LangChain/Ollama modules current R session. Use LangGraph workflows. PDF/embeddings load automatically.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_langgraph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize LangGraph for Current Session — init_langgraph","text":"","code":"init_langgraph(envname = \"textanalysisr-env\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_langgraph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize LangGraph for Current Session — init_langgraph","text":"envname Character string name virtual environment (default: \"textanalysisr-env\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_langgraph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialize LangGraph for Current Session — init_langgraph","text":"Invisible list LangGraph/LangChain/Ollama modules","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_langgraph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initialize LangGraph for Current Session — init_langgraph","text":"","code":"if (FALSE) { # \\dontrun{ lg <- init_langgraph() } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_spacy_nlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize spaCy NLP Module — init_spacy_nlp","title":"Initialize spaCy NLP Module — init_spacy_nlp","text":"Loads Python spaCy module initializes NLP processor. function called automatically spaCy functions.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_spacy_nlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize spaCy NLP Module — init_spacy_nlp","text":"","code":"init_spacy_nlp(model = \"en_core_web_sm\", force = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_spacy_nlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize spaCy NLP Module — init_spacy_nlp","text":"model Character; spaCy model use. Options: \"en_core_web_sm\" - Small model, fast, word vectors (default) \"en_core_web_md\" - Medium model, includes word vectors \"en_core_web_lg\" - Large model, better accuracy, word vectors \"en_core_web_trf\" - Transformer model, best accuracy force Logical; force reinitialization even already loaded","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_spacy_nlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialize spaCy NLP Module — init_spacy_nlp","text":"Invisible NULL. spaCy processor stored internally.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_spacy_nlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Initialize spaCy NLP Module — init_spacy_nlp","text":"Requires Python spaCy installed. Install :","code":"pip install spacy python -m spacy download en_core_web_sm"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/init_spacy_nlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initialize spaCy NLP Module — init_spacy_nlp","text":"","code":"if (FALSE) { # \\dontrun{ init_spacy_nlp(\"en_core_web_sm\") } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/lexical_diversity_analysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Lexical Diversity Analysis — lexical_diversity_analysis","title":"Lexical Diversity Analysis — lexical_diversity_analysis","text":"Calculates lexical diversity metrics measure vocabulary richness. MTLD MATTR stable text-length independent.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/lexical_diversity_analysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lexical Diversity Analysis — lexical_diversity_analysis","text":"","code":"lexical_diversity_analysis(x, measures = \"all\", texts = NULL, cache_key = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/lexical_diversity_analysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lexical Diversity Analysis — lexical_diversity_analysis","text":"x tokens object (preferred) document-feature matrix quanteda. accurate MTLD calculation, pass tokens object provide texts parameter. DFM input loses token order, affects MTLD accuracy (McCarthy & Jarvis, 2010). measures Character vector measures calculate. Options: \"\", \"MTLD\" (recommended), \"MATTR\" (recommended), \"MSTTR\", \"TTR\", \"CTTR\", \"Maas\", \"K\", \"D\" texts Optional character vector original texts. Required accurate MTLD passing DFM (since DFM loses token order). Also used average sentence length. cache_key Optional character string caching expensive computations. provided, results cached using key retrieved subsequent calls key. Use clear_lexdiv_cache() clear cache.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/lexical_diversity_analysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Lexical Diversity Analysis — lexical_diversity_analysis","text":"list lexical_diversity (data frame) summary_stats","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/lexical_diversity_analysis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Lexical Diversity Analysis — lexical_diversity_analysis","text":"","code":"if (FALSE) { # \\dontrun{ data(SpecialEduTech) texts <- SpecialEduTech$abstract[1:10] corp <- quanteda::corpus(texts) toks <- quanteda::tokens(corp) # Preferred: pass tokens object for accurate MTLD lex_div <- lexical_diversity_analysis(toks, texts = texts) # With caching for repeated analysis cache_key <- digest::digest(texts) lex_div <- lexical_diversity_analysis(toks, texts = texts, cache_key = cache_key) # Alternative: pass DFM with texts for MTLD accuracy dfm_obj <- quanteda::dfm(toks) lex_div <- lexical_diversity_analysis(dfm_obj, texts = texts) print(lex_div) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/lexical_frequency_analysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Lexical Frequency Analysis — lexical_frequency_analysis","title":"Lexical Frequency Analysis — lexical_frequency_analysis","text":"Wrapper function plot_word_frequency lexical analysis.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/lexical_frequency_analysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lexical Frequency Analysis — lexical_frequency_analysis","text":"","code":"lexical_frequency_analysis(...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/lexical_frequency_analysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lexical Frequency Analysis — lexical_frequency_analysis","text":"... Arguments passed plot_word_frequency","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/list_ollama_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Ollama Models — list_ollama_models","title":"List Available Ollama Models — list_ollama_models","text":"Lists models currently installed Ollama.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/list_ollama_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Ollama Models — list_ollama_models","text":"","code":"list_ollama_models(verbose = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/list_ollama_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Ollama Models — list_ollama_models","text":"verbose Logical, TRUE, prints status messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/list_ollama_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Available Ollama Models — list_ollama_models","text":"Character vector model names, NULL Ollama unavailable.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/list_ollama_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Available Ollama Models — list_ollama_models","text":"","code":"if (FALSE) { # \\dontrun{ models <- list_ollama_models() print(models) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/log_security_event.html","id":null,"dir":"Reference","previous_headings":"","what":"Log Security Event — log_security_event","title":"Log Security Event — log_security_event","text":"Log Security Event","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/log_security_event.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log Security Event — log_security_event","text":"","code":"log_security_event(event_type, details, session_info, level = \"INFO\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/log_security_event.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log Security Event — log_security_event","text":"event_type Type security event details Additional details event session_info Shiny session object level Log level (INFO, WARNING, ERROR)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_cluster_terms.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Cluster Top Terms — plot_cluster_terms","title":"Plot Cluster Top Terms — plot_cluster_terms","text":"Creates horizontal bar plot showing top terms cluster document group.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_cluster_terms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Cluster Top Terms — plot_cluster_terms","text":"","code":"plot_cluster_terms(   terms,   cluster_id = NULL,   title = NULL,   n_terms = 10,   color = \"#337ab7\",   height = 500,   width = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_cluster_terms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Cluster Top Terms — plot_cluster_terms","text":"terms Named numeric vector term frequencies, data frame 'term' 'frequency' columns cluster_id Cluster identifier title (default: NULL) title Custom title (default: NULL, auto-generated cluster_id) n_terms Number top terms display (default: 10) color Bar color (default: \"#337ab7\") height Plot height pixels (default: 500) width Plot width pixels (default: NULL auto)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_cluster_terms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Cluster Top Terms — plot_cluster_terms","text":"plotly object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_cross_category_heatmap.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Cross-Category Similarity Comparison — plot_cross_category_heatmap","title":"Plot Cross-Category Similarity Comparison — plot_cross_category_heatmap","text":"Creates faceted ggplot heatmap cross-category document similarity comparison. Accepts either pre-built long-format data frame extracts similarity matrix.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_cross_category_heatmap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Cross-Category Similarity Comparison — plot_cross_category_heatmap","text":"","code":"plot_cross_category_heatmap(   similarity_data,   docs_data = NULL,   row_var = \"ld_doc_name\",   col_var = \"other_doc_name\",   value_var = \"cosine_similarity\",   category_var = \"other_category\",   row_category = NULL,   col_categories = NULL,   row_display_var = NULL,   col_display_var = NULL,   method_name = \"Cosine\",   title = NULL,   show_values = TRUE,   row_label = \"Documents\",   label_max_chars = 25,   order_by_numeric = TRUE,   height = 600,   width = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_cross_category_heatmap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Cross-Category Similarity Comparison — plot_cross_category_heatmap","text":"similarity_data Either similarity matrix (square numeric matrix) data frame long format columns row labels, column labels, similarity values, category. docs_data Data frame document metadata (required similarity_data matrix) row_var Column name row document labels (default: \"ld_doc_name\") col_var Column name column document labels (default: \"other_doc_name\") value_var Column name similarity values (default: \"cosine_similarity\") category_var Column name category long-format data docs_data (default: \"other_category\") row_category Category row documents (used matrix input) col_categories Categories column documents (used matrix input) row_display_var Column name row display labels tooltip (default: NULL, uses row_var) col_display_var Column name column display labels tooltip (default: NULL, uses col_var) method_name Similarity method name legend (default: \"Cosine\") title Plot title (default: NULL) show_values Logical; show similarity values text tiles (default: TRUE) row_label Label y-axis (default: \"Documents\") label_max_chars Maximum characters axis labels truncation (default: 25) order_by_numeric Logical; order numeric ID extracted labels (default: TRUE) height Plot height (default: 600) width Plot width (default: NULL)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_cross_category_heatmap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Cross-Category Similarity Comparison — plot_cross_category_heatmap","text":"ggplot object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_cross_category_heatmap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Cross-Category Similarity Comparison — plot_cross_category_heatmap","text":"","code":"if (FALSE) { # \\dontrun{ # With pre-built long-format data plot_cross_category_heatmap(   similarity_data = ld_similarities,   row_var = \"ld_doc_name\",   col_var = \"other_doc_name\",   value_var = \"cosine_similarity\",   category_var = \"other_category\",   row_label = \"SLD Documents\" ) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_document_sentiment_trajectory.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Document Sentiment Trajectory — plot_document_sentiment_trajectory","title":"Plot Document Sentiment Trajectory — plot_document_sentiment_trajectory","text":"Creates line chart showing sentiment scores across documents color gradient.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_document_sentiment_trajectory.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Document Sentiment Trajectory — plot_document_sentiment_trajectory","text":"","code":"plot_document_sentiment_trajectory(   sentiment_data,   top_n = NULL,   doc_ids = NULL,   text_preview = NULL,   title = \"Document Sentiment Scores\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_document_sentiment_trajectory.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Document Sentiment Trajectory — plot_document_sentiment_trajectory","text":"sentiment_data Data frame analyze_sentiment() sentiment_score column top_n Number documents display (default: NULL ) doc_ids Optional vector custom document IDs display (default: NULL) text_preview Optional vector text snippets tooltips (default: NULL) title Plot title (default: \"Document Sentiment Scores\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_document_sentiment_trajectory.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Document Sentiment Trajectory — plot_document_sentiment_trajectory","text":"plotly line chart color gradient","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_emotion_radar.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Emotion Radar Chart — plot_emotion_radar","title":"Plot Emotion Radar Chart — plot_emotion_radar","text":"Creates polar/radar chart NRC emotion analysis optional grouping.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_emotion_radar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Emotion Radar Chart — plot_emotion_radar","text":"","code":"plot_emotion_radar(   emotion_data,   group_var = NULL,   normalize = FALSE,   title = \"Emotion Analysis\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_emotion_radar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Emotion Radar Chart — plot_emotion_radar","text":"emotion_data Data frame emotion scores (columns: emotion, total_score) group_var Optional grouping variable column name overlaid radars (default: NULL) normalize Logical, whether normalize scores 0-100 scale (default: FALSE) title Plot title (default: \"Emotion Analysis\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_emotion_radar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Emotion Radar Chart — plot_emotion_radar","text":"plotly polar chart","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_entity_frequencies.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Named Entity Frequencies — plot_entity_frequencies","title":"Plot Named Entity Frequencies — plot_entity_frequencies","text":"Creates bar plot showing frequency distribution named entity types.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_entity_frequencies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Named Entity Frequencies — plot_entity_frequencies","text":"","code":"plot_entity_frequencies(   entity_data,   top_n = 20,   title = \"Named Entity Type Frequency\",   color = \"#10B981\",   height = 500,   width = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_entity_frequencies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Named Entity Frequencies — plot_entity_frequencies","text":"entity_data Data frame containing entity data columns: entity: Named entity type (e.g., \"PERSON\", \"ORG\", \"GPE\") n: (optional) Pre-computed frequency count n present, frequencies computed data. top_n Number top entity types display (default: 20) title Plot title (default: \"Named Entity Type Frequency\") color Bar color (default: \"#10B981\") height Plot height pixels (default: 500) width Plot width pixels (default: NULL auto)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_entity_frequencies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Named Entity Frequencies — plot_entity_frequencies","text":"plotly object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_entity_frequencies.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Named Entity Frequencies — plot_entity_frequencies","text":"","code":"if (interactive()) {   entity_df <- data.frame(     entity = c(\"PERSON\", \"ORG\", \"GPE\", \"DATE\", \"MONEY\"),     n = c(300, 250, 200, 150, 100)   )   plot_entity_frequencies(entity_df) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Error Plot for Plotly — plot_error","title":"Create Error Plot for Plotly — plot_error","text":"Creates plotly error/status plot message","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Error Plot for Plotly — plot_error","text":"","code":"plot_error(message, color = \"#ef4444\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Error Plot for Plotly — plot_error","text":"message message display color Color text (default: \"#ef4444\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_error.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Error Plot for Plotly — plot_error","text":"plotly plot object displaying message","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_keyness_keywords.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Statistical Keyness — plot_keyness_keywords","title":"Plot Statistical Keyness — plot_keyness_keywords","text":"Creates horizontal bar plot distinctive keywords keyness score.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_keyness_keywords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Statistical Keyness — plot_keyness_keywords","text":"","code":"plot_keyness_keywords(keyness_data, title = NULL, group_label = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_keyness_keywords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Statistical Keyness — plot_keyness_keywords","text":"keyness_data Data frame extract_keywords_keyness() title Plot title (default: \"Top Keywords Keyness (G-squared)\") group_label Optional label target group (default: NULL)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_keyness_keywords.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Statistical Keyness — plot_keyness_keywords","text":"plotly bar chart","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_keyword_comparison.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Keyword Comparison (TF-IDF vs Frequency) — plot_keyword_comparison","title":"Plot Keyword Comparison (TF-IDF vs Frequency) — plot_keyword_comparison","text":"Creates grouped bar plot comparing TF-IDF scores term frequencies.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_keyword_comparison.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Keyword Comparison (TF-IDF vs Frequency) — plot_keyword_comparison","text":"","code":"plot_keyword_comparison(   tfidf_data,   top_n = 10,   title = NULL,   normalized = FALSE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_keyword_comparison.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Keyword Comparison (TF-IDF vs Frequency) — plot_keyword_comparison","text":"tfidf_data Data frame extract_keywords_tfidf() top_n Number keywords display (default: 10) title Plot title (default: auto-generated) normalized Logical, whether TF-IDF scores normalized (default: FALSE)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_keyword_comparison.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Keyword Comparison (TF-IDF vs Frequency) — plot_keyword_comparison","text":"plotly grouped bar chart","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_lexical_diversity_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Lexical Diversity Distribution — plot_lexical_diversity_distribution","title":"Plot Lexical Diversity Distribution — plot_lexical_diversity_distribution","text":"Creates boxplot showing distribution lexical diversity metric.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_lexical_diversity_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Lexical Diversity Distribution — plot_lexical_diversity_distribution","text":"","code":"plot_lexical_diversity_distribution(lexdiv_data, metric, title = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_lexical_diversity_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Lexical Diversity Distribution — plot_lexical_diversity_distribution","text":"lexdiv_data Data frame lexical_diversity_analysis() metric Metric plot. Recommended: \"MTLD\" \"MATTR\" (text-length independent) title Plot title (default: auto-generated)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_lexical_diversity_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Lexical Diversity Distribution — plot_lexical_diversity_distribution","text":"plotly boxplot","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_lexical_diversity_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Lexical Diversity Distribution — plot_lexical_diversity_distribution","text":"","code":"if (FALSE) { # \\dontrun{ data(SpecialEduTech) texts <- SpecialEduTech$abstract[1:10] corp <- quanteda::corpus(texts) toks <- quanteda::tokens(corp) dfm_obj <- quanteda::dfm(toks) result <- lexical_diversity_analysis(dfm_obj) plot <- plot_lexical_diversity_distribution(result$lexical_diversity, \"MTLD\") print(plot) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_model_comparison.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Topic Model Comparison Scatter — plot_model_comparison","title":"Plot Topic Model Comparison Scatter — plot_model_comparison","text":"Creates scatter plot comparing topic model metrics across K values. Automatically selects best available metric combination.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_model_comparison.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Topic Model Comparison Scatter — plot_model_comparison","text":"","code":"plot_model_comparison(   search_results,   title = \"Model Comparison\",   height = 600,   width = 800 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_model_comparison.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Topic Model Comparison Scatter — plot_model_comparison","text":"search_results Results stm::searchK find_optimal_k() title Plot title (default: \"Model Comparison\") height Plot height pixels (default: 600) width Plot width pixels (default: 800)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_model_comparison.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Topic Model Comparison Scatter — plot_model_comparison","text":"plotly scatter plot","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_mwe_frequency.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Multi-Word Expression Frequency — plot_mwe_frequency","title":"Plot Multi-Word Expression Frequency — plot_mwe_frequency","text":"Creates bar plot showing multi-word expression frequencies optional source-based coloring distinguish detected manually added expressions.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_mwe_frequency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Multi-Word Expression Frequency — plot_mwe_frequency","text":"","code":"plot_mwe_frequency(   mwe_data,   title = \"Multi-Word Expression Frequency\",   color_by_source = TRUE,   primary_color = \"#10B981\",   secondary_color = \"#A855F7\",   height = 500,   width = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_mwe_frequency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Multi-Word Expression Frequency — plot_mwe_frequency","text":"mwe_data Data frame containing MWE data columns: feature: multi-word expression text frequency: Frequency count rank: (optional) Rank expression docfreq: (optional) Document frequency source: (optional) Source category (e.g., \"Top 20\", \"Manual\") title Plot title (default: \"Multi-Word Expression Frequency\") color_by_source Whether color bars source column (default: TRUE) primary_color Color primary/top expressions (default: \"#10B981\") secondary_color Color secondary/manual expressions (default: \"#A855F7\") height Plot height pixels (default: 500) width Plot width pixels (default: NULL auto)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_mwe_frequency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Multi-Word Expression Frequency — plot_mwe_frequency","text":"plotly object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_ngram_frequency.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot N-gram Frequency — plot_ngram_frequency","title":"Plot N-gram Frequency — plot_ngram_frequency","text":"Creates bar plot showing n-gram frequencies optional highlighting selected n-grams. Supports detected n-grams selected multi-word expressions.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_ngram_frequency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot N-gram Frequency — plot_ngram_frequency","text":"","code":"plot_ngram_frequency(   ngram_data,   top_n = 30,   selected = NULL,   title = \"N-gram Frequency\",   highlight_color = \"#10B981\",   default_color = \"#6B7280\",   height = 500,   width = NULL,   show_stats = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_ngram_frequency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot N-gram Frequency — plot_ngram_frequency","text":"ngram_data Data frame containing n-gram data columns: collocation: n-gram text count: Frequency count lambda: (optional) Lambda statistic z: (optional) Z-score statistic top_n Number top n-grams display (default: 30) selected Character vector selected n-grams highlight (default: NULL) title Plot title (default: \"N-gram Frequency\") highlight_color Color highlighted bars (default: \"#10B981\") default_color Color non-highlighted bars (default: \"#6B7280\") height Plot height pixels (default: 500) width Plot width pixels (default: NULL auto) show_stats Whether show lambda z-score hover (default: TRUE)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_ngram_frequency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot N-gram Frequency — plot_ngram_frequency","text":"plotly object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_ngram_frequency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot N-gram Frequency — plot_ngram_frequency","text":"","code":"if (interactive()) {   ngram_df <- data.frame(     collocation = c(\"machine learning\", \"deep learning\", \"neural network\"),     count = c(150, 120, 90),     lambda = c(5.2, 4.8, 4.1),     z = c(12.3, 10.5, 9.2)   )   plot_ngram_frequency(ngram_df, selected = c(\"machine learning\")) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_pos_frequencies.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Part-of-Speech Tag Frequencies — plot_pos_frequencies","title":"Plot Part-of-Speech Tag Frequencies — plot_pos_frequencies","text":"Creates bar plot showing frequency distribution part--speech tags.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_pos_frequencies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Part-of-Speech Tag Frequencies — plot_pos_frequencies","text":"","code":"plot_pos_frequencies(   pos_data,   top_n = 20,   title = \"Part-of-Speech Tag Frequency\",   color = \"#337ab7\",   height = 500,   width = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_pos_frequencies.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Part-of-Speech Tag Frequencies — plot_pos_frequencies","text":"pos_data Data frame containing POS data columns: pos: Part--speech tag n: (optional) Pre-computed frequency count n present, frequencies computed data. top_n Number top POS tags display (default: 20) title Plot title (default: \"Part--Speech Tag Frequency\") color Bar color (default: \"#337ab7\") height Plot height pixels (default: 500) width Plot width pixels (default: NULL auto)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_pos_frequencies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Part-of-Speech Tag Frequencies — plot_pos_frequencies","text":"plotly object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_pos_frequencies.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Part-of-Speech Tag Frequencies — plot_pos_frequencies","text":"","code":"if (interactive()) {   pos_df <- data.frame(     pos = c(\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PRON\"),     n = c(500, 400, 250, 150, 100)   )   plot_pos_frequencies(pos_df) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_quality_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Topic Model Quality Metrics — plot_quality_metrics","title":"Plot Topic Model Quality Metrics — plot_quality_metrics","text":"Creates faceted plot showing diagnostic metrics across different K values stm::searchK results.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_quality_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Topic Model Quality Metrics — plot_quality_metrics","text":"","code":"plot_quality_metrics(   search_results,   title = \"Diagnostic Plots\",   height = 600,   width = 800 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_quality_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Topic Model Quality Metrics — plot_quality_metrics","text":"search_results Results stm::searchK find_optimal_k() title Plot title (default: \"Diagnostic Plots\") height Plot height pixels (default: 600) width Plot width pixels (default: 800)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_quality_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Topic Model Quality Metrics — plot_quality_metrics","text":"plotly object faceted diagnostic plots","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_readability_by_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Readability by Group — plot_readability_by_group","title":"Plot Readability by Group — plot_readability_by_group","text":"Creates grouped boxplots comparing readability across categories.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_readability_by_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Readability by Group — plot_readability_by_group","text":"","code":"plot_readability_by_group(readability_data, metric, group_var, title = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_readability_by_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Readability by Group — plot_readability_by_group","text":"readability_data Data frame calculate_text_readability() metric Metric plot group_var Name grouping variable column title Plot title (default: auto-generated)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_readability_by_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Readability by Group — plot_readability_by_group","text":"plotly boxplot","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_readability_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Readability Distribution — plot_readability_distribution","title":"Plot Readability Distribution — plot_readability_distribution","text":"Creates boxplot showing overall distribution readability metric.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_readability_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Readability Distribution — plot_readability_distribution","text":"","code":"plot_readability_distribution(readability_data, metric, title = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_readability_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Readability Distribution — plot_readability_distribution","text":"readability_data Data frame calculate_text_readability() metric Metric plot (e.g., \"flesch\", \"flesch_kincaid\", \"gunning_fog\") title Plot title (default: auto-generated)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_readability_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Readability Distribution — plot_readability_distribution","text":"plotly boxplot","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_readability_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Readability Distribution — plot_readability_distribution","text":"","code":"if (FALSE) { # \\dontrun{ texts <- c(\"Simple text.\", \"More complex sentence structure here.\") readability <- calculate_text_readability(texts) plot <- plot_readability_distribution(readability, \"flesch\") print(plot) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_semantic_viz.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Semantic Analysis Visualization — plot_semantic_viz","title":"Plot Semantic Analysis Visualization — plot_semantic_viz","text":"Creates interactive visualizations semantic analysis results including similarity heatmaps, dimensionality reduction plots, clustering visualizations.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_semantic_viz.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Semantic Analysis Visualization — plot_semantic_viz","text":"","code":"plot_semantic_viz(   analysis_result = NULL,   plot_type = \"similarity\",   data_labels = NULL,   color_by = NULL,   height = 600,   width = 800,   title = NULL,   coords = NULL,   clusters = NULL,   hover_text = NULL,   hover_config = NULL,   cluster_colors = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_semantic_viz.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Semantic Analysis Visualization — plot_semantic_viz","text":"analysis_result list containing semantic analysis results functions like semantic_similarity_analysis(), semantic_document_clustering(), reduce_dimensions(). plot_type Type visualization: \"similarity\" heatmap, \"dimensionality_reduction\" scatter plot, \"clustering\" cluster visualization (default: \"similarity\"). data_labels Optional character vector labels data points (default: NULL). color_by Optional variable color points scatter plots (default: NULL). height height resulting Plotly plot, pixels (default: 600). width width resulting Plotly plot, pixels (default: 800). title Optional custom title plot (default: NULL). coords Optional pre-computed coordinates dimensionality reduction plots (default: NULL). clusters Optional cluster assignments vector (default: NULL). hover_text Optional custom hover text points (default: NULL). hover_config Optional hover configuration list (default: NULL). cluster_colors Optional color palette clusters (default: NULL).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_semantic_viz.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Semantic Analysis Visualization — plot_semantic_viz","text":"plotly object showing specified visualization.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_semantic_viz.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Semantic Analysis Visualization — plot_semantic_viz","text":"","code":"if (interactive()) {   texts <- c(\"machine learning\", \"deep learning\", \"artificial intelligence\")   result <- semantic_similarity_analysis(texts)   plot <- plot_semantic_viz(result, plot_type = \"similarity\")   print(plot) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_boxplot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Sentiment Box Plot by Category — plot_sentiment_boxplot","title":"Plot Sentiment Box Plot by Category — plot_sentiment_boxplot","text":"Creates box plot showing sentiment score distribution category.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_boxplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Sentiment Box Plot by Category — plot_sentiment_boxplot","text":"","code":"plot_sentiment_boxplot(   sentiment_data,   category_var = \"category_var\",   title = \"Sentiment Score Distribution\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_boxplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Sentiment Box Plot by Category — plot_sentiment_boxplot","text":"sentiment_data Data frame analyze_sentiment() containing sentiment_score category columns category_var Name category variable column (default: \"category_var\") title Plot title (default: \"Sentiment Score Distribution\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_boxplot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Sentiment Box Plot by Category — plot_sentiment_boxplot","text":"plotly box plot","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_by_category.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Sentiment by Category — plot_sentiment_by_category","title":"Plot Sentiment by Category — plot_sentiment_by_category","text":"Creates grouped stacked bar plot showing sentiment distribution across categories.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_by_category.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Sentiment by Category — plot_sentiment_by_category","text":"","code":"plot_sentiment_by_category(   sentiment_data,   category_var,   plot_type = \"bar\",   title = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_by_category.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Sentiment by Category — plot_sentiment_by_category","text":"sentiment_data Data frame 'sentiment' column category_var Name category variable column plot_type Type plot: \"bar\" \"stacked\" (default: \"bar\") title Plot title (default: auto-generated)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_by_category.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Sentiment by Category — plot_sentiment_by_category","text":"plotly grouped/stacked bar chart","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_by_category.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Sentiment by Category — plot_sentiment_by_category","text":"","code":"if (FALSE) { # \\dontrun{ data <- data.frame(   text = c(\"Good\", \"Bad\", \"Okay\", \"Great\", \"Poor\"),   category = c(\"A\", \"A\", \"B\", \"B\", \"B\") ) data <- cbind(data, analyze_sentiment(data$text)) plot <- plot_sentiment_by_category(data, \"category\") print(plot) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Sentiment Distribution — plot_sentiment_distribution","title":"Plot Sentiment Distribution — plot_sentiment_distribution","text":"Creates bar plot showing distribution sentiment classifications.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Sentiment Distribution — plot_sentiment_distribution","text":"","code":"plot_sentiment_distribution(sentiment_data, title = \"Sentiment Distribution\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Sentiment Distribution — plot_sentiment_distribution","text":"sentiment_data Data frame analyze_sentiment() 'sentiment' column title Plot title (default: \"Sentiment Distribution\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Sentiment Distribution — plot_sentiment_distribution","text":"plotly bar chart","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Sentiment Distribution — plot_sentiment_distribution","text":"","code":"if (FALSE) { # \\dontrun{ texts <- c(\"Great results!\", \"Poor performance\", \"Okay outcome\") sentiment_data <- analyze_sentiment(texts) plot <- plot_sentiment_distribution(sentiment_data) print(plot) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_violin.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Sentiment Violin Plot by Category — plot_sentiment_violin","title":"Plot Sentiment Violin Plot by Category — plot_sentiment_violin","text":"Creates violin plot showing sentiment score distribution category.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_violin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Sentiment Violin Plot by Category — plot_sentiment_violin","text":"","code":"plot_sentiment_violin(   sentiment_data,   category_var = \"category_var\",   title = \"Sentiment Score Distribution\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_violin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Sentiment Violin Plot by Category — plot_sentiment_violin","text":"sentiment_data Data frame analyze_sentiment() containing sentiment_score category columns category_var Name category variable column (default: \"category_var\") title Plot title (default: \"Sentiment Score Distribution\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_sentiment_violin.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Sentiment Violin Plot by Category — plot_sentiment_violin","text":"plotly violin plot","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_similarity_heatmap.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Document Similarity Heatmap — plot_similarity_heatmap","title":"Plot Document Similarity Heatmap — plot_similarity_heatmap","text":"Creates interactive heatmap visualization document similarity matrices support document metadata, feature-specific colorscales, rich tooltips. Supports symmetric (-vs-) cross-category comparison modes.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_similarity_heatmap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Document Similarity Heatmap — plot_similarity_heatmap","text":"","code":"plot_similarity_heatmap(   similarity_matrix,   docs_data = NULL,   feature_type = \"words\",   method_name = \"Cosine\",   title = NULL,   category_filter = NULL,   doc_id_var = NULL,   colorscale = NULL,   height = 600,   width = NULL,   row_category = NULL,   col_categories = NULL,   category_var = \"category_display\",   show_values = FALSE,   facet = NULL,   row_label = NULL,   output_type = \"plotly\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_similarity_heatmap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Document Similarity Heatmap — plot_similarity_heatmap","text":"similarity_matrix square numeric matrix similarity scores docs_data Optional data frame document metadata containing: document_number: Document identifiers axis labels document_id_display: Document IDs hover text category_display: Category labels hover text feature_type Feature space type: \"words\", \"topics\", \"ngrams\", \"embeddings\" (determines colorscale display name) method_name Similarity method name display (default: \"Cosine\") title Plot title (default: NULL, auto-generated feature_type) category_filter Optional category filter label title (default: NULL) doc_id_var Name document ID variable (affects label text, default: NULL) colorscale Plotly colorscale override (default: NULL, uses feature_type default) height Plot height pixels (default: 600) width Plot width pixels (default: NULL auto) row_category Category row documents cross-category mode (default: NULL) col_categories Character vector categories column documents (default: NULL) category_var Name category variable docs_data (default: \"category_display\") show_values Logical; show similarity values text tiles (default: FALSE) facet Logical; facet column categories (default: TRUE col_categories specified) row_label Label row axis (default: NULL, uses row_category) output_type Output type: \"plotly\" \"ggplot\" (default: \"plotly\", auto-switches \"ggplot\" faceting)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_similarity_heatmap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Document Similarity Heatmap — plot_similarity_heatmap","text":"plotly ggplot2 heatmap object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_similarity_heatmap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Document Similarity Heatmap — plot_similarity_heatmap","text":"","code":"if (FALSE) { # \\dontrun{ # Simple usage with matrix only sim_matrix <- matrix(runif(25), nrow = 5) plot_similarity_heatmap(sim_matrix)  # With document metadata docs <- data.frame(   document_number = paste(\"Doc\", 1:5),   document_id_display = c(\"Paper A\", \"Paper B\", \"Paper C\", \"Paper D\", \"Paper E\"),   category_display = c(\"Science\", \"Science\", \"Tech\", \"Tech\", \"Health\") ) plot_similarity_heatmap(sim_matrix, docs_data = docs, feature_type = \"embeddings\")  # Cross-category comparison with faceting plot_similarity_heatmap(   sim_matrix,   docs_data = docs,   row_category = \"Science\",   col_categories = c(\"Tech\", \"Health\"),   show_values = TRUE,   facet = TRUE ) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_term_trends_continuous.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Term Frequency Trends by Continuous Variable — plot_term_trends_continuous","title":"Plot Term Frequency Trends by Continuous Variable — plot_term_trends_continuous","text":"Creates faceted line plot showing term frequencies vary across continuous variable (e.g., year, time period).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_term_trends_continuous.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Term Frequency Trends by Continuous Variable — plot_term_trends_continuous","text":"","code":"plot_term_trends_continuous(   term_data,   continuous_var,   terms = NULL,   title = NULL,   height = 600,   width = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_term_trends_continuous.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Term Frequency Trends by Continuous Variable — plot_term_trends_continuous","text":"term_data Data frame containing term frequencies columns: continuous_var, term, word_frequency continuous_var Name continuous variable column terms Character vector terms display (optional, filters provided) title Plot title (default: NULL, auto-generated) height Plot height pixels (default: 600) width Plot width pixels (default: NULL, auto)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_term_trends_continuous.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Term Frequency Trends by Continuous Variable — plot_term_trends_continuous","text":"plotly object faceted line plots","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_term_trends_continuous.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Term Frequency Trends by Continuous Variable — plot_term_trends_continuous","text":"","code":"if (FALSE) { # \\dontrun{ term_df <- data.frame(   year = rep(2010:2020, each = 3),   term = rep(c(\"learning\", \"education\", \"technology\"), 11),   word_frequency = sample(10:100, 33, replace = TRUE) ) plot_term_trends_continuous(term_df, \"year\", c(\"learning\", \"education\")) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_tfidf_keywords.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot TF-IDF Keywords — plot_tfidf_keywords","title":"Plot TF-IDF Keywords — plot_tfidf_keywords","text":"Creates horizontal bar plot top keywords TF-IDF score.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_tfidf_keywords.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot TF-IDF Keywords — plot_tfidf_keywords","text":"","code":"plot_tfidf_keywords(tfidf_data, title = NULL, normalized = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_tfidf_keywords.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot TF-IDF Keywords — plot_tfidf_keywords","text":"tfidf_data Data frame extract_keywords_tfidf() title Plot title (default: \"Top Keywords TF-IDF Score\") normalized Logical, whether scores normalized (label) (default: FALSE)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_tfidf_keywords.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot TF-IDF Keywords — plot_tfidf_keywords","text":"plotly bar chart","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_effects_categorical.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Topic Effects for Categorical Variables — plot_topic_effects_categorical","title":"Plot Topic Effects for Categorical Variables — plot_topic_effects_categorical","text":"Creates faceted plot showing categorical variables affect topic proportions.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_effects_categorical.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Topic Effects for Categorical Variables — plot_topic_effects_categorical","text":"","code":"plot_topic_effects_categorical(   effects_data,   ncol = 2,   height = 800,   width = 1000,   title = \"Category Effects\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_effects_categorical.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Topic Effects for Categorical Variables — plot_topic_effects_categorical","text":"effects_data Data frame columns: topic, value, proportion, lower, upper ncol Number columns faceting (default: 2) height Plot height pixels (default: 800) width Plot width pixels (default: 1000) title Plot title (default: \"Category Effects\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_effects_categorical.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Topic Effects for Categorical Variables — plot_topic_effects_categorical","text":"plotly object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_effects_continuous.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Topic Effects for Continuous Variables — plot_topic_effects_continuous","title":"Plot Topic Effects for Continuous Variables — plot_topic_effects_continuous","text":"Creates faceted plot showing continuous variables affect topic proportions.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_effects_continuous.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Topic Effects for Continuous Variables — plot_topic_effects_continuous","text":"","code":"plot_topic_effects_continuous(   effects_data,   ncol = 2,   height = 800,   width = 1000,   title = \"Continuous Variable Effects\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_effects_continuous.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Topic Effects for Continuous Variables — plot_topic_effects_continuous","text":"effects_data Data frame columns: topic, value, proportion, lower, upper ncol Number columns faceting (default: 2) height Plot height pixels (default: 800) width Plot width pixels (default: 1000) title Plot title (default: \"Continuous Variable Effects\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_effects_continuous.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Topic Effects for Continuous Variables — plot_topic_effects_continuous","text":"plotly object","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_probability.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Per-Document Per-Topic Probabilities — plot_topic_probability","title":"Plot Per-Document Per-Topic Probabilities — plot_topic_probability","text":"function generates bar plot showing prevalence topic across documents.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_probability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Per-Document Per-Topic Probabilities — plot_topic_probability","text":"","code":"plot_topic_probability(   stm_model = NULL,   gamma_data = NULL,   top_n = 10,   height = 800,   width = 1000,   topic_labels = NULL,   colors = NULL,   verbose = TRUE,   ... )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_probability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Per-Document Per-Topic Probabilities — plot_topic_probability","text":"stm_model fitted STM model object. stm_model fitted Structural Topic Model created using stm::stm(). gamma_data Optional pre-computed gamma data frame (default: NULL). provided, used instead stm_model. top_n number topics display, ordered mean prevalence. height height resulting Plotly plot, pixels (default: 800). width width resulting Plotly plot, pixels (default: 1000). topic_labels Optional topic labels (default: NULL). colors Optional color palette topics (default: NULL). verbose Logical, TRUE, prints progress messages. ... arguments passed tidytext::tidy.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_probability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Per-Document Per-Topic Probabilities — plot_topic_probability","text":"ggplot object showing bar plot topic prevalence. Topics ordered mean gamma value (average prevalence across documents).","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_topic_probability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Per-Document Per-Topic Probabilities — plot_topic_probability","text":"","code":"if (interactive()) {  mydata <- TextAnalysisR::SpecialEduTech   united_tbl <- TextAnalysisR::unite_cols(    mydata,    listed_vars = c(\"title\", \"keyword\", \"abstract\")  )   tokens <- TextAnalysisR::prep_texts(united_tbl, text_field = \"united_texts\")   dfm_object <- quanteda::dfm(tokens)   out <- quanteda::convert(dfm_object, to = \"stm\")  stm_15 <- stm::stm(   data = out$meta,   documents = out$documents,   vocab = out$vocab,   max.em.its = 75,   init.type = \"Spectral\",   K = 15,   prevalence = ~ reference_type + s(year),   verbose = TRUE)  topic_probability_plot <- TextAnalysisR::plot_topic_probability(  stm_model = stm_15,  top_n = 10,  height = 800,  width = 1000,  verbose = TRUE)  print(topic_probability_plot) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_top_readability_documents.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Top Documents by Readability — plot_top_readability_documents","title":"Plot Top Documents by Readability — plot_top_readability_documents","text":"Creates bar plot documents ranked readability metric.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_top_readability_documents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Top Documents by Readability — plot_top_readability_documents","text":"","code":"plot_top_readability_documents(   readability_data,   metric,   top_n = 15,   order = \"highest\",   title = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_top_readability_documents.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Top Documents by Readability — plot_top_readability_documents","text":"readability_data Data frame calculate_text_readability() metric Metric plot top_n Number documents show (default: 15) order Direction: \"highest\" \"lowest\" (default: \"highest\") title Plot title (default: auto-generated)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_top_readability_documents.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Top Documents by Readability — plot_top_readability_documents","text":"plotly bar chart","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_frequency.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Word Frequency — plot_word_frequency","title":"Plot Word Frequency — plot_word_frequency","text":"Creates bar plot showing frequent words document-feature matrix (dfm).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_frequency.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Word Frequency — plot_word_frequency","text":"","code":"plot_word_frequency(dfm_object, n = 20, height = NULL, width = NULL, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_frequency.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Word Frequency — plot_word_frequency","text":"dfm_object document-feature matrix created quanteda::dfm(). n number top words display (default: 20). height height resulting Plotly plot, pixels (default: 800). width width resulting Plotly plot, pixels (default: 1000). ... Additional arguments passed plotly::ggplotly().","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_frequency.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Word Frequency — plot_word_frequency","text":"plotly object showing word frequency.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_frequency.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Word Frequency — plot_word_frequency","text":"","code":"if (interactive()) {   texts <- c(\"mathematics technology\", \"education technology\", \"learning support\")   dfm <- quanteda::dfm(quanteda::tokens(texts))   plot <- plot_word_frequency(dfm, n = 5)   print(plot) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_probability.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Word Probabilities by Topic — plot_word_probability","title":"Plot Word Probabilities by Topic — plot_word_probability","text":"Creates faceted bar plot showing top terms probabilities (beta values) topic topic model.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_probability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Word Probabilities by Topic — plot_word_probability","text":"","code":"plot_word_probability(   top_topic_terms,   topic_label = NULL,   ncol = 3,   height = 1200,   width = 800,   ylab = \"Word probability\",   title = NULL,   colors = NULL,   measure_label = \"Beta\",   ... )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_probability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Word Probabilities by Topic — plot_word_probability","text":"top_topic_terms data frame containing topic terms columns: topic, term, beta. Typically created using get_topic_terms() similar functions. topic_label Optional topic labels. Can either named vector mapping topic numbers labels, character string specifying column name top_topic_terms (default: NULL). ncol Number columns facet wrap layout (default: 3). height height resulting Plotly plot, pixels (default: 1200). width width resulting Plotly plot, pixels (default: 800). ylab Y-axis label (default: \"Word probability\"). title Plot title (default: NULL auto-generated title). colors Color palette topics (default: NULL auto-generated colors). measure_label Label probability measure (default: \"Beta\"). ... Additional arguments passed plotly::ggplotly().","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_probability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Word Probabilities by Topic — plot_word_probability","text":"plotly object showing word probabilities faceted topic.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/plot_word_probability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Word Probabilities by Topic — plot_word_probability","text":"","code":"if (interactive()) {   top_terms <- data.frame(     topic = rep(1:2, each = 5),     term = c(\"learning\", \"student\", \"education\", \"school\", \"teacher\",              \"technology\", \"computer\", \"digital\", \"software\", \"system\"),     beta = c(0.05, 0.04, 0.03, 0.02, 0.01, 0.06, 0.05, 0.04, 0.03, 0.02)   )   plot <- plot_word_probability(top_terms)   print(plot) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/prep_texts.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Text Data — prep_texts","title":"Preprocess Text Data — prep_texts","text":"Preprocesses text data following complete workflow implemented Shiny application: Constructing corpus united texts Tokenizing text words configurable options Converting lowercase acronym preservation option Applying character length filtering Optional multi-word expression detection compound term creation Stopword removal lemmatization capabilities function serves foundation subsequent text analysis workflows.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/prep_texts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Text Data — prep_texts","text":"","code":"prep_texts(   united_tbl,   text_field = \"united_texts\",   min_char = 2,   lowercase = TRUE,   remove_punct = TRUE,   remove_symbols = TRUE,   remove_numbers = TRUE,   remove_url = TRUE,   remove_separators = TRUE,   split_hyphens = TRUE,   split_tags = TRUE,   include_docvars = TRUE,   keep_acronyms = FALSE,   padding = FALSE,   remove_stopwords = FALSE,   stopwords_source = \"snowball\",   stopwords_language = \"en\",   custom_stopwords = NULL,   custom_valuetype = \"glob\",   verbose = FALSE,   ... )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/prep_texts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Text Data — prep_texts","text":"united_tbl data frame contains text data. text_field name column contains text data. min_char minimum number characters token included (default: 2). lowercase Logical; convert tokens lowercase (default: TRUE). Recommended text analysis tasks. remove_punct Logical; remove punctuation text (default: TRUE). remove_symbols Logical; remove symbols text (default: TRUE). remove_numbers Logical; remove numbers text (default: TRUE). remove_url Logical; remove URLs text (default: TRUE). remove_separators Logical; remove separators text (default: TRUE). split_hyphens Logical; split hyphenated words separate tokens (default: TRUE). split_tags Logical; split tags separate tokens (default: TRUE). include_docvars Logical; include document variables tokens object (default: TRUE). keep_acronyms Logical; keep acronyms text (default: FALSE). padding Logical; add padding tokens object (default: FALSE). remove_stopwords Logical; remove stopwords text (default: FALSE). stopwords_source Character; source stopwords, e.g., \"snowball\", \"stopwords-iso\" (default: \"snowball\"). stopwords_language Character; language stopwords (default: \"en\"). custom_stopwords Character vector; additional words remove (default: NULL). custom_valuetype Character; valuetype custom_stopwords pattern matching, one \"glob\", \"regex\", \"fixed\" (default: \"glob\"). verbose Logical; print verbose output (default: FALSE). ... Additional arguments passed quanteda::tokens.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/prep_texts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Text Data — prep_texts","text":"tokens object contains preprocessed text data.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/prep_texts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess Text Data — prep_texts","text":"","code":"if (interactive()) { mydata <- TextAnalysisR::SpecialEduTech  united_tbl <- TextAnalysisR::unite_cols(   mydata,   listed_vars = c(\"title\", \"keyword\", \"abstract\") )  tokens <- TextAnalysisR::prep_texts(united_tbl,                                          text_field = \"united_texts\",                                          min_char = 2,                                          lowercase = TRUE,                                          remove_punct = TRUE,                                          remove_symbols = TRUE,                                          remove_numbers = TRUE,                                          remove_url = TRUE,                                          remove_separators = TRUE,                                          split_hyphens = TRUE,                                          split_tags = TRUE,                                          include_docvars = TRUE,                                          keep_acronyms = FALSE,                                          padding = FALSE,                                          verbose = FALSE) print(tokens) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Process PDF File — process_pdf_file","title":"Process PDF File — process_pdf_file","text":"Main function process PDF files - extracts text content using pdftools. table extraction, use process_pdf_file_py.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process PDF File — process_pdf_file","text":"","code":"process_pdf_file(file_path, content_type = \"auto\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process PDF File — process_pdf_file","text":"file_path Character string path PDF file content_type Character string: \"auto\" \"text\" (default: \"auto\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process PDF File — process_pdf_file","text":"List : data: Data frame extracted content type: Character string indicating content type (\"text\" \"error\") success: Logical indicating success message: Character string status message","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process PDF File — process_pdf_file","text":"function extracts text content PDFs using pdftools package. Works best text-based PDFs (scanned images). PDFs containing tables complex layouts, use Python-based process_pdf_file_py provides better table extraction.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process PDF File — process_pdf_file","text":"","code":"if (FALSE) { # \\dontrun{ pdf_path <- \"path/to/document.pdf\" result <- process_pdf_file(pdf_path)  if (result$success) {   print(head(result$data)) } else {   print(result$message) } } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file_py.html","id":null,"dir":"Reference","previous_headings":"","what":"Process PDF File using Python — process_pdf_file_py","title":"Process PDF File using Python — process_pdf_file_py","text":"Main function process PDF files using pdfplumber (Python). Automatically detects content type extracts data accordingly. Java required.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file_py.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process PDF File using Python — process_pdf_file_py","text":"","code":"process_pdf_file_py(   file_path,   content_type = \"auto\",   envname = \"textanalysisr-env\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file_py.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process PDF File using Python — process_pdf_file_py","text":"file_path Character string path PDF file content_type Character string: \"auto\", \"text\", \"tabular\" \"auto\", detect content type automatically envname Character string, name Python virtual environment (default: \"langgraph-env\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file_py.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process PDF File using Python — process_pdf_file_py","text":"List : data: Data frame extracted content type: Character string indicating content type success: Logical indicating success message: Character string status message","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file_py.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process PDF File using Python — process_pdf_file_py","text":"function uses Python's pdfplumber library : Handles text tables Java dependency Better accuracy tabulizer complex tables Uses Python environment LangGraph","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_file_py.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process PDF File using Python — process_pdf_file_py","text":"","code":"if (FALSE) { # \\dontrun{ setup_langgraph_env()  pdf_path <- \"path/to/document.pdf\" result <- process_pdf_file_py(pdf_path)  if (result$success) {   print(head(result$data)) } else {   print(result$message) } } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_unified.html","id":null,"dir":"Reference","previous_headings":"","what":"Process PDF File (Unified Entry Point) — process_pdf_unified","title":"Process PDF File (Unified Entry Point) — process_pdf_unified","text":"Unified PDF processing automatic fallback: Multimodal (Python + Vision) 2. Python pdfplumber 3. R pdftools","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_unified.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process PDF File (Unified Entry Point) — process_pdf_unified","text":"","code":"process_pdf_unified(   file_path,   use_multimodal = FALSE,   vision_provider = \"ollama\",   vision_model = NULL,   api_key = NULL,   describe_images = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_unified.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process PDF File (Unified Entry Point) — process_pdf_unified","text":"file_path Character string path PDF file use_multimodal Logical, enable multimodal extraction vision_provider Character, \"ollama\" \"openai\" vision_model Character, model name api_key Character, OpenAI API key (using OpenAI) describe_images Logical, generate image descriptions","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/process_pdf_unified.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process PDF File (Unified Entry Point) — process_pdf_unified","text":"List: success, data, type, method, message","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/reduce_dimensions.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction Analysis — reduce_dimensions","title":"Dimensionality Reduction Analysis — reduce_dimensions","text":"function performs dimensionality reduction using various methods including PCA, t-SNE, UMAP. efficiency consistency, PCA preprocessing always performed first, t-SNE/UMAP use PCA results input. follows best practices high-dimensional data analysis.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/reduce_dimensions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensionality Reduction Analysis — reduce_dimensions","text":"","code":"reduce_dimensions(   data_matrix,   method = \"PCA\",   n_components = 2,   pca_dims = 50,   tsne_perplexity = 30,   tsne_max_iter = 1000,   umap_neighbors = 15,   umap_min_dist = 0.1,   umap_metric = \"cosine\",   seed = 123,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/reduce_dimensions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensionality Reduction Analysis — reduce_dimensions","text":"data_matrix numeric matrix rows represent documents columns represent features. method dimensionality reduction method. Options: \"PCA\", \"t-SNE\", \"UMAP\". n_components number components/dimensions reduce (default: 2). pca_dims number dimensions PCA preprocessing (default: 50). tsne_perplexity perplexity parameter t-SNE (default: 30). tsne_max_iter maximum number iterations t-SNE (default: 1000). umap_neighbors number neighbors UMAP (default: 15). umap_min_dist minimum distance UMAP (default: 0.1). umap_metric metric UMAP (default: \"cosine\"). seed Random seed reproducibility (default: 123). verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/reduce_dimensions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensionality Reduction Analysis — reduce_dimensions","text":"list containing reduced dimensions, method used, additional metadata.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/reduce_dimensions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dimensionality Reduction Analysis — reduce_dimensions","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_cols(     mydata,     listed_vars = c(\"title\", \"keyword\", \"abstract\")   )    tokens <- TextAnalysisR::prep_texts(united_tbl, text_field = \"united_texts\")    dfm_object <- quanteda::dfm(tokens)    data_matrix <- as.matrix(dfm_object)    pca_result <- TextAnalysisR::reduce_dimensions(     data_matrix,     method = \"PCA\"   )   print(pca_result)    tsne_result <- TextAnalysisR::reduce_dimensions(     data_matrix,     method = \"t-SNE\"   )   print(tsne_result)    umap_result <- TextAnalysisR::reduce_dimensions(     data_matrix,     method = \"UMAP\"   )   print(umap_result) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/remove_notification_by_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove Notification by ID — remove_notification_by_id","title":"Remove Notification by ID — remove_notification_by_id","text":"Removes notification specific ID.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/remove_notification_by_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove Notification by ID — remove_notification_by_id","text":"","code":"remove_notification_by_id(id)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/remove_notification_by_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove Notification by ID — remove_notification_by_id","text":"id notification ID remove","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/remove_notification_by_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove Notification by ID — remove_notification_by_id","text":"Removes Shiny notification. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/require_feature.html","id":null,"dir":"Reference","previous_headings":"","what":"Require Feature — require_feature","title":"Require Feature — require_feature","text":"Checks feature availability shows notification unavailable.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/require_feature.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Require Feature — require_feature","text":"","code":"require_feature(feature, session = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/require_feature.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Require Feature — require_feature","text":"feature Character: feature name check session Shiny session object (optional)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/require_feature.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Require Feature — require_feature","text":"Logical TRUE available, FALSE ","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/require_feature.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Require Feature — require_feature","text":"","code":"if (FALSE) { # \\dontrun{ if (!require_feature(\"embeddings\", session)) return() } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_app.html","id":null,"dir":"Reference","previous_headings":"","what":"Launch the TextAnalysisR app — run_app","title":"Launch the TextAnalysisR app — run_app","text":"Launch TextAnalysisR Shiny application.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_app.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Launch the TextAnalysisR app — run_app","text":"","code":"run_app(launch.browser = interactive())"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_app.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Launch the TextAnalysisR app — run_app","text":"launch.browser Logical. Whether open app browser. Defaults interactive(), FALSE non-interactive sessions (e.g., Docker containers, servers).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_app.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Launch the TextAnalysisR app — run_app","text":"return value, called side effects (launching Shiny app)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_app.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Launch the TextAnalysisR app — run_app","text":"","code":"if (interactive()) {   library(TextAnalysisR)   run_app() }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_contrastive_topics_internal.html","id":null,"dir":"Reference","previous_headings":"","what":"Contrastive Learning Topic Modeling — run_contrastive_topics_internal","title":"Contrastive Learning Topic Modeling — run_contrastive_topics_internal","text":"Implements contrastive learning approaches topic modeling improve topic separation discriminability.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_contrastive_topics_internal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Contrastive Learning Topic Modeling — run_contrastive_topics_internal","text":"","code":"run_contrastive_topics_internal(   texts,   n_topics = 10,   temperature = 0.1,   negative_sampling_rate = 5,   embedding_model = \"all-MiniLM-L6-v2\",   seed = 123 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_contrastive_topics_internal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Contrastive Learning Topic Modeling — run_contrastive_topics_internal","text":"texts Character vector documents n_topics Number topics discover temperature Temperature parameter contrastive learning negative_sampling_rate Rate negative sampling embedding_model Transformer model embeddings seed Random seed reproducibility","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_contrastive_topics_internal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Contrastive Learning Topic Modeling — run_contrastive_topics_internal","text":"List containing contrastive topic model metrics","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_neural_topics_internal.html","id":null,"dir":"Reference","previous_headings":"","what":"Neural Topic Modeling — run_neural_topics_internal","title":"Neural Topic Modeling — run_neural_topics_internal","text":"Implements neural topic modeling using deep learning architectures improved topic discovery representation learning.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_neural_topics_internal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Neural Topic Modeling — run_neural_topics_internal","text":"","code":"run_neural_topics_internal(   texts,   n_topics = 10,   hidden_layers = 2,   hidden_units = 100,   dropout_rate = 0.2,   embedding_model = \"all-MiniLM-L6-v2\",   seed = 123 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_neural_topics_internal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Neural Topic Modeling — run_neural_topics_internal","text":"texts Character vector documents n_topics Number topics discover hidden_layers Number hidden layers neural network hidden_units Number units per hidden layer dropout_rate Dropout rate regularization embedding_model Transformer model initial embeddings seed Random seed reproducibility","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_neural_topics_internal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Neural Topic Modeling — run_neural_topics_internal","text":"List containing neural topic model diagnostics","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_rag_search.html","id":null,"dir":"Reference","previous_headings":"","what":"RAG-Enhanced Semantic Search — run_rag_search","title":"RAG-Enhanced Semantic Search — run_rag_search","text":"Uses LangGraph multi-agent workflow Retrieval Augmented Generation. Provides question-answering document corpus source attribution.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_rag_search.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"RAG-Enhanced Semantic Search — run_rag_search","text":"","code":"run_rag_search(   query,   documents,   ollama_model = \"llama3\",   ollama_base_url = \"http://localhost:11434\",   embedding_model = \"nomic-embed-text\",   top_k = 5,   envname = \"textanalysisr-env\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_rag_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"RAG-Enhanced Semantic Search — run_rag_search","text":"query Character string, user question documents Character vector, corpus search ollama_model Character string, LLM model (default: \"llama3\") ollama_base_url Character string, Ollama API endpoint embedding_model Character string, embedding model (default: \"nomic-embed-text\") top_k Integer, number documents retrieve (default: 5) envname Character string, Python environment name","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_rag_search.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"RAG-Enhanced Semantic Search — run_rag_search","text":"List : success: Logical answer: Generated answer confidence: Confidence score (0-1) sources: Vector source document IDs retrieved_docs: Retrieved document chunks scores: Similarity scores","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_rag_search.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"RAG-Enhanced Semantic Search — run_rag_search","text":"Multi-agent workflow: Retrieval Agent: Find relevant documents via embeddings Generation Agent: Create answer context Validation Agent: Assess answer quality Conditional retry confidence < 0.4 Requires Ollama embedding model:","code":"ollama pull llama3 ollama pull nomic-embed-text"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_rag_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"RAG-Enhanced Semantic Search — run_rag_search","text":"","code":"if (FALSE) { # \\dontrun{ documents <- c(   \"Assistive technology helps students with disabilities access curriculum.\",   \"Universal Design for Learning provides multiple means of engagement.\",   \"Response to Intervention uses tiered support systems.\" )  result <- run_rag_search(   query = \"How does assistive technology support learning?\",   documents = documents )  if (result$success) {   cat(\"Answer:\", result$answer, \"\\n\")   cat(\"Confidence:\", result$confidence, \"\\n\")   cat(\"Sources:\", paste(result$sources, collapse = \", \"), \"\\n\") } } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_temporal_topics_internal.html","id":null,"dir":"Reference","previous_headings":"","what":"Temporal Dynamic Topic Modeling — run_temporal_topics_internal","title":"Temporal Dynamic Topic Modeling — run_temporal_topics_internal","text":"Analyzes topic evolution time periods using dynamic modeling approaches track concept emergence, evolution, decline.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_temporal_topics_internal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Temporal Dynamic Topic Modeling — run_temporal_topics_internal","text":"","code":"run_temporal_topics_internal(   texts,   metadata = NULL,   n_topics = 10,   temporal_unit = \"year\",   temporal_window = 3,   detect_evolution = TRUE,   embedding_model = \"all-MiniLM-L6-v2\",   seed = 123 )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_temporal_topics_internal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Temporal Dynamic Topic Modeling — run_temporal_topics_internal","text":"texts Character vector documents metadata Data frame containing temporal information n_topics Number topics discover temporal_unit Unit temporal analysis (\"year\", \"quarter\", \"month\") temporal_window Size temporal window analysis detect_evolution Whether detect topic evolution patterns embedding_model Transformer model embeddings seed Random seed reproducibility","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_temporal_topics_internal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Temporal Dynamic Topic Modeling — run_temporal_topics_internal","text":"List containing temporal topic model evolution analysis","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_text_workflow.html","id":null,"dir":"Reference","previous_headings":"","what":"Complete Text Mining Workflow — run_text_workflow","title":"Complete Text Mining Workflow — run_text_workflow","text":"function provides complete text mining workflow follows sequence Shiny application: file processing → text uniting → preprocessing → DFM creation → analysis. serves convenience function users want execute entire pipeline programmatically.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_text_workflow.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Complete Text Mining Workflow — run_text_workflow","text":"","code":"run_text_workflow(   dataset_choice,   file_info = NULL,   text_input = NULL,   listed_vars,   min_char = 2,   remove_punct = TRUE,   remove_symbols = TRUE,   remove_numbers = TRUE,   remove_url = TRUE,   detect_compounds = FALSE,   compound_size = 2:3,   compound_min_count = 2,   verbose = TRUE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_text_workflow.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Complete Text Mining Workflow — run_text_workflow","text":"dataset_choice character string indicating dataset choice: \"Upload Example Dataset\", \"Upload File\", \"Copy Paste Text\". file_info data frame containing file information (file upload). text_input character string containing text input (copy-paste). listed_vars character vector column names unite text. min_char minimum number characters tokens (default: 2). remove_punct Logical; remove punctuation (default: TRUE). remove_symbols Logical; remove symbols (default: TRUE). remove_numbers Logical; remove numbers (default: TRUE). remove_url Logical; remove URLs (default: TRUE). detect_compounds Logical; detect multi-word expressions (default: FALSE). compound_size Size range compound detection (default: 2:3). compound_min_count Minimum count compounds (default: 2). verbose Logical; print progress messages (default: TRUE).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_text_workflow.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Complete Text Mining Workflow — run_text_workflow","text":"list containing processed data, tokens, DFM, metadata.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/run_text_workflow.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Complete Text Mining Workflow — run_text_workflow","text":"","code":"if (interactive()) {   # Using example dataset   workflow_result <- TextAnalysisR::run_text_workflow(     dataset_choice = \"Upload an Example Dataset\",     listed_vars = c(\"title\", \"keyword\", \"abstract\")   )    # Using file upload   file_info <- data.frame(filepath = \"path/to/your/file.xlsx\")   workflow_result <- TextAnalysisR::run_text_workflow(     dataset_choice = \"Upload Your File\",     file_info = file_info,     listed_vars = c(\"column1\", \"column2\")   )    # Using copy-paste text   workflow_result <- TextAnalysisR::run_text_workflow(     dataset_choice = \"Copy and Paste Text\",     text_input = \"Your text content here\",     listed_vars = \"text\"   ) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sanitize_text_input.html","id":null,"dir":"Reference","previous_headings":"","what":"Sanitize Text Input — sanitize_text_input","title":"Sanitize Text Input — sanitize_text_input","text":"Sanitize Text Input","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sanitize_text_input.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sanitize Text Input — sanitize_text_input","text":"","code":"sanitize_text_input(text)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sanitize_text_input.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sanitize Text Input — sanitize_text_input","text":"text Text input user","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sanitize_text_input.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sanitize Text Input — sanitize_text_input","text":"Sanitized text","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_cooccurrence_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Word Co-occurrence Network — semantic_cooccurrence_network","title":"Compute Word Co-occurrence Network — semantic_cooccurrence_network","text":"Computes word co-occurrence networks community detection network metrics. Supports multiple feature spaces: unigrams, n-grams, embeddings. Based proven implementation intuitive network visualization.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_cooccurrence_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Word Co-occurrence Network — semantic_cooccurrence_network","text":"","code":"semantic_cooccurrence_network(   dfm_object,   doc_var = NULL,   co_occur_n = 10,   top_node_n = 30,   node_label_size = 22,   pattern = NULL,   showlegend = TRUE,   seed = NULL,   feature_type = \"words\",   ngram_range = 2,   texts = NULL,   embeddings = NULL,   embedding_sim_threshold = 0.5,   community_method = \"leiden\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_cooccurrence_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Word Co-occurrence Network — semantic_cooccurrence_network","text":"dfm_object quanteda document-feature matrix (dfm). doc_var document-level metadata variable categories (default: NULL). co_occur_n Minimum co-occurrence count (default: 10). top_node_n Number top nodes display based degree centrality (default: 30). node_label_size Font size node labels (default: 14). pattern Regex pattern filter specific words (default: NULL). showlegend Whether show community legend (default: TRUE). seed Random seed reproducible layout (default: NULL). feature_type Feature space: \"words\", \"ngrams\", \"embeddings\" (default: \"words\"). ngram_range N-gram size feature_type = \"ngrams\" (default: 2). texts Optional character vector texts n-gram creation (default: NULL). embeddings Optional embedding matrix embedding-based networks (default: NULL). embedding_sim_threshold Similarity threshold embedding-based networks (default: 0.5). community_method Community detection method: \"leiden\" (default), \"louvain\", \"label_prop\", \"fast_greedy\".","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_cooccurrence_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Word Co-occurrence Network — semantic_cooccurrence_network","text":"list containing plot, table, nodes, edges, stats","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_correlation_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Word Correlation Network — semantic_correlation_network","title":"Compute Word Correlation Network — semantic_correlation_network","text":"Computes word correlation networks community detection network metrics. Supports multiple feature spaces: unigrams, n-grams, embeddings. Based proven implementation intuitive network visualization.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_correlation_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Word Correlation Network — semantic_correlation_network","text":"","code":"semantic_correlation_network(   dfm_object,   doc_var = NULL,   common_term_n = 20,   corr_n = 0.4,   top_node_n = 30,   node_label_size = 22,   pattern = NULL,   showlegend = TRUE,   seed = NULL,   feature_type = \"words\",   ngram_range = 2,   texts = NULL,   embeddings = NULL,   embedding_sim_threshold = 0.5,   community_method = \"leiden\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_correlation_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Word Correlation Network — semantic_correlation_network","text":"dfm_object quanteda document-feature matrix (dfm). doc_var document-level metadata variable categories (default: NULL). common_term_n Minimum term frequency include (default: 20). corr_n Minimum correlation threshold (default: 0.4). top_node_n Number top nodes display (default: 30). node_label_size Font size node labels (default: 14). pattern Regex pattern filter specific words (default: NULL). showlegend Whether show community legend (default: TRUE). seed Random seed reproducible layout (default: NULL). feature_type Feature space: \"words\", \"ngrams\", \"embeddings\" (default: \"words\"). ngram_range N-gram size feature_type = \"ngrams\" (default: 2). texts Optional character vector texts n-gram creation (default: NULL). embeddings Optional embedding matrix embedding-based networks (default: NULL). embedding_sim_threshold Similarity threshold embedding-based networks (default: 0.5). community_method Community detection method: \"leiden\" (default), \"louvain\", \"label_prop\", \"fast_greedy\".","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_correlation_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Word Correlation Network — semantic_correlation_network","text":"list containing plot, table, nodes, edges, stats","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_document_clustering.html","id":null,"dir":"Reference","previous_headings":"","what":"Semantic Document Clustering — semantic_document_clustering","title":"Semantic Document Clustering — semantic_document_clustering","text":"Creates unified visualization document clustering optional clustering","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_document_clustering.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semantic Document Clustering — semantic_document_clustering","text":"","code":"semantic_document_clustering(embeddings, method = \"UMAP\", clusters = NULL, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_document_clustering.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semantic Document Clustering — semantic_document_clustering","text":"embeddings Document embeddings matrix method Dimensionality reduction method (\"PCA\", \"t-SNE\", \"UMAP\") clusters Optional cluster assignments ... Additional arguments","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_similarity_analysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Semantic Similarity Analysis — semantic_similarity_analysis","title":"Semantic Similarity Analysis — semantic_similarity_analysis","text":"Wrapper calculate_document_similarity","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_similarity_analysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semantic Similarity Analysis — semantic_similarity_analysis","text":"","code":"semantic_similarity_analysis(...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_similarity_analysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semantic Similarity Analysis — semantic_similarity_analysis","text":"... Arguments passed calculate_document_similarity","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/semantic_similarity_analysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semantic Similarity Analysis — semantic_similarity_analysis","text":"Similarity analysis results","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sentiment_embedding_analysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Embedding-based Sentiment Analysis — sentiment_embedding_analysis","title":"Embedding-based Sentiment Analysis — sentiment_embedding_analysis","text":"Performs sentiment analysis using transformer-based embeddings neural models. approach uses pre-trained language models contextual sentiment detection without requiring sentiment lexicons. Particularly effective handling: Complex contextual sentiment Implicit sentiment sarcasm Domain-specific sentiment Negation intensifiers (automatically handled model)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sentiment_embedding_analysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedding-based Sentiment Analysis — sentiment_embedding_analysis","text":"","code":"sentiment_embedding_analysis(   texts,   embeddings = NULL,   model_name = \"distilbert-base-uncased-finetuned-sst-2-english\",   doc_names = NULL,   use_gpu = FALSE )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sentiment_embedding_analysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedding-based Sentiment Analysis — sentiment_embedding_analysis","text":"texts Character vector texts analyze embeddings Optional pre-computed embedding matrix (generate_embeddings) model_name Sentiment model name (default: \"distilbert-base-uncased-finetuned-sst-2-english\") doc_names Optional document names/IDs use_gpu Whether use GPU available (default: FALSE)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sentiment_embedding_analysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Embedding-based Sentiment Analysis — sentiment_embedding_analysis","text":"list containing: document_sentiment Data frame document-level sentiment scores classifications emotion_scores NULL (emotion detection currently supported embeddings) summary_stats Summary statistics including document counts average scores model_used Name transformer model used feature_type \"embeddings\"","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sentiment_embedding_analysis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Embedding-based Sentiment Analysis — sentiment_embedding_analysis","text":"","code":"if (FALSE) { # \\dontrun{ texts <- c(   \"The results significantly improved student outcomes.\",   \"The intervention showed no clear benefit.\",   \"Students reported difficulty with the material.\" ) result <- sentiment_embedding_analysis(texts) print(result$document_sentiment) print(result$summary_stats) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sentiment_lexicon_analysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze Sentiment Using Tidytext Lexicons — sentiment_lexicon_analysis","title":"Analyze Sentiment Using Tidytext Lexicons — sentiment_lexicon_analysis","text":"Performs lexicon-based sentiment analysis DFM object using tidytext lexicons. Supports AFINN, Bing, NRC lexicons comprehensive scoring emotion analysis. Now supports n-grams improved negation intensifier handling.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sentiment_lexicon_analysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze Sentiment Using Tidytext Lexicons — sentiment_lexicon_analysis","text":"","code":"sentiment_lexicon_analysis(   dfm_object,   lexicon = \"afinn\",   texts_df = NULL,   feature_type = \"words\",   ngram_range = 2,   texts = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sentiment_lexicon_analysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze Sentiment Using Tidytext Lexicons — sentiment_lexicon_analysis","text":"dfm_object quanteda DFM object (unigram n-gram) lexicon Lexicon use: \"afinn\", \"bing\", \"nrc\" (default: \"afinn\") texts_df Optional data frame original texts metadata (default: NULL) feature_type Feature space: \"words\" (unigrams) \"ngrams\" (default: \"words\") ngram_range N-gram size feature_type = \"ngrams\" (default: 2 bigrams) texts Optional character vector texts n-gram creation (default: NULL)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sentiment_lexicon_analysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze Sentiment Using Tidytext Lexicons — sentiment_lexicon_analysis","text":"list containing: document_sentiment Data frame sentiment scores per document emotion_scores Data frame emotion scores (NRC ) summary_stats List summary statistics feature_type Feature type used analysis","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/sentiment_lexicon_analysis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analyze Sentiment Using Tidytext Lexicons — sentiment_lexicon_analysis","text":"","code":"if (FALSE) { # \\dontrun{ corp <- quanteda::corpus(c(\"I love this!\", \"I hate that\", \"It's okay\")) dfm_obj <- quanteda::dfm(quanteda::tokens(corp)) results <- sentiment_lexicon_analysis(dfm_obj, lexicon = \"afinn\") print(results$document_sentiment)  texts <- c(\"not good at all\", \"very happy indeed\") results_ngram <- sentiment_lexicon_analysis(   dfm_obj,   lexicon = \"bing\",   feature_type = \"ngrams\",   ngram_range = 2,   texts = texts ) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/setup_python_env.html","id":null,"dir":"Reference","previous_headings":"","what":"Setup Python Environment — setup_python_env","title":"Setup Python Environment — setup_python_env","text":"Intelligently sets Python virtual environment required packages. Detects existing Python installations guides users Python missing.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/setup_python_env.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setup Python Environment — setup_python_env","text":"","code":"setup_python_env(envname = \"textanalysisr-env\", force = FALSE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/setup_python_env.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Setup Python Environment — setup_python_env","text":"envname Character string name virtual environment (default: \"textanalysisr-env\") force Logical, whether recreate environment exists (default: FALSE)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/setup_python_env.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setup Python Environment — setup_python_env","text":"Invisible TRUE successful, stops error message failed","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/setup_python_env.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Setup Python Environment — setup_python_env","text":"function: Automatically detects Python already installed Offers install Miniconda Python found Creates isolated virtual environment (modify system Python) Installs 6 core packages (minimal installation): langchain-core (core LangChain functionality) langchain-ollama (Ollama integration) langgraph (workflow graphs) langgraph-checkpoint (workflow state management) ollama (Ollama client) pdfplumber (PDF table extraction) Dependencies installed automatically pip Avoids heavy packages (marker-pdf, nougat-ocr, torch) virtual environment approach means: conflicts Python projects Easy remove (just delete environment) System Python remains untouched Much smaller download (~100MB vs 5GB+) setup, restart R session activate enhanced features.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/setup_python_env.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Setup Python Environment — setup_python_env","text":"","code":"if (FALSE) { # \\dontrun{ # First time setup (auto-detects Python) setup_python_env()  # Recreate environment setup_python_env(force = TRUE) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_completion_notification.html","id":null,"dir":"Reference","previous_headings":"","what":"Show Completion Notification — show_completion_notification","title":"Show Completion Notification — show_completion_notification","text":"Displays temporary success notification task completes.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_completion_notification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show Completion Notification — show_completion_notification","text":"","code":"show_completion_notification(message, duration = 5)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_completion_notification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show Completion Notification — show_completion_notification","text":"message completion message display duration Duration seconds (default: 5)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_completion_notification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show Completion Notification — show_completion_notification","text":"Displays Shiny notification. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_dfm_instructions_modal.html","id":null,"dir":"Reference","previous_headings":"","what":"Show DFM Setup Instructions Modal — show_dfm_instructions_modal","title":"Show DFM Setup Instructions Modal — show_dfm_instructions_modal","text":"Displays modal dialog console-style instructions creating DFM. Uses verbatimTextOutput formatting.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_dfm_instructions_modal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show DFM Setup Instructions Modal — show_dfm_instructions_modal","text":"","code":"show_dfm_instructions_modal(   output_id,   feature_name = \"this feature\",   session = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_dfm_instructions_modal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show DFM Setup Instructions Modal — show_dfm_instructions_modal","text":"output_id Shiny output ID verbatimTextOutput feature_name Name feature requiring DFM (default: \"feature\") session Shiny session object (default: getDefaultReactiveDomain())","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_dfm_instructions_modal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show DFM Setup Instructions Modal — show_dfm_instructions_modal","text":"Displays Shiny modal dialog. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_dfm_instructions_modal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show DFM Setup Instructions Modal — show_dfm_instructions_modal","text":"","code":"if (FALSE) { # \\dontrun{ output$dfm_instructions <- renderPrint({   cat(get_dfm_setup_instructions(\"keywords\"), sep = \"\\n\") })  show_dfm_instructions_modal(\"dfm_instructions\", \"keywords\") } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_dfm_required_modal.html","id":null,"dir":"Reference","previous_headings":"","what":"Show DFM Requirement Modal — show_dfm_required_modal","title":"Show DFM Requirement Modal — show_dfm_required_modal","text":"Displays standardized modal dialog informing users need complete preprocessing steps using feature requires document-feature matrix.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_dfm_required_modal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show DFM Requirement Modal — show_dfm_required_modal","text":"","code":"show_dfm_required_modal(   feature_name = \"this feature\",   additional_message = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_dfm_required_modal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show DFM Requirement Modal — show_dfm_required_modal","text":"feature_name Name feature requiring DFM (e.g., \"topic modeling\", \"keyword extraction\") additional_message Optional additional message display (default: NULL)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_dfm_required_modal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show DFM Requirement Modal — show_dfm_required_modal","text":"Displays Shiny modal dialog. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_dfm_required_modal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show DFM Requirement Modal — show_dfm_required_modal","text":"","code":"if (FALSE) { # \\dontrun{ if (is.null(dfm_init())) {   show_dfm_required_modal(\"topic modeling\")   return(NULL) } } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_error_notification.html","id":null,"dir":"Reference","previous_headings":"","what":"Show Error Notification — show_error_notification","title":"Show Error Notification — show_error_notification","text":"Displays error notification user.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_error_notification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show Error Notification — show_error_notification","text":"","code":"show_error_notification(message, duration = 7)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_error_notification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show Error Notification — show_error_notification","text":"message error message display duration Duration seconds (default: 7)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_error_notification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show Error Notification — show_error_notification","text":"Displays Shiny notification. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_guide_modal.html","id":null,"dir":"Reference","previous_headings":"","what":"Show Guide Modal Dialog from HTML File — show_guide_modal","title":"Show Guide Modal Dialog from HTML File — show_guide_modal","text":"Loads displays modal dialog guide content HTML file. function designed Shiny applications display help documentation stored external HTML files, reducing server.R file size improving maintainability.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_guide_modal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show Guide Modal Dialog from HTML File — show_guide_modal","text":"","code":"show_guide_modal(guide_name, title, size = \"l\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_guide_modal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show Guide Modal Dialog from HTML File — show_guide_modal","text":"guide_name Name guide file (without .html extension). Files located inst/TextAnalysisR.app/markdown/guides/ title Modal dialog title display size Size modal dialog (default: \"l\" large). Options: \"s\" (small), \"m\" (medium), \"l\" (large)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_guide_modal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show Guide Modal Dialog from HTML File — show_guide_modal","text":"Displays Shiny modal dialog. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_guide_modal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Show Guide Modal Dialog from HTML File — show_guide_modal","text":"Guide HTML files placed : inst/TextAnalysisR.app/markdown/guides/<guide_name>.html function look guide file installed package location. file found, displays error message modal.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_guide_modal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show Guide Modal Dialog from HTML File — show_guide_modal","text":"","code":"if (FALSE) { # \\dontrun{ observeEvent(input$showDimRedInfo, {   show_guide_modal(\"dimensionality_reduction_guide\", \"Dimensionality Reduction Guide\") })  observeEvent(input$showClusteringInfo, {   show_guide_modal(\"clustering_guide\", \"Document Clustering Guide\") }) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_loading_notification.html","id":null,"dir":"Reference","previous_headings":"","what":"Show Loading/Progress Notification — show_loading_notification","title":"Show Loading/Progress Notification — show_loading_notification","text":"Displays persistent loading notification specific ID can removed later.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_loading_notification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show Loading/Progress Notification — show_loading_notification","text":"","code":"show_loading_notification(message, id = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_loading_notification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show Loading/Progress Notification — show_loading_notification","text":"message loading message display id Notification ID later removal (optional)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_loading_notification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show Loading/Progress Notification — show_loading_notification","text":"Displays Shiny notification. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_no_dfm_notification.html","id":null,"dir":"Reference","previous_headings":"","what":"Show No DFM Notification — show_no_dfm_notification","title":"Show No DFM Notification — show_no_dfm_notification","text":"Displays standardized error notification DFM required available. Shorter alternative modal dialog simple error messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_no_dfm_notification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show No DFM Notification — show_no_dfm_notification","text":"","code":"show_no_dfm_notification(feature_name = \"this feature\", duration = 7)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_no_dfm_notification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show No DFM Notification — show_no_dfm_notification","text":"feature_name Name feature requiring DFM (default: \"feature\") duration Duration seconds (default: 7)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_no_dfm_notification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show No DFM Notification — show_no_dfm_notification","text":"Displays Shiny notification. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_no_feature_matrix_notification.html","id":null,"dir":"Reference","previous_headings":"","what":"Show Feature Matrix Notification — show_no_feature_matrix_notification","title":"Show Feature Matrix Notification — show_no_feature_matrix_notification","text":"Displays error notification feature matrix required available. Similar show_no_dfm_notification uses \"feature matrix\" terminology.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_no_feature_matrix_notification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show Feature Matrix Notification — show_no_feature_matrix_notification","text":"","code":"show_no_feature_matrix_notification(duration = 7)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_no_feature_matrix_notification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show Feature Matrix Notification — show_no_feature_matrix_notification","text":"duration Duration seconds (default: 7)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_no_feature_matrix_notification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show Feature Matrix Notification — show_no_feature_matrix_notification","text":"Displays Shiny notification. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_preprocessing_required_modal.html","id":null,"dir":"Reference","previous_headings":"","what":"Show Generic Preprocessing Required Modal — show_preprocessing_required_modal","title":"Show Generic Preprocessing Required Modal — show_preprocessing_required_modal","text":"Displays simple modal indicating preprocessing required. Lightweight alternative detailed steps needed.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_preprocessing_required_modal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show Generic Preprocessing Required Modal — show_preprocessing_required_modal","text":"","code":"show_preprocessing_required_modal(   message = \"Please complete preprocessing steps first.\",   title = \"Preprocessing Required\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_preprocessing_required_modal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show Generic Preprocessing Required Modal — show_preprocessing_required_modal","text":"message Custom message (default: \"Please complete preprocessing steps first.\") title Modal title (default: \"Preprocessing Required\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_preprocessing_required_modal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show Generic Preprocessing Required Modal — show_preprocessing_required_modal","text":"Displays Shiny modal dialog. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_preprocessing_required_modal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show Generic Preprocessing Required Modal — show_preprocessing_required_modal","text":"","code":"if (FALSE) { # \\dontrun{ if (!preprocessing_complete()) {   show_preprocessing_required_modal()   return() } } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_preprocessing_steps_modal.html","id":null,"dir":"Reference","previous_headings":"","what":"Show Preprocessing Steps Modal — show_preprocessing_steps_modal","title":"Show Preprocessing Steps Modal — show_preprocessing_steps_modal","text":"Displays modal dialog listing required preprocessing steps feature. Generic version works feature requiring preprocessing.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_preprocessing_steps_modal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show Preprocessing Steps Modal — show_preprocessing_steps_modal","text":"","code":"show_preprocessing_steps_modal(   title = \"Preprocessing Required\",   message,   required_steps,   optional_steps = NULL,   additional_note = NULL )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_preprocessing_steps_modal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show Preprocessing Steps Modal — show_preprocessing_steps_modal","text":"title Modal title (default: \"Preprocessing Required\") message Main message display required_steps Character vector required preprocessing steps optional_steps Character vector optional preprocessing steps (default: NULL) additional_note Optional additional note display (default: NULL)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_preprocessing_steps_modal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show Preprocessing Steps Modal — show_preprocessing_steps_modal","text":"Displays Shiny modal dialog. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_preprocessing_steps_modal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show Preprocessing Steps Modal — show_preprocessing_steps_modal","text":"","code":"if (FALSE) { # \\dontrun{ show_preprocessing_steps_modal(   message = \"Please complete preprocessing to generate tokens.\",   required_steps = c(\"Step 1: Unite Texts\", \"Step 4: Document-Feature Matrix\"),   optional_steps = c(\"Steps 2, 3, 5, and 6\") ) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_unite_texts_required_notification.html","id":null,"dir":"Reference","previous_headings":"","what":"Show Unite Texts Required Notification — show_unite_texts_required_notification","title":"Show Unite Texts Required Notification — show_unite_texts_required_notification","text":"Displays error notification Step 1 (Unite Texts) required.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_unite_texts_required_notification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show Unite Texts Required Notification — show_unite_texts_required_notification","text":"","code":"show_unite_texts_required_notification(duration = 5)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_unite_texts_required_notification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show Unite Texts Required Notification — show_unite_texts_required_notification","text":"duration Duration seconds (default: 5)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_unite_texts_required_notification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show Unite Texts Required Notification — show_unite_texts_required_notification","text":"Displays Shiny notification. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_warning_notification.html","id":null,"dir":"Reference","previous_headings":"","what":"Show Warning Notification — show_warning_notification","title":"Show Warning Notification — show_warning_notification","text":"Displays warning notification user.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_warning_notification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show Warning Notification — show_warning_notification","text":"","code":"show_warning_notification(message, duration = 5)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_warning_notification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show Warning Notification — show_warning_notification","text":"message warning message display duration Duration seconds (default: 5)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_warning_notification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show Warning Notification — show_warning_notification","text":"Displays Shiny notification. Returns NULL invisibly.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_web_banner.html","id":null,"dir":"Reference","previous_headings":"","what":"Show Web Deployment Banner — show_web_banner","title":"Show Web Deployment Banner — show_web_banner","text":"Creates Shiny UI banner web deployments showing feature limitations.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_web_banner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show Web Deployment Banner — show_web_banner","text":"","code":"show_web_banner(disabled = NULL)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_web_banner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show Web Deployment Banner — show_web_banner","text":"disabled Character vector disabled feature names (optional)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_web_banner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show Web Deployment Banner — show_web_banner","text":"shiny tagList UI element (NULL local)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/show_web_banner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Show Web Deployment Banner — show_web_banner","text":"","code":"if (FALSE) { # \\dontrun{ output$banner <- renderUI({ show_web_banner() }) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_extract_entities.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Named Entities — spacy_extract_entities","title":"Extract Named Entities — spacy_extract_entities","text":"Extract named entities span level using spaCy.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_extract_entities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Named Entities — spacy_extract_entities","text":"","code":"spacy_extract_entities(texts, model = \"en_core_web_sm\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_extract_entities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Named Entities — spacy_extract_entities","text":"texts Character vector texts. model Character; spaCy model use.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_extract_entities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Named Entities — spacy_extract_entities","text":"data.frame columns: doc_id: Document identifier text: Entity text label: Entity type (PERSON, ORG, GPE, DATE, etc.) start_char: Start character position end_char: End character position","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_extract_entities.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Named Entities — spacy_extract_entities","text":"","code":"if (FALSE) { # \\dontrun{ texts <- c(\"Apple was founded by Steve Jobs in Cupertino, California.\") entities <- spacy_extract_entities(texts) print(entities) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_extract_noun_chunks.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Noun Chunks — spacy_extract_noun_chunks","title":"Extract Noun Chunks — spacy_extract_noun_chunks","text":"Extract noun chunks (base noun phrases) using spaCy.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_extract_noun_chunks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Noun Chunks — spacy_extract_noun_chunks","text":"","code":"spacy_extract_noun_chunks(texts, model = \"en_core_web_sm\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_extract_noun_chunks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Noun Chunks — spacy_extract_noun_chunks","text":"texts Character vector texts. model Character; spaCy model use.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_extract_noun_chunks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Noun Chunks — spacy_extract_noun_chunks","text":"data.frame noun chunk information.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_extract_noun_chunks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Noun Chunks — spacy_extract_noun_chunks","text":"","code":"if (FALSE) { # \\dontrun{ texts <- c(\"The quick brown fox jumps over the lazy dog.\") chunks <- spacy_extract_noun_chunks(texts) print(chunks) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_model_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get spaCy Model Information — spacy_model_info","title":"Get spaCy Model Information — spacy_model_info","text":"Get information loaded spaCy model.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_model_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get spaCy Model Information — spacy_model_info","text":"","code":"spacy_model_info(model = \"en_core_web_sm\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_model_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get spaCy Model Information — spacy_model_info","text":"model Character; spaCy model query.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_model_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get spaCy Model Information — spacy_model_info","text":"list model metadata including: model_name: Model identifier lang: Language code pipeline: List pipeline components has_vectors: Whether model includes word vectors vector_dim: Dimension word vectors (available)","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_model_info.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get spaCy Model Information — spacy_model_info","text":"","code":"if (FALSE) { # \\dontrun{ info <- spacy_model_info() print(info) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_nlp.html","id":null,"dir":"Reference","previous_headings":"","what":"spaCy NLP Functions via Python — spacy_nlp","title":"spaCy NLP Functions via Python — spacy_nlp","text":"Comprehensive spaCy integration using Python via reticulate. Provides access spaCy linguistic features including morphology, dependency parsing, named entity recognition, word vectors.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_parse_full.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Texts with Full spaCy Features — spacy_parse_full","title":"Parse Texts with Full spaCy Features — spacy_parse_full","text":"Parse texts using spaCy extract linguistic features including morphology, dependency parsing, POS tags, lemmas, named entities.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_parse_full.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Texts with Full spaCy Features — spacy_parse_full","text":"","code":"spacy_parse_full(   texts,   include_pos = TRUE,   include_lemma = TRUE,   include_entity = TRUE,   include_dependency = TRUE,   include_morphology = TRUE,   model = \"en_core_web_sm\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_parse_full.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Texts with Full spaCy Features — spacy_parse_full","text":"texts Character vector texts parse. include_pos Logical; include part--speech tags (default: TRUE). include_lemma Logical; include lemmatized forms (default: TRUE). include_entity Logical; include named entity recognition (default: TRUE). include_dependency Logical; include dependency parsing (default: TRUE). include_morphology Logical; include morphological features (default: TRUE). model Character; spaCy model use (default: \"en_core_web_sm\").","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_parse_full.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Texts with Full spaCy Features — spacy_parse_full","text":"data.frame token-level annotations: doc_id: Document identifier sentence_id: Sentence number within document token_id: Token position within sentence token: Original token text pos: Universal POS tag (NOUN, VERB, ADJ, etc.) tag: Fine-grained POS tag (NN, VBD, JJ, etc.) lemma: Lemmatized form entity: Named entity type (PERSON, ORG, GPE, etc.) entity_iob: IOB tag (B=beginning, =inside, O=outside) dep_rel: Dependency relation (nsubj, dobj, amod, etc.) head_token_id: Head token dependency tree morph: Full morphological features string morph_*: Individual morphological features columns","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_parse_full.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse Texts with Full spaCy Features — spacy_parse_full","text":"function uses Python spaCy via reticulate, providing access features available spacyr, including morphological analysis. Morphological features include: morph_Number: Sing, Plur morph_Person: 1, 2, 3 morph_Tense: Past, Pres, Fut morph_VerbForm: Fin, Inf, Part, Ger morph_Mood: Ind, Imp, Sub morph_Case: Nom, Acc, Dat, Gen depending language model","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_parse_full.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse Texts with Full spaCy Features — spacy_parse_full","text":"","code":"if (FALSE) { # \\dontrun{ texts <- c(   \"Apple Inc. was founded by Steve Jobs.\",   \"The cats are sleeping on the couch.\" ) result <- spacy_parse_full(texts)  # View morphological features result[, c(\"token\", \"pos\", \"morph\")]  # Filter by POS verbs <- result[result$pos == \"VERB\", ] print(verbs[, c(\"token\", \"lemma\", \"morph_Tense\", \"morph_VerbForm\")]) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Text Similarity — spacy_similarity","title":"Calculate Text Similarity — spacy_similarity","text":"Calculate semantic similarity two texts using spaCy word vectors. Requires model word vectors (en_core_web_md en_core_web_lg).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Text Similarity — spacy_similarity","text":"","code":"spacy_similarity(text1, text2, model = \"en_core_web_md\")"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Text Similarity — spacy_similarity","text":"text1 First text string. text2 Second text string. model Character; spaCy model use (must word vectors).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Text Similarity — spacy_similarity","text":"Numeric similarity score 0 1.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/spacy_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Text Similarity — spacy_similarity","text":"","code":"if (FALSE) { # \\dontrun{ # Requires medium or large model init_spacy_nlp(\"en_core_web_md\") sim <- spacy_similarity(\"I love dogs\", \"I adore puppies\") print(sim)  # High similarity  sim2 <- spacy_similarity(\"I love dogs\", \"The weather is nice\") print(sim2)  # Low similarity } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/SpecialEduTech.html","id":null,"dir":"Reference","previous_headings":"","what":"Special education technology bibliographic data — SpecialEduTech","title":"Special education technology bibliographic data — SpecialEduTech","text":"Contains bibliographic data journal articles dissertations use technology teaching mathematics students disabilities.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/SpecialEduTech.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Special education technology bibliographic data — SpecialEduTech","text":"","code":"SpecialEduTech"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/SpecialEduTech.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Special education technology bibliographic data — SpecialEduTech","text":"object class tbl_df (inherits tbl, data.frame) 490 rows 6 columns.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/SpecialEduTech.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Special education technology bibliographic data — SpecialEduTech","text":"","code":"SpecialEduTech #> # A tibble: 490 × 6 #>    reference_type  author                            year title keyword abstract #>    <chr>           <chr>                            <dbl> <chr> <chr>   <chr>    #>  1 journal_article Block, G. H.                      1980 Dysc… Arithm… Notes t… #>  2 thesis          Bukatman, K. L.                   1981 The … locus … This st… #>  3 journal_article Watkins, M. W., & Webb, C.        1981 Comp… Comput… Results… #>  4 journal_article Chaffin, J. D.                    1982 Arc-… Comput… The Arc… #>  5 journal_article Chaffin, J. D., Maxwell, B., & …  1982 ARC-… Electr… This ar… #>  6 thesis          Golden, C. K.                     1982 The … NA      The pur… #>  7 journal_article Neal, D.                          1982 A re… tradit… Discuss… #>  8 thesis          Englebert, B. B.                  1983 A st… microc… The pur… #>  9 thesis          Foster, K.                        1983 The … comput… The eff… #> 10 journal_article Pommer, L. T.                     1983 Usin… Comput… The art… #> # ℹ 480 more rows"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stm_15.html","id":null,"dir":"Reference","previous_headings":"","what":"An example structure of a structural topic model — stm_15","title":"An example structure of a structural topic model — stm_15","text":"Contains 15 topics, topic prevalences, etc. stm::stm.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stm_15.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An example structure of a structural topic model — stm_15","text":"","code":"stm_15"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stm_15.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"An example structure of a structural topic model — stm_15","text":"object class STM length 11.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stm_15.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"An example structure of a structural topic model — stm_15","text":"","code":"stm_15 #> A topic model with 15 topics, 488 documents and a 4511 word dictionary."},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stopwords_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Stopwords List — stopwords_list","title":"Stopwords List — stopwords_list","text":"dataset containing stopwords text preprocessing","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/stopwords_list.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Stopwords List — stopwords_list","text":"character vector stopwords","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/temporal_semantic_analysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Temporal Semantic Analysis — temporal_semantic_analysis","title":"Temporal Semantic Analysis — temporal_semantic_analysis","text":"Analyzes semantic patterns time","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/temporal_semantic_analysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Temporal Semantic Analysis — temporal_semantic_analysis","text":"","code":"temporal_semantic_analysis(   texts,   dates,   time_windows = \"month\",   embeddings = NULL,   verbose = FALSE,   ... )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/temporal_semantic_analysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Temporal Semantic Analysis — temporal_semantic_analysis","text":"texts Character vector texts analyze dates Date vector corresponding texts time_windows Time window size grouping (default: \"month\") embeddings Optional pre-computed embeddings verbose Logical indicating whether print progress messages ... Additional parameters","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/temporal_semantic_analysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Temporal Semantic Analysis — temporal_semantic_analysis","text":"List containing temporal analysis results","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/TextAnalysisR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"TextAnalysisR: Text Mining Workflow Tool — TextAnalysisR-package","title":"TextAnalysisR: Text Mining Workflow Tool — TextAnalysisR-package","text":"Comprehensive toolkit text mining natural language processing interactive 'Shiny' interface. Import documents multiple formats (PDF, DOCX, XLSX, CSV, TXT), preprocess 'quanteda', perform topic modeling ('stm'), semantic analysis, sentiment analysis, network visualization. Features AI-assisted workflows via 'LangGraph', WCAG 2.1 AA accessibility, multi-language support, enterprise security production deployment.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/TextAnalysisR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"TextAnalysisR: Text Mining Workflow Tool — TextAnalysisR-package","text":"Maintainer: Mikyung Shin shin.mikyung@gmail.com (ORCID) (Illinois State University)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/unite_cols.html","id":null,"dir":"Reference","previous_headings":"","what":"Unite Text Columns — unite_cols","title":"Unite Text Columns — unite_cols","text":"function unites specified text columns data frame single column named \"united_texts\" retaining original columns.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/unite_cols.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unite Text Columns — unite_cols","text":"","code":"unite_cols(df, listed_vars)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/unite_cols.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unite Text Columns — unite_cols","text":"df data frame contains text data. listed_vars character vector column names united \"united_texts\".","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/unite_cols.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unite Text Columns — unite_cols","text":"data frame new column \"united_texts\" created uniting specified variables.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/unite_cols.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unite Text Columns — unite_cols","text":"","code":"if (interactive()) {   mydata <- TextAnalysisR::SpecialEduTech    united_tbl <- TextAnalysisR::unite_cols(     mydata,     listed_vars = c(\"title\", \"keyword\", \"abstract\")   )   print(united_tbl) }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_api_key.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate OpenAI API Key Format — validate_api_key","title":"Validate OpenAI API Key Format — validate_api_key","text":"Validates OpenAI API key format according NIST IA-5(1) authenticator management. Checks key prefix, length, basic format requirements.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_api_key.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate OpenAI API Key Format — validate_api_key","text":"","code":"validate_api_key(api_key, strict = TRUE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_api_key.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate OpenAI API Key Format — validate_api_key","text":"api_key Character string containing API key strict Logical, TRUE performs additional validation checks","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_api_key.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate OpenAI API Key Format — validate_api_key","text":"Logical TRUE valid, FALSE warnings invalid","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_api_key.html","id":"nist-compliance","dir":"Reference","previous_headings":"","what":"NIST Compliance","title":"Validate OpenAI API Key Format — validate_api_key","text":"Implements NIST IA-5(1): Authenticator Management - Password-Based Authentication. Validates format, length, character composition prevent weak malformed keys.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_api_key.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate OpenAI API Key Format — validate_api_key","text":"","code":"if (FALSE) { # \\dontrun{ validate_api_key(\"sk-proj...\") } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_column_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Column Name — validate_column_name","title":"Validate Column Name — validate_column_name","text":"Validates column names prevent code injection formula construction. Ensures column names follow R naming conventions contain malicious patterns.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_column_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Column Name — validate_column_name","text":"","code":"validate_column_name(col_name)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_column_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Column Name — validate_column_name","text":"col_name Character string containing column name","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_column_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Column Name — validate_column_name","text":"TRUE valid, stops error invalid","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_column_name.html","id":"security","dir":"Reference","previous_headings":"","what":"Security","title":"Validate Column Name — validate_column_name","text":"Protects formula injection attacks malicious column names execute arbitrary code used model formulas. Part NIST SI-10 input validation.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_column_name.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate Column Name — validate_column_name","text":"","code":"if (FALSE) { # \\dontrun{ validate_column_name(\"age\") validate_column_name(\"my_variable\") } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_cross_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Analysis Validation — validate_cross_models","title":"Cross-Analysis Validation — validate_cross_models","text":"Performs cross-validation different analysis types (STM, semantic, clustering).","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_cross_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Analysis Validation — validate_cross_models","text":"","code":"validate_cross_models(semantic_results, stm_results = NULL, verbose = TRUE)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_cross_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Analysis Validation — validate_cross_models","text":"semantic_results Results semantic analysis. stm_results Optional STM results comparison. verbose Logical, TRUE, prints progress messages.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_cross_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Analysis Validation — validate_cross_models","text":"list containing cross-validation metrics.","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_file_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Cybersecurity Utility Functions — validate_file_upload","title":"Cybersecurity Utility Functions — validate_file_upload","text":"Functions input validation, sanitization, security logging","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_file_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cybersecurity Utility Functions — validate_file_upload","text":"","code":"validate_file_upload(file_info)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_file_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cybersecurity Utility Functions — validate_file_upload","text":"file_info File info object Shiny fileInput","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_file_upload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cybersecurity Utility Functions — validate_file_upload","text":"TRUE valid, stops error message invalid","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_file_upload.html","id":"nist-compliance","dir":"Reference","previous_headings":"","what":"NIST Compliance","title":"Cybersecurity Utility Functions — validate_file_upload","text":"package follows NIST security standards (based NIST SP 800-53): SC-8: Transmission Confidentiality Integrity (HTTPS encryption) SC-28: Protection Information Rest (secure API key storage) IA-5: Authenticator Management (API key validation format checking) AC-3: Access Enforcement (rate limiting, input validation, file type restrictions) SI-10: Information Input Validation (malicious content detection) AU-2: Audit Events (security logging monitoring) Validate File Upload","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_keyboard_navigation.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Keyboard Navigation — validate_keyboard_navigation","title":"Validate Keyboard Navigation — validate_keyboard_navigation","text":"Checks interactive elements proper tabindex keyboard handlers. Used WCAG 2.1.1 (Keyboard) compliance.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_keyboard_navigation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Keyboard Navigation — validate_keyboard_navigation","text":"","code":"validate_keyboard_navigation(tabindex = 0)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_keyboard_navigation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Keyboard Navigation — validate_keyboard_navigation","text":"tabindex Integer, tab order (-1 tab, 0 natural order, 1+ specific order)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_keyboard_navigation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Keyboard Navigation — validate_keyboard_navigation","text":"Logical TRUE valid, FALSE warning invalid","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_keyboard_navigation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate Keyboard Navigation — validate_keyboard_navigation","text":"","code":"if (FALSE) { # \\dontrun{ validate_keyboard_navigation(0)   # TRUE validate_keyboard_navigation(999) # FALSE (too high) } # }"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_semantic_coherence.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Semantic Coherence — validate_semantic_coherence","title":"Validate Semantic Coherence — validate_semantic_coherence","text":"Validates semantic coherence topic assignments","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_semantic_coherence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Semantic Coherence — validate_semantic_coherence","text":"","code":"validate_semantic_coherence(embeddings, topic_assignments, ...)"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_semantic_coherence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Semantic Coherence — validate_semantic_coherence","text":"embeddings Document embeddings matrix topic_assignments Vector topic assignments documents ... Additional parameters","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_semantic_coherence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Semantic Coherence — validate_semantic_coherence","text":"List containing coherence score metrics","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_topic_labels_langgraph.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate User-Selected Topic Labels — validate_topic_labels_langgraph","title":"Validate User-Selected Topic Labels — validate_topic_labels_langgraph","text":"Uses LangGraph workflow validate user-selected topic labels using LLM.","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_topic_labels_langgraph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate User-Selected Topic Labels — validate_topic_labels_langgraph","text":"","code":"validate_topic_labels_langgraph(   user_labels,   topic_terms,   ollama_model = \"llama3\",   ollama_base_url = \"http://localhost:11434\",   envname = \"textanalysisr-env\" )"},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_topic_labels_langgraph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate User-Selected Topic Labels — validate_topic_labels_langgraph","text":"user_labels Character vector user-selected labels topic topic_terms List character vectors top terms topic ollama_model Character string, Ollama model name (default: \"llama3\") ollama_base_url Character string, Ollama API URL (default: \"http://localhost:11434\") envname Character string, Python virtual environment name (default: \"langgraph-env\")","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_topic_labels_langgraph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate User-Selected Topic Labels — validate_topic_labels_langgraph","text":"List : success: Logical, TRUE validation completed validation_metrics: List coherence distinctiveness scores error: Error message (failed)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_topic_labels_langgraph.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Validate User-Selected Topic Labels — validate_topic_labels_langgraph","text":"Validation metrics include: coherence_scores: well labels match term distributions (0-10 scale) distinctiveness_scores: unique/specific labels (0-10 scale) overall_quality: Average coherence distinctiveness","code":""},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/reference/validate_topic_labels_langgraph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate User-Selected Topic Labels — validate_topic_labels_langgraph","text":"","code":"if (FALSE) { # \\dontrun{ user_labels <- c(\"Education Policy\", \"Healthcare Services\", \"Climate Action\") topic_terms <- list(   c(\"education\", \"student\", \"learning\"),   c(\"health\", \"medical\", \"patient\"),   c(\"environment\", \"climate\", \"carbon\") )  validation <- validate_topic_labels_langgraph(   user_labels = user_labels,   topic_terms = topic_terms )  print(validation$validation_metrics$overall_quality) } # }"},{"path":[]},{"path":"https://mshin77.github.io/TextAnalysisR/news/index.html","id":"new-features-0-0-3","dir":"Changelog","previous_headings":"","what":"New Features","title":"TextAnalysisR 0.0.3 (2025-11-29)","text":"Interactive Shiny web app point--click interface Multi-format file import (PDF, DOCX, XLSX, CSV, TXT) Semantic analysis neural network embeddings Hybrid topic modeling (STM + BERTopic) OpenAI integration AI-generated topic labels","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/news/index.html","id":"accessibility-0-0-3","dir":"Changelog","previous_headings":"","what":"Accessibility","title":"TextAnalysisR 0.0.3 (2025-11-29)","text":"WCAG 2.1 Level AA compliant Dark mode high contrast Keyboard navigation screen reader support Multi-language support (100+ languages)","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/news/index.html","id":"security-0-0-3","dir":"Changelog","previous_headings":"","what":"Security","title":"TextAnalysisR 0.0.3 (2025-11-29)","text":"Session-based rate limiting Input validation sanitization API key protection Security event logging","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/news/index.html","id":"textanalysisr-002-2024-12-05","dir":"Changelog","previous_headings":"","what":"TextAnalysisR 0.0.2 (2024-12-05)","title":"TextAnalysisR 0.0.2 (2024-12-05)","text":"Improved documentation Enhanced DESCRIPTION metadata CRAN policy compliance updates","code":""},{"path":"https://mshin77.github.io/TextAnalysisR/news/index.html","id":"textanalysisr-001-2023-10-18","dir":"Changelog","previous_headings":"","what":"TextAnalysisR 0.0.1 (2023-10-18)","title":"TextAnalysisR 0.0.1 (2023-10-18)","text":"Initial CRAN release Core text mining functionality STM topic modeling Text preprocessing capabilities Basic Shiny application","code":""}]
