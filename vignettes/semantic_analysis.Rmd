---
title: "Semantic Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Semantic Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, eval = FALSE)
```

Semantic analysis finds patterns of meaning using embeddings and neural networks.

## Setup

```{r}
library(TextAnalysisR)

mydata <- SpecialEduTech
united_tbl <- unite_cols(mydata, listed_vars = c("title", "keyword", "abstract"))
tokens <- prep_texts(united_tbl, text_field = "united_texts")
dfm_object <- quanteda::dfm(tokens)
```

## Document Similarity

```{r}
similarity <- semantic_similarity_analysis(
  texts = united_tbl$united_texts,
  method = "cosine"
)
```

---

<details>
<summary><strong>Similarity Methods</strong></summary>

Semantic analysis measures document similarity using different approaches to capture meaning, from simple vocabulary matching to deep neural representations.

**Methods:**

| Method | Description | Best For |
|--------|-------------|----------|
| Words | Lexical analysis using word frequency vectors (bag-of-words) | Finding documents with shared terminology |
| N-grams | Phrase-based analysis capturing word sequences | Detecting similar phraseology |
| Embeddings | Deep semantic analysis using transformer models | Conceptual similarity, handles synonyms |

**Usage:** Choose method based on your analysis goals. Words and n-grams are faster and interpretable. Embeddings capture deeper meaning but require more computation. All methods use cosine similarity for comparison.

**Learn More:** [Sentence Transformers Documentation](https://www.sbert.net/)

</details>

---

## Sentiment Analysis

### Lexicon-based (no Python)

```{r}
sentiment <- sentiment_lexicon_analysis(dfm_object, lexicon = "afinn")
plot_sentiment_distribution(sentiment$document_sentiment)
```

### Neural (requires Python)

```{r}
sentiment <- sentiment_embedding_analysis(united_tbl$united_texts)
```

---

## Document Clustering

```{r}
clusters <- semantic_document_clustering(
  texts = united_tbl$united_texts,
  n_clusters = 5
)

# AI-generated labels (optional)
labels <- generate_cluster_labels(
  clusters$cluster_keywords,
  provider = "ollama"  # or "openai"
)
```

---

<details>
<summary><strong>Clustering Algorithms</strong></summary>

Clustering groups documents with similar semantic content into categories. Documents within a cluster are more similar to each other than to documents in other clusters.

**Algorithms:**

| Algorithm | Description | Use Case |
|-----------|-------------|----------|
| K-means | Creates K spherical clusters | Fast, simple, requires specifying K |
| Hierarchical | Builds tree of clusters | Exploring nested structures |
| DBSCAN | Density-based, finds outliers | Arbitrarily shaped clusters |
| HDBSCAN | Hierarchical density-based | Auto-determines cluster count |

**Usage:** Choose discovery mode (Automatic, Manual, Advanced). Select semantic feature space and algorithm. Automatic mode finds optimal cluster count. Use visualizations and quality metrics to evaluate results.

**Learn More:** [scikit-learn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html)

</details>

---

<details>
<summary><strong>Dimensionality Reduction</strong></summary>

Dimensionality reduction transforms high-dimensional data into 2D or 3D visualizations while preserving the structure and relationships between documents.

**Algorithms:**

| Algorithm | Description | Trade-offs |
|-----------|-------------|------------|
| PCA | Principal Component Analysis, finds linear patterns | Fast, interpretable |
| t-SNE | Preserves local structure, reveals clusters | Slow, good for visualization |
| UMAP | Balances local and global structure | Faster than t-SNE, better topology |

**Usage:** Select a semantic feature space (words, n-grams, or embeddings), then choose a reduction method. Adjust parameters (perplexity, neighbors, dimensions) based on your data size and structure. Use for visual exploration before clustering.

```{r}
reduced <- reduce_dimensions(embeddings, method = "umap", n_components = 2)
plot_semantic_viz(reduced)
```

**Learn More:** [scikit-learn Manifold Learning](https://scikit-learn.org/stable/modules/manifold.html)

</details>

---

## Temporal Analysis

Track themes over time:

```{r}
temporal <- temporal_semantic_analysis(
  texts = united_tbl$united_texts,
  timestamps = united_tbl$year
)
```

---

<details>
<summary><strong>Embedding Models</strong></summary>

| Model | Speed | Quality | Use Case |
|-------|-------|---------|----------|
| all-MiniLM-L6-v2 | Fast | Good | General purpose |
| all-mpnet-base-v2 | Slow | Best | Highest quality |
| paraphrase-multilingual | Medium | Good | Multiple languages |

**Learn More:** [Sentence Transformers Models](https://www.sbert.net/docs/pretrained_models.html)

</details>

---

## Next Steps

- [Topic Modeling](topic_modeling.html)
