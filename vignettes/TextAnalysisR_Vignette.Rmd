---
title: "TextAnalysisR Vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{TextAnalysisR Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE
)
```

`TextAnalysisR` provides a supporting workflow tool for text mining analysis. The web app incorporates [quanteda](https://github.com/quanteda/quanteda) (text preprocessing), [stm](https://github.com/bstewart/stm) (structural topic modeling), [ggraph](https://github.com/thomasp85/ggraph), and [widyr](https://github.com/juliasilge/widyr) (network analysis). [tidytext](https://github.com/cran/tidytext) is implemented to tidy non-tidy format objects. The R Shiny web app is available at `TextAnalysisR::TextAnalysisR.app()` or <https://textanalysisr.org>. Functions are provided for completing word-topic probabilities, document-topic probabilities, estimated effects of covariates on topic prevalence, and network analysis, similar to tasks available in the web app.

**These steps are similar to those demonstrated in the Shiny web app at `TextAnalysisR::TextAnalysisR.app()`.**

## Install the TextAnalysisR Package

The development version can be installed from [GitHub](https://github.com/mshin77/TextAnalysisR):

```{r, eval=FALSE}
install.packages("devtools")
devtools::install_github("mshin77/TextAnalysisR")
```

## Load the TextAnalysisR Library

```{r}
library(TextAnalysisR)
```

## Alternatively, Launch and Browse the Shiny App

```{r, eval=FALSE}
if (interactive()) {
  TextAnalysisR.app()
}
```

## Process Files

#### Choose the Dataset

##### Upload an example dataset

```{r}
mydata <- TextAnalysisR::process_files(dataset_choice = "Upload an Example Dataset")

head(mydata)
```

##### Upload your file

```{r}
file_info <- data.frame(filepath = "inst/extdata/SpecialEduTech.xlsx")
mydata <- TextAnalysisR::process_files(dataset_choice = "Upload Your File",
                                       file_info = file_info)
head(mydata)
```

##### Copy and paste text

```{r}
text_input <- paste0("The purpose of this study was to conduct a content analysis of ",
                     "research on technology use.")
mydata <- TextAnalysisR::process_files(dataset_choice = "Copy and Paste Text",
                                       text_input = text_input)
mydata
```

## Preprocess Text Data

#### Unite Text Columns

```{r}
df <- TextAnalysisR::SpecialEduTech

united_tbl <- TextAnalysisR::unite_text_cols(df, listed_vars = c("title", "keyword", "abstract"))

united_tbl
```

#### Segment a Corpus Into Tokens

```{r}
tokens <- TextAnalysisR::preprocess_texts(united_tbl,
                                          text_field = "united_texts",
                                          min_char = 2,
                                          remove_punct = TRUE,
                                          remove_symbols = TRUE,
                                          remove_numbers = TRUE,
                                          remove_url = TRUE,
                                          remove_separators = TRUE,
                                          split_hyphens = TRUE,
                                          split_tags = TRUE,
                                          include_docvars = TRUE,
                                          keep_acronyms = FALSE,
                                          padding = FALSE,
                                          verbose = FALSE)
```

#### Detect Multi-Word Expressions

```{r}
collocations <- TextAnalysisR::detect_multi_word_expressions(tokens, size = 2:5, 
                                                             min_count = 2)

head(collocations)
```

#### Process Tokens With Compound Words

```{r}
library(quanteda)
custom_dict <- quanteda::dictionary(list(custom = c("learning disabilities", "problem solving", "assisted instruction")))

toks_compound <- quanteda::tokens_compound(
  tokens,
  pattern = custom_dict,
  concatenator = "_",
  valuetype = "glob",
  window = 0,
  case_insensitive = TRUE,
  join = TRUE,
  keep_unigrams = FALSE,
  verbose = TRUE
)
```

#### Plot Word Frequency

```{r, fig.alt= "Word Frequency Plot"}
dfm_object_init <- quanteda::dfm(toks_compound)

TextAnalysisR::plot_word_frequency(dfm_object_init, n = 20)
```

#### Remove Predefined Stopwords

```{r}
stopwords <- stopwords::stopwords("en", source = "snowball")

toks_removed <- quanteda::tokens_remove(toks_compound, pattern = stopwords, verbose = FALSE)

dfm_object_init <- quanteda::dfm(toks_removed)

TextAnalysisR::plot_word_frequency(dfm_object_init, n = 20)
```

#### Remove Common Words in the Dataset

```{r}
common_words <- c("study", "based", "learning", "students", "research", "results")

toks_removed_common <- quanteda::tokens_remove(toks_removed, pattern = common_words, verbose = FALSE)

dfm_object_init <- quanteda::dfm(toks_removed_common)

TextAnalysisR::plot_word_frequency(dfm_object_init, n = 20)
```

#### Lemmatize Tokens

```{r}
library(spacyr)
texts <- sapply(toks_removed_common, paste, collapse = " ")

parsed <- spacyr::spacy_parse(x = texts, lemma = TRUE, entity = FALSE, pos = FALSE)

toks_lemmatized <- quanteda::as.tokens(parsed, use_lemma = TRUE)

dfm_object <- quanteda::dfm(toks_lemmatized)

quanteda::docvars(dfm_object) <- quanteda::docvars(dfm_object_init)

TextAnalysisR::plot_word_frequency(dfm_object, n = 20)
```

## Word Analysis

#### Analyze and Visualize Word Co-occurrence Networks

##### Word co-occurrence network plot

```{r, fig.alt= "Word Co-occurrence Network"}
word_co_occurrence_network_results <- 
    TextAnalysisR::word_co_occurrence_network(
        dfm_object,
        doc_var = "reference_type",
        co_occur_n = 100,
        top_node_n = 30,
        nrows = 1,
        height = 1200,
        width = 800)

word_co_occurrence_network_results$plot
```

##### Word co-occurrence network centrality table

```{r}
word_co_occurrence_network_results$table
```

##### Word co-occurrence network summary

```{r}
word_co_occurrence_network_results$summary
```

#### Analyze and Visualize Word Correlation Networks

##### Word correlation network plot

```{r, fig.alt= "Word Correlation Network"}
word_correlation_network_results <- 
  TextAnalysisR::word_correlation_network(
    dfm_object,
    doc_var = "reference_type",
    common_term_n = 30,
    corr_n = 0.42,
    top_node_n = 40,
    nrows = 2,
    height = 1400,
    width = 800)

word_correlation_network_results$plot
```

##### Word correlation network centrality table

```{r}
word_correlation_network_results$table
```

##### Word correlation network summary

```{r}
word_correlation_network_results$summary
```

#### Analyze and Visualize Word Frequency Distributions Across a Continuous Variable

##### Word frequency distribution plot

```{r, fig.alt= "Word Proportion across a Continuous Variable"}
word_frequency_distribution_results <- TextAnalysisR::word_frequency_distribution(
  dfm_object,
  continuous_variable = "year",
  selected_terms = c("calculator", "computer"),
  height = 500,
  width = 800)

word_frequency_distribution_results$plot
```

##### Word frequency distribution table

```{r}
word_frequency_distribution_results$table
```

## Structural Topic Modeling

#### Evaluate Optimal Number of Topics

```{r, fig.alt= "Evaluate Optimal Number of Topics"}
TextAnalysisR::evaluate_optimal_topic_number(
  dfm_object = dfm_object,
  topic_range = 5:30,
  max.em.its = 75,
  categorical_var = "reference_type",
  continuous_var = "year",
  height = 600,
  width = 800,
  verbose = FALSE)
```

#### Select Top Terms for Each Topic

```{r}
out <- quanteda::convert(dfm_object, to = "stm")

library(stm)
stm_15 <- stm(
  data = out$meta,
  documents = out$documents,
  vocab = out$vocab,
  max.em.its = 75,
  init.type = "Spectral",
  K = 15,
  prevalence = ~ reference_type + s(year),
  verbose = FALSE)

top_topic_terms <- TextAnalysisR::select_top_topic_terms(
  stm_model = stm_15,
  top_term_n = 10,
  verbose = FALSE
)

library(dplyr)
top_topic_terms %>%
  mutate_if(is.numeric, ~ round(., 3)) %>%
      DT::datatable(
      rownames = FALSE,
      extensions = 'Buttons',
      options = list(
        scrollX = TRUE,
        scrollY = "400px",
        width = "80%",
        dom = 'Bfrtip',
        buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
      )
    ) %>%
    DT::formatStyle(
      columns = c("topic", "term", "beta"),
      fontSize = '16px'
    )
```

#### Generate Topic Labels Using OpenAI's API

##### Directly input the Open AI API key

```{r, eval=FALSE}
top_labeled_topic_terms <- TextAnalysisR::generate_topic_labels(
  top_topic_terms,
  model = "gpt-3.5-turbo",
  temperature = 0.5,
  openai_api_key = "your_openai_api_key",
  verbose = FALSE)

top_labeled_topic_terms 
```

##### Load the Open AI API key from the .env file in the working directory

```{r}
top_labeled_topic_terms <- TextAnalysisR::generate_topic_labels(
  top_topic_terms,
  model = "gpt-3.5-turbo",
  temperature = 0.5,
  verbose = FALSE)

top_labeled_topic_terms %>%
  mutate_if(is.numeric, ~ round(., 3)) %>%
      DT::datatable(
      rownames = FALSE,
      extensions = 'Buttons',
      options = list(
        scrollX = TRUE,
        scrollY = "400px",
        width = "80%",
        dom = 'Bfrtip',
        buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
      )
    ) %>%
    DT::formatStyle(
      columns = c("topic", "topic_label", "term", "beta"),
      fontSize = '16px'
    )
```

#### Plot Highest Word Probabilities for Each Topic

##### Add generated topic lables to the top topic terms

```{r, fig.alt= "Plot Highest Word Probabilities for Each Topic"}
word_probability_plot <- TextAnalysisR::word_probability_plot(
  top_labeled_topic_terms,
  topic_label = "topic_label",
  ncol = 1,
  height = 3500,
  width = 800)

word_probability_plot
```

##### Use non-labeled topics

```{r, fig.alt= "Plot Highest Word Probabilities for Each Topic"}
word_probability_plot <- TextAnalysisR::word_probability_plot(
  top_topic_terms,
  ncol = 2,
  height = 2000,
  width = 800)

word_probability_plot
```

##### Manually label topics

```{r, fig.alt= "Plot Highest Word Probabilities for Each Topic"}
manual_labels <- c("1" = "CAI for math problem solving",
                   "2" = "STEM technology",
                   "3" = "Use of manipulatives")

word_probability_plot <- TextAnalysisR::word_probability_plot(
  top_topic_terms,
  topic_label = manual_labels,
  ncol = 2,
  height = 2000,
  width = 850)

word_probability_plot
```

#### Per-Document Per-Topic Probabilities

##### Plot mean topic prevalence across documents

```{r}
topic_probability_plot <- TextAnalysisR::topic_probability_plot(
  stm_model= stm_15,
  top_n = 10,
  height = 500,
  width = 800,
  verbose = TRUE)

topic_probability_plot
```

##### create a table of mean topic prevalence across documents

```{r}
topic_probability_table <- TextAnalysisR::topic_probability_table(
  stm_model= stm_15,
  top_n = 10,
  verbose = TRUE)

topic_probability_table
```
